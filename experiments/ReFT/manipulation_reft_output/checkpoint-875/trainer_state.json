{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 875,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.045714285714285714,
      "grad_norm": 8.690507888793945,
      "learning_rate": 9.977064220183486e-05,
      "loss": 10.3159,
      "step": 10
    },
    {
      "epoch": 0.09142857142857143,
      "grad_norm": 4.060633182525635,
      "learning_rate": 9.954128440366974e-05,
      "loss": 6.3988,
      "step": 20
    },
    {
      "epoch": 0.13714285714285715,
      "grad_norm": 4.209723949432373,
      "learning_rate": 9.931192660550459e-05,
      "loss": 6.1603,
      "step": 30
    },
    {
      "epoch": 0.18285714285714286,
      "grad_norm": 3.8396549224853516,
      "learning_rate": 9.908256880733946e-05,
      "loss": 5.961,
      "step": 40
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 4.556296348571777,
      "learning_rate": 9.885321100917432e-05,
      "loss": 5.9653,
      "step": 50
    },
    {
      "epoch": 0.2742857142857143,
      "grad_norm": 4.246315956115723,
      "learning_rate": 9.862385321100918e-05,
      "loss": 5.7879,
      "step": 60
    },
    {
      "epoch": 0.32,
      "grad_norm": 4.510865211486816,
      "learning_rate": 9.839449541284404e-05,
      "loss": 5.867,
      "step": 70
    },
    {
      "epoch": 0.3657142857142857,
      "grad_norm": 3.938235282897949,
      "learning_rate": 9.816513761467891e-05,
      "loss": 5.819,
      "step": 80
    },
    {
      "epoch": 0.4114285714285714,
      "grad_norm": 3.643468141555786,
      "learning_rate": 9.793577981651376e-05,
      "loss": 5.7243,
      "step": 90
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 3.7212650775909424,
      "learning_rate": 9.770642201834863e-05,
      "loss": 5.7096,
      "step": 100
    },
    {
      "epoch": 0.5028571428571429,
      "grad_norm": 3.82885479927063,
      "learning_rate": 9.74770642201835e-05,
      "loss": 5.689,
      "step": 110
    },
    {
      "epoch": 0.5485714285714286,
      "grad_norm": 4.126797676086426,
      "learning_rate": 9.724770642201836e-05,
      "loss": 5.6834,
      "step": 120
    },
    {
      "epoch": 0.5942857142857143,
      "grad_norm": 4.498561382293701,
      "learning_rate": 9.701834862385321e-05,
      "loss": 5.6837,
      "step": 130
    },
    {
      "epoch": 0.64,
      "grad_norm": 3.5291748046875,
      "learning_rate": 9.678899082568808e-05,
      "loss": 5.6049,
      "step": 140
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 4.003115177154541,
      "learning_rate": 9.655963302752295e-05,
      "loss": 5.6886,
      "step": 150
    },
    {
      "epoch": 0.7314285714285714,
      "grad_norm": 3.8343048095703125,
      "learning_rate": 9.63302752293578e-05,
      "loss": 5.5523,
      "step": 160
    },
    {
      "epoch": 0.7771428571428571,
      "grad_norm": 4.415511131286621,
      "learning_rate": 9.610091743119267e-05,
      "loss": 5.5251,
      "step": 170
    },
    {
      "epoch": 0.8228571428571428,
      "grad_norm": 3.9554286003112793,
      "learning_rate": 9.587155963302753e-05,
      "loss": 5.5614,
      "step": 180
    },
    {
      "epoch": 0.8685714285714285,
      "grad_norm": 4.694336414337158,
      "learning_rate": 9.564220183486238e-05,
      "loss": 5.651,
      "step": 190
    },
    {
      "epoch": 0.9142857142857143,
      "grad_norm": 4.5557942390441895,
      "learning_rate": 9.541284403669725e-05,
      "loss": 5.5208,
      "step": 200
    },
    {
      "epoch": 0.96,
      "grad_norm": 3.2789740562438965,
      "learning_rate": 9.518348623853212e-05,
      "loss": 5.4728,
      "step": 210
    },
    {
      "epoch": 1.0057142857142858,
      "grad_norm": 4.646069526672363,
      "learning_rate": 9.495412844036697e-05,
      "loss": 5.5467,
      "step": 220
    },
    {
      "epoch": 1.0514285714285714,
      "grad_norm": 4.240537166595459,
      "learning_rate": 9.472477064220184e-05,
      "loss": 5.5378,
      "step": 230
    },
    {
      "epoch": 1.0971428571428572,
      "grad_norm": 3.5913121700286865,
      "learning_rate": 9.44954128440367e-05,
      "loss": 5.4913,
      "step": 240
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 3.4259543418884277,
      "learning_rate": 9.426605504587156e-05,
      "loss": 5.5671,
      "step": 250
    },
    {
      "epoch": 1.1885714285714286,
      "grad_norm": 3.788799285888672,
      "learning_rate": 9.403669724770642e-05,
      "loss": 5.5992,
      "step": 260
    },
    {
      "epoch": 1.2342857142857142,
      "grad_norm": 4.091202735900879,
      "learning_rate": 9.380733944954129e-05,
      "loss": 5.5042,
      "step": 270
    },
    {
      "epoch": 1.28,
      "grad_norm": 4.819816589355469,
      "learning_rate": 9.357798165137616e-05,
      "loss": 5.5506,
      "step": 280
    },
    {
      "epoch": 1.3257142857142856,
      "grad_norm": 4.1068243980407715,
      "learning_rate": 9.334862385321101e-05,
      "loss": 5.4103,
      "step": 290
    },
    {
      "epoch": 1.3714285714285714,
      "grad_norm": 3.874763011932373,
      "learning_rate": 9.311926605504587e-05,
      "loss": 5.5472,
      "step": 300
    },
    {
      "epoch": 1.4171428571428573,
      "grad_norm": 4.296212673187256,
      "learning_rate": 9.288990825688074e-05,
      "loss": 5.4604,
      "step": 310
    },
    {
      "epoch": 1.4628571428571429,
      "grad_norm": 4.06827974319458,
      "learning_rate": 9.266055045871561e-05,
      "loss": 5.5135,
      "step": 320
    },
    {
      "epoch": 1.5085714285714285,
      "grad_norm": 4.138288497924805,
      "learning_rate": 9.243119266055046e-05,
      "loss": 5.5174,
      "step": 330
    },
    {
      "epoch": 1.5542857142857143,
      "grad_norm": 4.608002185821533,
      "learning_rate": 9.220183486238533e-05,
      "loss": 5.5935,
      "step": 340
    },
    {
      "epoch": 1.6,
      "grad_norm": 4.3186845779418945,
      "learning_rate": 9.197247706422019e-05,
      "loss": 5.5827,
      "step": 350
    },
    {
      "epoch": 1.6457142857142857,
      "grad_norm": 4.5327253341674805,
      "learning_rate": 9.174311926605506e-05,
      "loss": 5.531,
      "step": 360
    },
    {
      "epoch": 1.6914285714285713,
      "grad_norm": 4.0583319664001465,
      "learning_rate": 9.151376146788991e-05,
      "loss": 5.4387,
      "step": 370
    },
    {
      "epoch": 1.737142857142857,
      "grad_norm": 4.766531944274902,
      "learning_rate": 9.128440366972478e-05,
      "loss": 5.5131,
      "step": 380
    },
    {
      "epoch": 1.782857142857143,
      "grad_norm": 3.6075353622436523,
      "learning_rate": 9.105504587155964e-05,
      "loss": 5.4429,
      "step": 390
    },
    {
      "epoch": 1.8285714285714287,
      "grad_norm": 3.409393787384033,
      "learning_rate": 9.08256880733945e-05,
      "loss": 5.5295,
      "step": 400
    },
    {
      "epoch": 1.8742857142857143,
      "grad_norm": 4.079815864562988,
      "learning_rate": 9.059633027522936e-05,
      "loss": 5.424,
      "step": 410
    },
    {
      "epoch": 1.92,
      "grad_norm": 3.615144729614258,
      "learning_rate": 9.036697247706423e-05,
      "loss": 5.4586,
      "step": 420
    },
    {
      "epoch": 1.9657142857142857,
      "grad_norm": 3.831975221633911,
      "learning_rate": 9.013761467889908e-05,
      "loss": 5.4219,
      "step": 430
    },
    {
      "epoch": 2.0114285714285716,
      "grad_norm": 4.201025009155273,
      "learning_rate": 8.990825688073395e-05,
      "loss": 5.5265,
      "step": 440
    },
    {
      "epoch": 2.057142857142857,
      "grad_norm": 5.375524520874023,
      "learning_rate": 8.967889908256882e-05,
      "loss": 5.5147,
      "step": 450
    },
    {
      "epoch": 2.1028571428571428,
      "grad_norm": 4.890577793121338,
      "learning_rate": 8.944954128440367e-05,
      "loss": 5.4673,
      "step": 460
    },
    {
      "epoch": 2.1485714285714286,
      "grad_norm": 4.402785778045654,
      "learning_rate": 8.922018348623854e-05,
      "loss": 5.4434,
      "step": 470
    },
    {
      "epoch": 2.1942857142857144,
      "grad_norm": 4.369905471801758,
      "learning_rate": 8.89908256880734e-05,
      "loss": 5.4096,
      "step": 480
    },
    {
      "epoch": 2.24,
      "grad_norm": 4.365204334259033,
      "learning_rate": 8.876146788990825e-05,
      "loss": 5.4219,
      "step": 490
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 4.517707824707031,
      "learning_rate": 8.853211009174312e-05,
      "loss": 5.3675,
      "step": 500
    },
    {
      "epoch": 2.3314285714285714,
      "grad_norm": 3.7274880409240723,
      "learning_rate": 8.830275229357799e-05,
      "loss": 5.4508,
      "step": 510
    },
    {
      "epoch": 2.3771428571428572,
      "grad_norm": 4.621002197265625,
      "learning_rate": 8.807339449541285e-05,
      "loss": 5.4863,
      "step": 520
    },
    {
      "epoch": 2.422857142857143,
      "grad_norm": 4.385350227355957,
      "learning_rate": 8.78440366972477e-05,
      "loss": 5.4241,
      "step": 530
    },
    {
      "epoch": 2.4685714285714284,
      "grad_norm": 4.000860214233398,
      "learning_rate": 8.761467889908257e-05,
      "loss": 5.468,
      "step": 540
    },
    {
      "epoch": 2.5142857142857142,
      "grad_norm": 3.9110169410705566,
      "learning_rate": 8.738532110091744e-05,
      "loss": 5.4472,
      "step": 550
    },
    {
      "epoch": 2.56,
      "grad_norm": 4.362401962280273,
      "learning_rate": 8.715596330275229e-05,
      "loss": 5.3737,
      "step": 560
    },
    {
      "epoch": 2.605714285714286,
      "grad_norm": 4.091636657714844,
      "learning_rate": 8.692660550458716e-05,
      "loss": 5.3847,
      "step": 570
    },
    {
      "epoch": 2.6514285714285712,
      "grad_norm": 4.545400142669678,
      "learning_rate": 8.669724770642202e-05,
      "loss": 5.3562,
      "step": 580
    },
    {
      "epoch": 2.697142857142857,
      "grad_norm": 3.8049914836883545,
      "learning_rate": 8.646788990825688e-05,
      "loss": 5.4022,
      "step": 590
    },
    {
      "epoch": 2.742857142857143,
      "grad_norm": 3.939196825027466,
      "learning_rate": 8.623853211009176e-05,
      "loss": 5.5032,
      "step": 600
    },
    {
      "epoch": 2.7885714285714287,
      "grad_norm": 3.6486592292785645,
      "learning_rate": 8.600917431192661e-05,
      "loss": 5.4827,
      "step": 610
    },
    {
      "epoch": 2.8342857142857145,
      "grad_norm": 4.51725959777832,
      "learning_rate": 8.577981651376146e-05,
      "loss": 5.4077,
      "step": 620
    },
    {
      "epoch": 2.88,
      "grad_norm": 4.100818157196045,
      "learning_rate": 8.555045871559634e-05,
      "loss": 5.3898,
      "step": 630
    },
    {
      "epoch": 2.9257142857142857,
      "grad_norm": 3.4127581119537354,
      "learning_rate": 8.53211009174312e-05,
      "loss": 5.3707,
      "step": 640
    },
    {
      "epoch": 2.9714285714285715,
      "grad_norm": 4.068953990936279,
      "learning_rate": 8.509174311926605e-05,
      "loss": 5.4023,
      "step": 650
    },
    {
      "epoch": 3.0171428571428573,
      "grad_norm": 5.201420307159424,
      "learning_rate": 8.486238532110093e-05,
      "loss": 5.4602,
      "step": 660
    },
    {
      "epoch": 3.0628571428571427,
      "grad_norm": 3.7560510635375977,
      "learning_rate": 8.463302752293578e-05,
      "loss": 5.4247,
      "step": 670
    },
    {
      "epoch": 3.1085714285714285,
      "grad_norm": 4.285306930541992,
      "learning_rate": 8.440366972477065e-05,
      "loss": 5.3067,
      "step": 680
    },
    {
      "epoch": 3.1542857142857144,
      "grad_norm": 3.879349946975708,
      "learning_rate": 8.417431192660551e-05,
      "loss": 5.4513,
      "step": 690
    },
    {
      "epoch": 3.2,
      "grad_norm": 4.249948024749756,
      "learning_rate": 8.394495412844037e-05,
      "loss": 5.4525,
      "step": 700
    },
    {
      "epoch": 3.2457142857142856,
      "grad_norm": 4.378665924072266,
      "learning_rate": 8.371559633027523e-05,
      "loss": 5.4336,
      "step": 710
    },
    {
      "epoch": 3.2914285714285714,
      "grad_norm": 3.8757987022399902,
      "learning_rate": 8.34862385321101e-05,
      "loss": 5.3651,
      "step": 720
    },
    {
      "epoch": 3.337142857142857,
      "grad_norm": 4.673858642578125,
      "learning_rate": 8.325688073394495e-05,
      "loss": 5.3176,
      "step": 730
    },
    {
      "epoch": 3.382857142857143,
      "grad_norm": 4.076067924499512,
      "learning_rate": 8.302752293577982e-05,
      "loss": 5.302,
      "step": 740
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 3.6039628982543945,
      "learning_rate": 8.279816513761469e-05,
      "loss": 5.4804,
      "step": 750
    },
    {
      "epoch": 3.474285714285714,
      "grad_norm": 5.652397155761719,
      "learning_rate": 8.256880733944955e-05,
      "loss": 5.337,
      "step": 760
    },
    {
      "epoch": 3.52,
      "grad_norm": 4.671160697937012,
      "learning_rate": 8.23394495412844e-05,
      "loss": 5.3676,
      "step": 770
    },
    {
      "epoch": 3.565714285714286,
      "grad_norm": 4.187767505645752,
      "learning_rate": 8.211009174311927e-05,
      "loss": 5.4697,
      "step": 780
    },
    {
      "epoch": 3.611428571428571,
      "grad_norm": 4.642769813537598,
      "learning_rate": 8.188073394495414e-05,
      "loss": 5.3974,
      "step": 790
    },
    {
      "epoch": 3.657142857142857,
      "grad_norm": 4.37007999420166,
      "learning_rate": 8.165137614678899e-05,
      "loss": 5.3368,
      "step": 800
    },
    {
      "epoch": 3.702857142857143,
      "grad_norm": 3.4600095748901367,
      "learning_rate": 8.142201834862386e-05,
      "loss": 5.4456,
      "step": 810
    },
    {
      "epoch": 3.7485714285714287,
      "grad_norm": 3.84537672996521,
      "learning_rate": 8.119266055045872e-05,
      "loss": 5.4429,
      "step": 820
    },
    {
      "epoch": 3.7942857142857145,
      "grad_norm": 4.3511576652526855,
      "learning_rate": 8.096330275229358e-05,
      "loss": 5.3204,
      "step": 830
    },
    {
      "epoch": 3.84,
      "grad_norm": 4.081774711608887,
      "learning_rate": 8.073394495412844e-05,
      "loss": 5.4368,
      "step": 840
    },
    {
      "epoch": 3.8857142857142857,
      "grad_norm": 4.320436954498291,
      "learning_rate": 8.050458715596331e-05,
      "loss": 5.4105,
      "step": 850
    },
    {
      "epoch": 3.9314285714285715,
      "grad_norm": 4.2525224685668945,
      "learning_rate": 8.027522935779816e-05,
      "loss": 5.4111,
      "step": 860
    },
    {
      "epoch": 3.977142857142857,
      "grad_norm": 4.078608989715576,
      "learning_rate": 8.004587155963303e-05,
      "loss": 5.3413,
      "step": 870
    }
  ],
  "logging_steps": 10,
  "max_steps": 4360,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
