{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1458,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013717421124828532,
      "grad_norm": 7.102995872497559,
      "learning_rate": 0.003994513031550069,
      "loss": 7.909,
      "step": 10
    },
    {
      "epoch": 0.027434842249657063,
      "grad_norm": 5.449845790863037,
      "learning_rate": 0.003989026063100137,
      "loss": 4.2655,
      "step": 20
    },
    {
      "epoch": 0.0411522633744856,
      "grad_norm": 9.055180549621582,
      "learning_rate": 0.0039835390946502056,
      "loss": 3.5615,
      "step": 30
    },
    {
      "epoch": 0.05486968449931413,
      "grad_norm": 9.314657211303711,
      "learning_rate": 0.003978052126200274,
      "loss": 3.0856,
      "step": 40
    },
    {
      "epoch": 0.06858710562414266,
      "grad_norm": 19.166606903076172,
      "learning_rate": 0.003972565157750343,
      "loss": 2.6442,
      "step": 50
    },
    {
      "epoch": 0.0823045267489712,
      "grad_norm": 13.384329795837402,
      "learning_rate": 0.003967078189300412,
      "loss": 2.2823,
      "step": 60
    },
    {
      "epoch": 0.09602194787379972,
      "grad_norm": 16.77313995361328,
      "learning_rate": 0.00396159122085048,
      "loss": 1.9177,
      "step": 70
    },
    {
      "epoch": 0.10973936899862825,
      "grad_norm": 12.387334823608398,
      "learning_rate": 0.003956104252400549,
      "loss": 1.6896,
      "step": 80
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 14.599374771118164,
      "learning_rate": 0.003950617283950617,
      "loss": 1.6219,
      "step": 90
    },
    {
      "epoch": 0.13717421124828533,
      "grad_norm": 12.941634178161621,
      "learning_rate": 0.003945130315500686,
      "loss": 1.4573,
      "step": 100
    },
    {
      "epoch": 0.15089163237311384,
      "grad_norm": 14.008685111999512,
      "learning_rate": 0.003939643347050755,
      "loss": 1.1522,
      "step": 110
    },
    {
      "epoch": 0.1646090534979424,
      "grad_norm": 16.199888229370117,
      "learning_rate": 0.003934156378600823,
      "loss": 1.0397,
      "step": 120
    },
    {
      "epoch": 0.17832647462277093,
      "grad_norm": 18.411029815673828,
      "learning_rate": 0.003928669410150892,
      "loss": 1.1183,
      "step": 130
    },
    {
      "epoch": 0.19204389574759945,
      "grad_norm": 21.276185989379883,
      "learning_rate": 0.00392318244170096,
      "loss": 0.8733,
      "step": 140
    },
    {
      "epoch": 0.205761316872428,
      "grad_norm": 16.384662628173828,
      "learning_rate": 0.003917695473251029,
      "loss": 0.8043,
      "step": 150
    },
    {
      "epoch": 0.2194787379972565,
      "grad_norm": 17.66779327392578,
      "learning_rate": 0.003912208504801098,
      "loss": 0.6612,
      "step": 160
    },
    {
      "epoch": 0.23319615912208505,
      "grad_norm": 12.729955673217773,
      "learning_rate": 0.003906721536351166,
      "loss": 0.6627,
      "step": 170
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 10.72763442993164,
      "learning_rate": 0.0039012345679012347,
      "loss": 0.6115,
      "step": 180
    },
    {
      "epoch": 0.2606310013717421,
      "grad_norm": 19.470672607421875,
      "learning_rate": 0.0038957475994513035,
      "loss": 0.6592,
      "step": 190
    },
    {
      "epoch": 0.27434842249657065,
      "grad_norm": 9.79128360748291,
      "learning_rate": 0.003890260631001372,
      "loss": 0.4412,
      "step": 200
    },
    {
      "epoch": 0.2880658436213992,
      "grad_norm": 17.319538116455078,
      "learning_rate": 0.0038847736625514406,
      "loss": 0.4442,
      "step": 210
    },
    {
      "epoch": 0.3017832647462277,
      "grad_norm": 20.70102310180664,
      "learning_rate": 0.003879286694101509,
      "loss": 0.4374,
      "step": 220
    },
    {
      "epoch": 0.31550068587105623,
      "grad_norm": 11.593032836914062,
      "learning_rate": 0.0038737997256515777,
      "loss": 0.3548,
      "step": 230
    },
    {
      "epoch": 0.3292181069958848,
      "grad_norm": 10.380108833312988,
      "learning_rate": 0.0038683127572016465,
      "loss": 0.3424,
      "step": 240
    },
    {
      "epoch": 0.3429355281207133,
      "grad_norm": 9.524140357971191,
      "learning_rate": 0.003862825788751715,
      "loss": 0.3228,
      "step": 250
    },
    {
      "epoch": 0.35665294924554186,
      "grad_norm": 9.343277931213379,
      "learning_rate": 0.0038573388203017836,
      "loss": 0.2435,
      "step": 260
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 13.622091293334961,
      "learning_rate": 0.0038518518518518515,
      "loss": 0.2275,
      "step": 270
    },
    {
      "epoch": 0.3840877914951989,
      "grad_norm": 14.499710083007812,
      "learning_rate": 0.0038463648834019203,
      "loss": 0.3775,
      "step": 280
    },
    {
      "epoch": 0.39780521262002744,
      "grad_norm": 16.33543586730957,
      "learning_rate": 0.003840877914951989,
      "loss": 0.263,
      "step": 290
    },
    {
      "epoch": 0.411522633744856,
      "grad_norm": 10.830759048461914,
      "learning_rate": 0.0038353909465020574,
      "loss": 0.1856,
      "step": 300
    },
    {
      "epoch": 0.4252400548696845,
      "grad_norm": 6.924062728881836,
      "learning_rate": 0.0038299039780521262,
      "loss": 0.152,
      "step": 310
    },
    {
      "epoch": 0.438957475994513,
      "grad_norm": 6.442953109741211,
      "learning_rate": 0.003824417009602195,
      "loss": 0.1087,
      "step": 320
    },
    {
      "epoch": 0.45267489711934156,
      "grad_norm": 8.133803367614746,
      "learning_rate": 0.0038189300411522633,
      "loss": 0.1757,
      "step": 330
    },
    {
      "epoch": 0.4663923182441701,
      "grad_norm": 13.79932689666748,
      "learning_rate": 0.003813443072702332,
      "loss": 0.1208,
      "step": 340
    },
    {
      "epoch": 0.48010973936899864,
      "grad_norm": 10.184813499450684,
      "learning_rate": 0.0038079561042524005,
      "loss": 0.1287,
      "step": 350
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 3.2577786445617676,
      "learning_rate": 0.0038024691358024693,
      "loss": 0.1784,
      "step": 360
    },
    {
      "epoch": 0.5075445816186557,
      "grad_norm": 4.324190616607666,
      "learning_rate": 0.003796982167352538,
      "loss": 0.112,
      "step": 370
    },
    {
      "epoch": 0.5212620027434842,
      "grad_norm": 3.94382905960083,
      "learning_rate": 0.0037914951989026064,
      "loss": 0.1031,
      "step": 380
    },
    {
      "epoch": 0.5349794238683128,
      "grad_norm": 4.351624011993408,
      "learning_rate": 0.003786008230452675,
      "loss": 0.0946,
      "step": 390
    },
    {
      "epoch": 0.5486968449931413,
      "grad_norm": 0.9790947437286377,
      "learning_rate": 0.0037805212620027435,
      "loss": 0.1055,
      "step": 400
    },
    {
      "epoch": 0.5624142661179699,
      "grad_norm": 11.866278648376465,
      "learning_rate": 0.0037750342935528123,
      "loss": 0.0813,
      "step": 410
    },
    {
      "epoch": 0.5761316872427984,
      "grad_norm": 4.954287052154541,
      "learning_rate": 0.003769547325102881,
      "loss": 0.1122,
      "step": 420
    },
    {
      "epoch": 0.5898491083676269,
      "grad_norm": 3.6281094551086426,
      "learning_rate": 0.0037640603566529494,
      "loss": 0.0781,
      "step": 430
    },
    {
      "epoch": 0.6035665294924554,
      "grad_norm": 4.920687675476074,
      "learning_rate": 0.0037585733882030178,
      "loss": 0.1171,
      "step": 440
    },
    {
      "epoch": 0.6172839506172839,
      "grad_norm": 7.36521577835083,
      "learning_rate": 0.003753086419753086,
      "loss": 0.1346,
      "step": 450
    },
    {
      "epoch": 0.6310013717421125,
      "grad_norm": 3.5788347721099854,
      "learning_rate": 0.003747599451303155,
      "loss": 0.088,
      "step": 460
    },
    {
      "epoch": 0.644718792866941,
      "grad_norm": 3.4002041816711426,
      "learning_rate": 0.0037421124828532237,
      "loss": 0.0826,
      "step": 470
    },
    {
      "epoch": 0.6584362139917695,
      "grad_norm": 2.4556453227996826,
      "learning_rate": 0.003736625514403292,
      "loss": 0.0534,
      "step": 480
    },
    {
      "epoch": 0.6721536351165981,
      "grad_norm": 2.1229560375213623,
      "learning_rate": 0.003731138545953361,
      "loss": 0.0935,
      "step": 490
    },
    {
      "epoch": 0.6858710562414266,
      "grad_norm": 1.7936378717422485,
      "learning_rate": 0.0037256515775034296,
      "loss": 0.074,
      "step": 500
    },
    {
      "epoch": 0.6995884773662552,
      "grad_norm": 4.88102388381958,
      "learning_rate": 0.003720164609053498,
      "loss": 0.0567,
      "step": 510
    },
    {
      "epoch": 0.7133058984910837,
      "grad_norm": 2.9650697708129883,
      "learning_rate": 0.0037146776406035667,
      "loss": 0.042,
      "step": 520
    },
    {
      "epoch": 0.7270233196159122,
      "grad_norm": 3.334686040878296,
      "learning_rate": 0.003709190672153635,
      "loss": 0.085,
      "step": 530
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 10.097945213317871,
      "learning_rate": 0.003703703703703704,
      "loss": 0.0608,
      "step": 540
    },
    {
      "epoch": 0.7544581618655692,
      "grad_norm": 5.526248931884766,
      "learning_rate": 0.0036982167352537726,
      "loss": 0.0475,
      "step": 550
    },
    {
      "epoch": 0.7681755829903978,
      "grad_norm": 2.3787221908569336,
      "learning_rate": 0.003692729766803841,
      "loss": 0.0489,
      "step": 560
    },
    {
      "epoch": 0.7818930041152263,
      "grad_norm": 9.216018676757812,
      "learning_rate": 0.0036872427983539098,
      "loss": 0.2081,
      "step": 570
    },
    {
      "epoch": 0.7956104252400549,
      "grad_norm": 3.5417046546936035,
      "learning_rate": 0.003681755829903978,
      "loss": 0.0483,
      "step": 580
    },
    {
      "epoch": 0.8093278463648834,
      "grad_norm": 2.9877257347106934,
      "learning_rate": 0.003676268861454047,
      "loss": 0.0465,
      "step": 590
    },
    {
      "epoch": 0.823045267489712,
      "grad_norm": 2.1434154510498047,
      "learning_rate": 0.0036707818930041157,
      "loss": 0.0446,
      "step": 600
    },
    {
      "epoch": 0.8367626886145405,
      "grad_norm": 0.45098498463630676,
      "learning_rate": 0.0036652949245541836,
      "loss": 0.0276,
      "step": 610
    },
    {
      "epoch": 0.850480109739369,
      "grad_norm": 1.5228772163391113,
      "learning_rate": 0.0036598079561042524,
      "loss": 0.0393,
      "step": 620
    },
    {
      "epoch": 0.8641975308641975,
      "grad_norm": 3.817784070968628,
      "learning_rate": 0.0036543209876543207,
      "loss": 0.0506,
      "step": 630
    },
    {
      "epoch": 0.877914951989026,
      "grad_norm": 1.6846778392791748,
      "learning_rate": 0.0036488340192043895,
      "loss": 0.0483,
      "step": 640
    },
    {
      "epoch": 0.8916323731138546,
      "grad_norm": 0.6752075552940369,
      "learning_rate": 0.0036433470507544583,
      "loss": 0.0302,
      "step": 650
    },
    {
      "epoch": 0.9053497942386831,
      "grad_norm": 0.5913817882537842,
      "learning_rate": 0.0036378600823045266,
      "loss": 0.0677,
      "step": 660
    },
    {
      "epoch": 0.9190672153635117,
      "grad_norm": 0.4712817370891571,
      "learning_rate": 0.0036323731138545954,
      "loss": 0.0311,
      "step": 670
    },
    {
      "epoch": 0.9327846364883402,
      "grad_norm": 0.5882495641708374,
      "learning_rate": 0.003626886145404664,
      "loss": 0.0364,
      "step": 680
    },
    {
      "epoch": 0.9465020576131687,
      "grad_norm": 7.433635234832764,
      "learning_rate": 0.0036213991769547325,
      "loss": 0.0494,
      "step": 690
    },
    {
      "epoch": 0.9602194787379973,
      "grad_norm": 3.19130802154541,
      "learning_rate": 0.0036159122085048013,
      "loss": 0.0379,
      "step": 700
    },
    {
      "epoch": 0.9739368998628258,
      "grad_norm": 1.097072720527649,
      "learning_rate": 0.0036104252400548697,
      "loss": 0.0248,
      "step": 710
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 3.953758955001831,
      "learning_rate": 0.0036049382716049384,
      "loss": 0.0362,
      "step": 720
    },
    {
      "epoch": 1.0013717421124828,
      "grad_norm": 1.9627350568771362,
      "learning_rate": 0.003599451303155007,
      "loss": 0.0169,
      "step": 730
    },
    {
      "epoch": 1.0150891632373114,
      "grad_norm": 7.423328876495361,
      "learning_rate": 0.0035939643347050756,
      "loss": 0.0419,
      "step": 740
    },
    {
      "epoch": 1.02880658436214,
      "grad_norm": 1.5784693956375122,
      "learning_rate": 0.0035884773662551443,
      "loss": 0.0295,
      "step": 750
    },
    {
      "epoch": 1.0425240054869684,
      "grad_norm": 1.6370735168457031,
      "learning_rate": 0.0035829903978052127,
      "loss": 0.0295,
      "step": 760
    },
    {
      "epoch": 1.056241426611797,
      "grad_norm": 1.974791407585144,
      "learning_rate": 0.0035775034293552815,
      "loss": 0.0357,
      "step": 770
    },
    {
      "epoch": 1.0699588477366255,
      "grad_norm": 3.13201904296875,
      "learning_rate": 0.0035720164609053503,
      "loss": 0.0297,
      "step": 780
    },
    {
      "epoch": 1.083676268861454,
      "grad_norm": 2.4929585456848145,
      "learning_rate": 0.003566529492455418,
      "loss": 0.0502,
      "step": 790
    },
    {
      "epoch": 1.0973936899862826,
      "grad_norm": 2.195676803588867,
      "learning_rate": 0.003561042524005487,
      "loss": 0.0333,
      "step": 800
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 3.32153582572937,
      "learning_rate": 0.0035555555555555553,
      "loss": 0.0288,
      "step": 810
    },
    {
      "epoch": 1.1248285322359397,
      "grad_norm": 0.33802416920661926,
      "learning_rate": 0.003550068587105624,
      "loss": 0.0299,
      "step": 820
    },
    {
      "epoch": 1.1385459533607682,
      "grad_norm": 3.0308303833007812,
      "learning_rate": 0.003544581618655693,
      "loss": 0.0136,
      "step": 830
    },
    {
      "epoch": 1.1522633744855968,
      "grad_norm": 0.5001335144042969,
      "learning_rate": 0.003539094650205761,
      "loss": 0.0166,
      "step": 840
    },
    {
      "epoch": 1.1659807956104253,
      "grad_norm": 1.3040770292282104,
      "learning_rate": 0.00353360768175583,
      "loss": 0.0221,
      "step": 850
    },
    {
      "epoch": 1.1796982167352539,
      "grad_norm": 1.849737286567688,
      "learning_rate": 0.0035281207133058983,
      "loss": 0.0176,
      "step": 860
    },
    {
      "epoch": 1.1934156378600824,
      "grad_norm": 3.2244155406951904,
      "learning_rate": 0.003522633744855967,
      "loss": 0.0261,
      "step": 870
    },
    {
      "epoch": 1.2071330589849107,
      "grad_norm": 5.836394786834717,
      "learning_rate": 0.003517146776406036,
      "loss": 0.0257,
      "step": 880
    },
    {
      "epoch": 1.2208504801097393,
      "grad_norm": 1.5565156936645508,
      "learning_rate": 0.0035116598079561042,
      "loss": 0.0308,
      "step": 890
    },
    {
      "epoch": 1.2345679012345678,
      "grad_norm": 0.5373246669769287,
      "learning_rate": 0.003506172839506173,
      "loss": 0.0073,
      "step": 900
    },
    {
      "epoch": 1.2482853223593964,
      "grad_norm": 0.11659617722034454,
      "learning_rate": 0.003500685871056242,
      "loss": 0.0035,
      "step": 910
    },
    {
      "epoch": 1.262002743484225,
      "grad_norm": 0.24629347026348114,
      "learning_rate": 0.00349519890260631,
      "loss": 0.0041,
      "step": 920
    },
    {
      "epoch": 1.2757201646090535,
      "grad_norm": 0.1418679803609848,
      "learning_rate": 0.003489711934156379,
      "loss": 0.003,
      "step": 930
    },
    {
      "epoch": 1.289437585733882,
      "grad_norm": 2.758559465408325,
      "learning_rate": 0.0034842249657064473,
      "loss": 0.0081,
      "step": 940
    },
    {
      "epoch": 1.3031550068587106,
      "grad_norm": 4.704490661621094,
      "learning_rate": 0.003478737997256516,
      "loss": 0.0327,
      "step": 950
    },
    {
      "epoch": 1.316872427983539,
      "grad_norm": 1.6501725912094116,
      "learning_rate": 0.0034732510288065844,
      "loss": 0.0142,
      "step": 960
    },
    {
      "epoch": 1.3305898491083676,
      "grad_norm": 3.161900043487549,
      "learning_rate": 0.0034677640603566528,
      "loss": 0.0149,
      "step": 970
    },
    {
      "epoch": 1.3443072702331962,
      "grad_norm": 2.3209738731384277,
      "learning_rate": 0.0034622770919067215,
      "loss": 0.0193,
      "step": 980
    },
    {
      "epoch": 1.3580246913580247,
      "grad_norm": 0.1960245668888092,
      "learning_rate": 0.00345679012345679,
      "loss": 0.0086,
      "step": 990
    },
    {
      "epoch": 1.3717421124828533,
      "grad_norm": 1.492215633392334,
      "learning_rate": 0.0034513031550068587,
      "loss": 0.0107,
      "step": 1000
    },
    {
      "epoch": 1.3854595336076818,
      "grad_norm": 0.2733677327632904,
      "learning_rate": 0.0034458161865569274,
      "loss": 0.0117,
      "step": 1010
    },
    {
      "epoch": 1.3991769547325104,
      "grad_norm": 1.0395753383636475,
      "learning_rate": 0.003440329218106996,
      "loss": 0.0105,
      "step": 1020
    },
    {
      "epoch": 1.412894375857339,
      "grad_norm": 2.691480875015259,
      "learning_rate": 0.0034348422496570646,
      "loss": 0.0158,
      "step": 1030
    },
    {
      "epoch": 1.4266117969821672,
      "grad_norm": 2.866596221923828,
      "learning_rate": 0.003429355281207133,
      "loss": 0.0213,
      "step": 1040
    },
    {
      "epoch": 1.4403292181069958,
      "grad_norm": 0.7269473671913147,
      "learning_rate": 0.0034238683127572017,
      "loss": 0.0124,
      "step": 1050
    },
    {
      "epoch": 1.4540466392318243,
      "grad_norm": 3.53381609916687,
      "learning_rate": 0.0034183813443072705,
      "loss": 0.0154,
      "step": 1060
    },
    {
      "epoch": 1.4677640603566529,
      "grad_norm": 5.748310565948486,
      "learning_rate": 0.003412894375857339,
      "loss": 0.0259,
      "step": 1070
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 2.57140851020813,
      "learning_rate": 0.0034074074074074076,
      "loss": 0.0155,
      "step": 1080
    },
    {
      "epoch": 1.49519890260631,
      "grad_norm": 1.7304370403289795,
      "learning_rate": 0.0034019204389574764,
      "loss": 0.0244,
      "step": 1090
    },
    {
      "epoch": 1.5089163237311385,
      "grad_norm": 0.2701631486415863,
      "learning_rate": 0.0033964334705075447,
      "loss": 0.0079,
      "step": 1100
    },
    {
      "epoch": 1.522633744855967,
      "grad_norm": 0.058133356273174286,
      "learning_rate": 0.0033909465020576135,
      "loss": 0.006,
      "step": 1110
    },
    {
      "epoch": 1.5363511659807956,
      "grad_norm": 0.13815462589263916,
      "learning_rate": 0.003385459533607682,
      "loss": 0.0072,
      "step": 1120
    },
    {
      "epoch": 1.5500685871056241,
      "grad_norm": 0.10726580023765564,
      "learning_rate": 0.00337997256515775,
      "loss": 0.003,
      "step": 1130
    },
    {
      "epoch": 1.5637860082304527,
      "grad_norm": 0.07423245161771774,
      "learning_rate": 0.003374485596707819,
      "loss": 0.0025,
      "step": 1140
    },
    {
      "epoch": 1.5775034293552812,
      "grad_norm": 0.042405132204294205,
      "learning_rate": 0.0033689986282578873,
      "loss": 0.0029,
      "step": 1150
    },
    {
      "epoch": 1.5912208504801097,
      "grad_norm": 0.15457452833652496,
      "learning_rate": 0.003363511659807956,
      "loss": 0.0038,
      "step": 1160
    },
    {
      "epoch": 1.6049382716049383,
      "grad_norm": 0.19390183687210083,
      "learning_rate": 0.0033580246913580245,
      "loss": 0.0037,
      "step": 1170
    },
    {
      "epoch": 1.6186556927297668,
      "grad_norm": 0.5642995834350586,
      "learning_rate": 0.0033525377229080932,
      "loss": 0.0021,
      "step": 1180
    },
    {
      "epoch": 1.6323731138545954,
      "grad_norm": 0.041702333837747574,
      "learning_rate": 0.003347050754458162,
      "loss": 0.0014,
      "step": 1190
    },
    {
      "epoch": 1.646090534979424,
      "grad_norm": 1.3787133693695068,
      "learning_rate": 0.0033415637860082304,
      "loss": 0.0023,
      "step": 1200
    },
    {
      "epoch": 1.6598079561042525,
      "grad_norm": 0.3663009703159332,
      "learning_rate": 0.003336076817558299,
      "loss": 0.0022,
      "step": 1210
    },
    {
      "epoch": 1.673525377229081,
      "grad_norm": 0.029018864035606384,
      "learning_rate": 0.0033305898491083675,
      "loss": 0.0012,
      "step": 1220
    },
    {
      "epoch": 1.6872427983539096,
      "grad_norm": 0.030573470517992973,
      "learning_rate": 0.0033251028806584363,
      "loss": 0.0011,
      "step": 1230
    },
    {
      "epoch": 1.700960219478738,
      "grad_norm": 0.019351551309227943,
      "learning_rate": 0.003319615912208505,
      "loss": 0.0008,
      "step": 1240
    },
    {
      "epoch": 1.7146776406035666,
      "grad_norm": 0.06518267095088959,
      "learning_rate": 0.0033141289437585734,
      "loss": 0.0007,
      "step": 1250
    },
    {
      "epoch": 1.7283950617283952,
      "grad_norm": 0.007179753389209509,
      "learning_rate": 0.003308641975308642,
      "loss": 0.0005,
      "step": 1260
    },
    {
      "epoch": 1.7421124828532237,
      "grad_norm": 0.004783597309142351,
      "learning_rate": 0.003303155006858711,
      "loss": 0.0005,
      "step": 1270
    },
    {
      "epoch": 1.7558299039780523,
      "grad_norm": 0.006074214819818735,
      "learning_rate": 0.0032976680384087793,
      "loss": 0.0004,
      "step": 1280
    },
    {
      "epoch": 1.7695473251028808,
      "grad_norm": 0.21831804513931274,
      "learning_rate": 0.003292181069958848,
      "loss": 0.0006,
      "step": 1290
    },
    {
      "epoch": 1.7832647462277091,
      "grad_norm": 0.008676720783114433,
      "learning_rate": 0.0032866941015089165,
      "loss": 0.0005,
      "step": 1300
    },
    {
      "epoch": 1.7969821673525377,
      "grad_norm": 0.018993107602000237,
      "learning_rate": 0.003281207133058985,
      "loss": 0.0004,
      "step": 1310
    },
    {
      "epoch": 1.8106995884773662,
      "grad_norm": 0.01370448712259531,
      "learning_rate": 0.0032757201646090536,
      "loss": 0.0004,
      "step": 1320
    },
    {
      "epoch": 1.8244170096021948,
      "grad_norm": 0.0071778870187699795,
      "learning_rate": 0.003270233196159122,
      "loss": 0.0004,
      "step": 1330
    },
    {
      "epoch": 1.8381344307270233,
      "grad_norm": 0.004453766159713268,
      "learning_rate": 0.0032647462277091907,
      "loss": 0.0005,
      "step": 1340
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 0.010501434095203876,
      "learning_rate": 0.003259259259259259,
      "loss": 0.0003,
      "step": 1350
    },
    {
      "epoch": 1.8655692729766804,
      "grad_norm": 0.003696098690852523,
      "learning_rate": 0.003253772290809328,
      "loss": 0.0003,
      "step": 1360
    },
    {
      "epoch": 1.879286694101509,
      "grad_norm": 0.006534223910421133,
      "learning_rate": 0.0032482853223593966,
      "loss": 0.0059,
      "step": 1370
    },
    {
      "epoch": 1.8930041152263375,
      "grad_norm": 0.1527172178030014,
      "learning_rate": 0.003242798353909465,
      "loss": 0.0052,
      "step": 1380
    },
    {
      "epoch": 1.906721536351166,
      "grad_norm": 0.31320658326148987,
      "learning_rate": 0.0032373113854595337,
      "loss": 0.0043,
      "step": 1390
    },
    {
      "epoch": 1.9204389574759944,
      "grad_norm": 2.9821789264678955,
      "learning_rate": 0.003231824417009602,
      "loss": 0.0075,
      "step": 1400
    },
    {
      "epoch": 1.934156378600823,
      "grad_norm": 0.3538370430469513,
      "learning_rate": 0.003226337448559671,
      "loss": 0.0082,
      "step": 1410
    },
    {
      "epoch": 1.9478737997256514,
      "grad_norm": 0.32395270466804504,
      "learning_rate": 0.0032208504801097397,
      "loss": 0.0035,
      "step": 1420
    },
    {
      "epoch": 1.96159122085048,
      "grad_norm": 2.4334521293640137,
      "learning_rate": 0.003215363511659808,
      "loss": 0.003,
      "step": 1430
    },
    {
      "epoch": 1.9753086419753085,
      "grad_norm": 17.39132308959961,
      "learning_rate": 0.0032098765432098768,
      "loss": 0.0242,
      "step": 1440
    },
    {
      "epoch": 1.989026063100137,
      "grad_norm": 3.919358491897583,
      "learning_rate": 0.0032043895747599456,
      "loss": 0.0182,
      "step": 1450
    }
  ],
  "logging_steps": 10,
  "max_steps": 7290,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
