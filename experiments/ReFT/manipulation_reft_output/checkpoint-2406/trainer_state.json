{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.998857142857142,
  "eval_steps": 500,
  "global_step": 2406,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.045714285714285714,
      "grad_norm": 8.690507888793945,
      "learning_rate": 9.977064220183486e-05,
      "loss": 10.3159,
      "step": 10
    },
    {
      "epoch": 0.09142857142857143,
      "grad_norm": 4.060633182525635,
      "learning_rate": 9.954128440366974e-05,
      "loss": 6.3988,
      "step": 20
    },
    {
      "epoch": 0.13714285714285715,
      "grad_norm": 4.209723949432373,
      "learning_rate": 9.931192660550459e-05,
      "loss": 6.1603,
      "step": 30
    },
    {
      "epoch": 0.18285714285714286,
      "grad_norm": 3.8396549224853516,
      "learning_rate": 9.908256880733946e-05,
      "loss": 5.961,
      "step": 40
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 4.556296348571777,
      "learning_rate": 9.885321100917432e-05,
      "loss": 5.9653,
      "step": 50
    },
    {
      "epoch": 0.2742857142857143,
      "grad_norm": 4.246315956115723,
      "learning_rate": 9.862385321100918e-05,
      "loss": 5.7879,
      "step": 60
    },
    {
      "epoch": 0.32,
      "grad_norm": 4.510865211486816,
      "learning_rate": 9.839449541284404e-05,
      "loss": 5.867,
      "step": 70
    },
    {
      "epoch": 0.3657142857142857,
      "grad_norm": 3.938235282897949,
      "learning_rate": 9.816513761467891e-05,
      "loss": 5.819,
      "step": 80
    },
    {
      "epoch": 0.4114285714285714,
      "grad_norm": 3.643468141555786,
      "learning_rate": 9.793577981651376e-05,
      "loss": 5.7243,
      "step": 90
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 3.7212650775909424,
      "learning_rate": 9.770642201834863e-05,
      "loss": 5.7096,
      "step": 100
    },
    {
      "epoch": 0.5028571428571429,
      "grad_norm": 3.82885479927063,
      "learning_rate": 9.74770642201835e-05,
      "loss": 5.689,
      "step": 110
    },
    {
      "epoch": 0.5485714285714286,
      "grad_norm": 4.126797676086426,
      "learning_rate": 9.724770642201836e-05,
      "loss": 5.6834,
      "step": 120
    },
    {
      "epoch": 0.5942857142857143,
      "grad_norm": 4.498561382293701,
      "learning_rate": 9.701834862385321e-05,
      "loss": 5.6837,
      "step": 130
    },
    {
      "epoch": 0.64,
      "grad_norm": 3.5291748046875,
      "learning_rate": 9.678899082568808e-05,
      "loss": 5.6049,
      "step": 140
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 4.003115177154541,
      "learning_rate": 9.655963302752295e-05,
      "loss": 5.6886,
      "step": 150
    },
    {
      "epoch": 0.7314285714285714,
      "grad_norm": 3.8343048095703125,
      "learning_rate": 9.63302752293578e-05,
      "loss": 5.5523,
      "step": 160
    },
    {
      "epoch": 0.7771428571428571,
      "grad_norm": 4.415511131286621,
      "learning_rate": 9.610091743119267e-05,
      "loss": 5.5251,
      "step": 170
    },
    {
      "epoch": 0.8228571428571428,
      "grad_norm": 3.9554286003112793,
      "learning_rate": 9.587155963302753e-05,
      "loss": 5.5614,
      "step": 180
    },
    {
      "epoch": 0.8685714285714285,
      "grad_norm": 4.694336414337158,
      "learning_rate": 9.564220183486238e-05,
      "loss": 5.651,
      "step": 190
    },
    {
      "epoch": 0.9142857142857143,
      "grad_norm": 4.5557942390441895,
      "learning_rate": 9.541284403669725e-05,
      "loss": 5.5208,
      "step": 200
    },
    {
      "epoch": 0.96,
      "grad_norm": 3.2789740562438965,
      "learning_rate": 9.518348623853212e-05,
      "loss": 5.4728,
      "step": 210
    },
    {
      "epoch": 1.0057142857142858,
      "grad_norm": 4.646069526672363,
      "learning_rate": 9.495412844036697e-05,
      "loss": 5.5467,
      "step": 220
    },
    {
      "epoch": 1.0514285714285714,
      "grad_norm": 4.240537166595459,
      "learning_rate": 9.472477064220184e-05,
      "loss": 5.5378,
      "step": 230
    },
    {
      "epoch": 1.0971428571428572,
      "grad_norm": 3.5913121700286865,
      "learning_rate": 9.44954128440367e-05,
      "loss": 5.4913,
      "step": 240
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 3.4259543418884277,
      "learning_rate": 9.426605504587156e-05,
      "loss": 5.5671,
      "step": 250
    },
    {
      "epoch": 1.1885714285714286,
      "grad_norm": 3.788799285888672,
      "learning_rate": 9.403669724770642e-05,
      "loss": 5.5992,
      "step": 260
    },
    {
      "epoch": 1.2342857142857142,
      "grad_norm": 4.091202735900879,
      "learning_rate": 9.380733944954129e-05,
      "loss": 5.5042,
      "step": 270
    },
    {
      "epoch": 1.28,
      "grad_norm": 4.819816589355469,
      "learning_rate": 9.357798165137616e-05,
      "loss": 5.5506,
      "step": 280
    },
    {
      "epoch": 1.3257142857142856,
      "grad_norm": 4.1068243980407715,
      "learning_rate": 9.334862385321101e-05,
      "loss": 5.4103,
      "step": 290
    },
    {
      "epoch": 1.3714285714285714,
      "grad_norm": 3.874763011932373,
      "learning_rate": 9.311926605504587e-05,
      "loss": 5.5472,
      "step": 300
    },
    {
      "epoch": 1.4171428571428573,
      "grad_norm": 4.296212673187256,
      "learning_rate": 9.288990825688074e-05,
      "loss": 5.4604,
      "step": 310
    },
    {
      "epoch": 1.4628571428571429,
      "grad_norm": 4.06827974319458,
      "learning_rate": 9.266055045871561e-05,
      "loss": 5.5135,
      "step": 320
    },
    {
      "epoch": 1.5085714285714285,
      "grad_norm": 4.138288497924805,
      "learning_rate": 9.243119266055046e-05,
      "loss": 5.5174,
      "step": 330
    },
    {
      "epoch": 1.5542857142857143,
      "grad_norm": 4.608002185821533,
      "learning_rate": 9.220183486238533e-05,
      "loss": 5.5935,
      "step": 340
    },
    {
      "epoch": 1.6,
      "grad_norm": 4.3186845779418945,
      "learning_rate": 9.197247706422019e-05,
      "loss": 5.5827,
      "step": 350
    },
    {
      "epoch": 1.6457142857142857,
      "grad_norm": 4.5327253341674805,
      "learning_rate": 9.174311926605506e-05,
      "loss": 5.531,
      "step": 360
    },
    {
      "epoch": 1.6914285714285713,
      "grad_norm": 4.0583319664001465,
      "learning_rate": 9.151376146788991e-05,
      "loss": 5.4387,
      "step": 370
    },
    {
      "epoch": 1.737142857142857,
      "grad_norm": 4.766531944274902,
      "learning_rate": 9.128440366972478e-05,
      "loss": 5.5131,
      "step": 380
    },
    {
      "epoch": 1.782857142857143,
      "grad_norm": 3.6075353622436523,
      "learning_rate": 9.105504587155964e-05,
      "loss": 5.4429,
      "step": 390
    },
    {
      "epoch": 1.8285714285714287,
      "grad_norm": 3.409393787384033,
      "learning_rate": 9.08256880733945e-05,
      "loss": 5.5295,
      "step": 400
    },
    {
      "epoch": 1.8742857142857143,
      "grad_norm": 4.079815864562988,
      "learning_rate": 9.059633027522936e-05,
      "loss": 5.424,
      "step": 410
    },
    {
      "epoch": 1.92,
      "grad_norm": 3.615144729614258,
      "learning_rate": 9.036697247706423e-05,
      "loss": 5.4586,
      "step": 420
    },
    {
      "epoch": 1.9657142857142857,
      "grad_norm": 3.831975221633911,
      "learning_rate": 9.013761467889908e-05,
      "loss": 5.4219,
      "step": 430
    },
    {
      "epoch": 2.0114285714285716,
      "grad_norm": 4.201025009155273,
      "learning_rate": 8.990825688073395e-05,
      "loss": 5.5265,
      "step": 440
    },
    {
      "epoch": 2.057142857142857,
      "grad_norm": 5.375524520874023,
      "learning_rate": 8.967889908256882e-05,
      "loss": 5.5147,
      "step": 450
    },
    {
      "epoch": 2.1028571428571428,
      "grad_norm": 4.890577793121338,
      "learning_rate": 8.944954128440367e-05,
      "loss": 5.4673,
      "step": 460
    },
    {
      "epoch": 2.1485714285714286,
      "grad_norm": 4.402785778045654,
      "learning_rate": 8.922018348623854e-05,
      "loss": 5.4434,
      "step": 470
    },
    {
      "epoch": 2.1942857142857144,
      "grad_norm": 4.369905471801758,
      "learning_rate": 8.89908256880734e-05,
      "loss": 5.4096,
      "step": 480
    },
    {
      "epoch": 2.24,
      "grad_norm": 4.365204334259033,
      "learning_rate": 8.876146788990825e-05,
      "loss": 5.4219,
      "step": 490
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 4.517707824707031,
      "learning_rate": 8.853211009174312e-05,
      "loss": 5.3675,
      "step": 500
    },
    {
      "epoch": 2.3314285714285714,
      "grad_norm": 3.7274880409240723,
      "learning_rate": 8.830275229357799e-05,
      "loss": 5.4508,
      "step": 510
    },
    {
      "epoch": 2.3771428571428572,
      "grad_norm": 4.621002197265625,
      "learning_rate": 8.807339449541285e-05,
      "loss": 5.4863,
      "step": 520
    },
    {
      "epoch": 2.422857142857143,
      "grad_norm": 4.385350227355957,
      "learning_rate": 8.78440366972477e-05,
      "loss": 5.4241,
      "step": 530
    },
    {
      "epoch": 2.4685714285714284,
      "grad_norm": 4.000860214233398,
      "learning_rate": 8.761467889908257e-05,
      "loss": 5.468,
      "step": 540
    },
    {
      "epoch": 2.5142857142857142,
      "grad_norm": 3.9110169410705566,
      "learning_rate": 8.738532110091744e-05,
      "loss": 5.4472,
      "step": 550
    },
    {
      "epoch": 2.56,
      "grad_norm": 4.362401962280273,
      "learning_rate": 8.715596330275229e-05,
      "loss": 5.3737,
      "step": 560
    },
    {
      "epoch": 2.605714285714286,
      "grad_norm": 4.091636657714844,
      "learning_rate": 8.692660550458716e-05,
      "loss": 5.3847,
      "step": 570
    },
    {
      "epoch": 2.6514285714285712,
      "grad_norm": 4.545400142669678,
      "learning_rate": 8.669724770642202e-05,
      "loss": 5.3562,
      "step": 580
    },
    {
      "epoch": 2.697142857142857,
      "grad_norm": 3.8049914836883545,
      "learning_rate": 8.646788990825688e-05,
      "loss": 5.4022,
      "step": 590
    },
    {
      "epoch": 2.742857142857143,
      "grad_norm": 3.939196825027466,
      "learning_rate": 8.623853211009176e-05,
      "loss": 5.5032,
      "step": 600
    },
    {
      "epoch": 2.7885714285714287,
      "grad_norm": 3.6486592292785645,
      "learning_rate": 8.600917431192661e-05,
      "loss": 5.4827,
      "step": 610
    },
    {
      "epoch": 2.8342857142857145,
      "grad_norm": 4.51725959777832,
      "learning_rate": 8.577981651376146e-05,
      "loss": 5.4077,
      "step": 620
    },
    {
      "epoch": 2.88,
      "grad_norm": 4.100818157196045,
      "learning_rate": 8.555045871559634e-05,
      "loss": 5.3898,
      "step": 630
    },
    {
      "epoch": 2.9257142857142857,
      "grad_norm": 3.4127581119537354,
      "learning_rate": 8.53211009174312e-05,
      "loss": 5.3707,
      "step": 640
    },
    {
      "epoch": 2.9714285714285715,
      "grad_norm": 4.068953990936279,
      "learning_rate": 8.509174311926605e-05,
      "loss": 5.4023,
      "step": 650
    },
    {
      "epoch": 3.0171428571428573,
      "grad_norm": 5.201420307159424,
      "learning_rate": 8.486238532110093e-05,
      "loss": 5.4602,
      "step": 660
    },
    {
      "epoch": 3.0628571428571427,
      "grad_norm": 3.7560510635375977,
      "learning_rate": 8.463302752293578e-05,
      "loss": 5.4247,
      "step": 670
    },
    {
      "epoch": 3.1085714285714285,
      "grad_norm": 4.285306930541992,
      "learning_rate": 8.440366972477065e-05,
      "loss": 5.3067,
      "step": 680
    },
    {
      "epoch": 3.1542857142857144,
      "grad_norm": 3.879349946975708,
      "learning_rate": 8.417431192660551e-05,
      "loss": 5.4513,
      "step": 690
    },
    {
      "epoch": 3.2,
      "grad_norm": 4.249948024749756,
      "learning_rate": 8.394495412844037e-05,
      "loss": 5.4525,
      "step": 700
    },
    {
      "epoch": 3.2457142857142856,
      "grad_norm": 4.378665924072266,
      "learning_rate": 8.371559633027523e-05,
      "loss": 5.4336,
      "step": 710
    },
    {
      "epoch": 3.2914285714285714,
      "grad_norm": 3.8757987022399902,
      "learning_rate": 8.34862385321101e-05,
      "loss": 5.3651,
      "step": 720
    },
    {
      "epoch": 3.337142857142857,
      "grad_norm": 4.673858642578125,
      "learning_rate": 8.325688073394495e-05,
      "loss": 5.3176,
      "step": 730
    },
    {
      "epoch": 3.382857142857143,
      "grad_norm": 4.076067924499512,
      "learning_rate": 8.302752293577982e-05,
      "loss": 5.302,
      "step": 740
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 3.6039628982543945,
      "learning_rate": 8.279816513761469e-05,
      "loss": 5.4804,
      "step": 750
    },
    {
      "epoch": 3.474285714285714,
      "grad_norm": 5.652397155761719,
      "learning_rate": 8.256880733944955e-05,
      "loss": 5.337,
      "step": 760
    },
    {
      "epoch": 3.52,
      "grad_norm": 4.671160697937012,
      "learning_rate": 8.23394495412844e-05,
      "loss": 5.3676,
      "step": 770
    },
    {
      "epoch": 3.565714285714286,
      "grad_norm": 4.187767505645752,
      "learning_rate": 8.211009174311927e-05,
      "loss": 5.4697,
      "step": 780
    },
    {
      "epoch": 3.611428571428571,
      "grad_norm": 4.642769813537598,
      "learning_rate": 8.188073394495414e-05,
      "loss": 5.3974,
      "step": 790
    },
    {
      "epoch": 3.657142857142857,
      "grad_norm": 4.37007999420166,
      "learning_rate": 8.165137614678899e-05,
      "loss": 5.3368,
      "step": 800
    },
    {
      "epoch": 3.702857142857143,
      "grad_norm": 3.4600095748901367,
      "learning_rate": 8.142201834862386e-05,
      "loss": 5.4456,
      "step": 810
    },
    {
      "epoch": 3.7485714285714287,
      "grad_norm": 3.84537672996521,
      "learning_rate": 8.119266055045872e-05,
      "loss": 5.4429,
      "step": 820
    },
    {
      "epoch": 3.7942857142857145,
      "grad_norm": 4.3511576652526855,
      "learning_rate": 8.096330275229358e-05,
      "loss": 5.3204,
      "step": 830
    },
    {
      "epoch": 3.84,
      "grad_norm": 4.081774711608887,
      "learning_rate": 8.073394495412844e-05,
      "loss": 5.4368,
      "step": 840
    },
    {
      "epoch": 3.8857142857142857,
      "grad_norm": 4.320436954498291,
      "learning_rate": 8.050458715596331e-05,
      "loss": 5.4105,
      "step": 850
    },
    {
      "epoch": 3.9314285714285715,
      "grad_norm": 4.2525224685668945,
      "learning_rate": 8.027522935779816e-05,
      "loss": 5.4111,
      "step": 860
    },
    {
      "epoch": 3.977142857142857,
      "grad_norm": 4.078608989715576,
      "learning_rate": 8.004587155963303e-05,
      "loss": 5.3413,
      "step": 870
    },
    {
      "epoch": 4.022857142857143,
      "grad_norm": 4.022980690002441,
      "learning_rate": 7.98165137614679e-05,
      "loss": 5.3254,
      "step": 880
    },
    {
      "epoch": 4.0685714285714285,
      "grad_norm": 3.7108235359191895,
      "learning_rate": 7.958715596330275e-05,
      "loss": 5.2567,
      "step": 890
    },
    {
      "epoch": 4.114285714285714,
      "grad_norm": 3.995485782623291,
      "learning_rate": 7.935779816513761e-05,
      "loss": 5.3732,
      "step": 900
    },
    {
      "epoch": 4.16,
      "grad_norm": 4.704823017120361,
      "learning_rate": 7.912844036697248e-05,
      "loss": 5.2692,
      "step": 910
    },
    {
      "epoch": 4.2057142857142855,
      "grad_norm": 4.40085506439209,
      "learning_rate": 7.889908256880735e-05,
      "loss": 5.4399,
      "step": 920
    },
    {
      "epoch": 4.251428571428572,
      "grad_norm": 5.0185980796813965,
      "learning_rate": 7.86697247706422e-05,
      "loss": 5.2784,
      "step": 930
    },
    {
      "epoch": 4.297142857142857,
      "grad_norm": 4.5097784996032715,
      "learning_rate": 7.844036697247707e-05,
      "loss": 5.4029,
      "step": 940
    },
    {
      "epoch": 4.3428571428571425,
      "grad_norm": 4.217061996459961,
      "learning_rate": 7.821100917431193e-05,
      "loss": 5.3685,
      "step": 950
    },
    {
      "epoch": 4.388571428571429,
      "grad_norm": 4.717717170715332,
      "learning_rate": 7.79816513761468e-05,
      "loss": 5.365,
      "step": 960
    },
    {
      "epoch": 4.434285714285714,
      "grad_norm": 4.161102771759033,
      "learning_rate": 7.775229357798165e-05,
      "loss": 5.4335,
      "step": 970
    },
    {
      "epoch": 4.48,
      "grad_norm": 4.298272132873535,
      "learning_rate": 7.752293577981652e-05,
      "loss": 5.4728,
      "step": 980
    },
    {
      "epoch": 4.525714285714286,
      "grad_norm": 4.522868633270264,
      "learning_rate": 7.729357798165138e-05,
      "loss": 5.4181,
      "step": 990
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 3.9142873287200928,
      "learning_rate": 7.706422018348625e-05,
      "loss": 5.3974,
      "step": 1000
    },
    {
      "epoch": 4.617142857142857,
      "grad_norm": 4.230809211730957,
      "learning_rate": 7.68348623853211e-05,
      "loss": 5.3922,
      "step": 1010
    },
    {
      "epoch": 4.662857142857143,
      "grad_norm": 4.94810152053833,
      "learning_rate": 7.660550458715597e-05,
      "loss": 5.2824,
      "step": 1020
    },
    {
      "epoch": 4.708571428571428,
      "grad_norm": 3.8377726078033447,
      "learning_rate": 7.637614678899084e-05,
      "loss": 5.2872,
      "step": 1030
    },
    {
      "epoch": 4.7542857142857144,
      "grad_norm": 3.818596839904785,
      "learning_rate": 7.614678899082569e-05,
      "loss": 5.3737,
      "step": 1040
    },
    {
      "epoch": 4.8,
      "grad_norm": 3.8673551082611084,
      "learning_rate": 7.591743119266055e-05,
      "loss": 5.4038,
      "step": 1050
    },
    {
      "epoch": 4.845714285714286,
      "grad_norm": 4.950047492980957,
      "learning_rate": 7.568807339449542e-05,
      "loss": 5.3996,
      "step": 1060
    },
    {
      "epoch": 4.8914285714285715,
      "grad_norm": 4.503044605255127,
      "learning_rate": 7.545871559633027e-05,
      "loss": 5.3116,
      "step": 1070
    },
    {
      "epoch": 4.937142857142857,
      "grad_norm": 3.86262583732605,
      "learning_rate": 7.522935779816514e-05,
      "loss": 5.2772,
      "step": 1080
    },
    {
      "epoch": 4.982857142857143,
      "grad_norm": 4.178839206695557,
      "learning_rate": 7.500000000000001e-05,
      "loss": 5.3833,
      "step": 1090
    },
    {
      "epoch": 5.0285714285714285,
      "grad_norm": 4.072139263153076,
      "learning_rate": 7.477064220183486e-05,
      "loss": 5.2047,
      "step": 1100
    },
    {
      "epoch": 5.074285714285715,
      "grad_norm": 4.239393711090088,
      "learning_rate": 7.454128440366973e-05,
      "loss": 5.2317,
      "step": 1110
    },
    {
      "epoch": 5.12,
      "grad_norm": 4.682635307312012,
      "learning_rate": 7.431192660550459e-05,
      "loss": 5.394,
      "step": 1120
    },
    {
      "epoch": 5.1657142857142855,
      "grad_norm": 4.121759414672852,
      "learning_rate": 7.408256880733946e-05,
      "loss": 5.298,
      "step": 1130
    },
    {
      "epoch": 5.211428571428572,
      "grad_norm": 5.788208484649658,
      "learning_rate": 7.385321100917431e-05,
      "loss": 5.4459,
      "step": 1140
    },
    {
      "epoch": 5.257142857142857,
      "grad_norm": 4.279709815979004,
      "learning_rate": 7.362385321100918e-05,
      "loss": 5.3993,
      "step": 1150
    },
    {
      "epoch": 5.3028571428571425,
      "grad_norm": 4.031208515167236,
      "learning_rate": 7.339449541284404e-05,
      "loss": 5.2639,
      "step": 1160
    },
    {
      "epoch": 5.348571428571429,
      "grad_norm": 3.81715726852417,
      "learning_rate": 7.31651376146789e-05,
      "loss": 5.2666,
      "step": 1170
    },
    {
      "epoch": 5.394285714285714,
      "grad_norm": 3.8320887088775635,
      "learning_rate": 7.293577981651376e-05,
      "loss": 5.3348,
      "step": 1180
    },
    {
      "epoch": 5.44,
      "grad_norm": 4.251369476318359,
      "learning_rate": 7.270642201834863e-05,
      "loss": 5.339,
      "step": 1190
    },
    {
      "epoch": 5.485714285714286,
      "grad_norm": 3.8313140869140625,
      "learning_rate": 7.247706422018348e-05,
      "loss": 5.3741,
      "step": 1200
    },
    {
      "epoch": 5.531428571428571,
      "grad_norm": 4.341080188751221,
      "learning_rate": 7.224770642201836e-05,
      "loss": 5.3157,
      "step": 1210
    },
    {
      "epoch": 5.577142857142857,
      "grad_norm": 4.158011436462402,
      "learning_rate": 7.201834862385322e-05,
      "loss": 5.365,
      "step": 1220
    },
    {
      "epoch": 5.622857142857143,
      "grad_norm": 4.436295986175537,
      "learning_rate": 7.178899082568807e-05,
      "loss": 5.3994,
      "step": 1230
    },
    {
      "epoch": 5.668571428571429,
      "grad_norm": 4.46081018447876,
      "learning_rate": 7.155963302752295e-05,
      "loss": 5.3904,
      "step": 1240
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 5.440611839294434,
      "learning_rate": 7.13302752293578e-05,
      "loss": 5.3053,
      "step": 1250
    },
    {
      "epoch": 5.76,
      "grad_norm": 3.6678695678710938,
      "learning_rate": 7.110091743119265e-05,
      "loss": 5.3769,
      "step": 1260
    },
    {
      "epoch": 5.805714285714286,
      "grad_norm": 5.724841117858887,
      "learning_rate": 7.087155963302753e-05,
      "loss": 5.2156,
      "step": 1270
    },
    {
      "epoch": 5.851428571428571,
      "grad_norm": 4.053743839263916,
      "learning_rate": 7.064220183486239e-05,
      "loss": 5.422,
      "step": 1280
    },
    {
      "epoch": 5.897142857142857,
      "grad_norm": 3.9640543460845947,
      "learning_rate": 7.041284403669725e-05,
      "loss": 5.2594,
      "step": 1290
    },
    {
      "epoch": 5.942857142857143,
      "grad_norm": 4.166876792907715,
      "learning_rate": 7.018348623853212e-05,
      "loss": 5.3039,
      "step": 1300
    },
    {
      "epoch": 5.988571428571428,
      "grad_norm": 3.916982650756836,
      "learning_rate": 6.995412844036697e-05,
      "loss": 5.2732,
      "step": 1310
    },
    {
      "epoch": 6.034285714285715,
      "grad_norm": 4.009678840637207,
      "learning_rate": 6.972477064220184e-05,
      "loss": 5.3991,
      "step": 1320
    },
    {
      "epoch": 6.08,
      "grad_norm": 6.586594104766846,
      "learning_rate": 6.94954128440367e-05,
      "loss": 5.2665,
      "step": 1330
    },
    {
      "epoch": 6.1257142857142854,
      "grad_norm": 5.185860633850098,
      "learning_rate": 6.926605504587156e-05,
      "loss": 5.2865,
      "step": 1340
    },
    {
      "epoch": 6.171428571428572,
      "grad_norm": 4.478105545043945,
      "learning_rate": 6.903669724770642e-05,
      "loss": 5.2589,
      "step": 1350
    },
    {
      "epoch": 6.217142857142857,
      "grad_norm": 4.4842634201049805,
      "learning_rate": 6.880733944954129e-05,
      "loss": 5.359,
      "step": 1360
    },
    {
      "epoch": 6.2628571428571425,
      "grad_norm": 4.485800743103027,
      "learning_rate": 6.857798165137616e-05,
      "loss": 5.3503,
      "step": 1370
    },
    {
      "epoch": 6.308571428571429,
      "grad_norm": 4.645104885101318,
      "learning_rate": 6.834862385321101e-05,
      "loss": 5.3441,
      "step": 1380
    },
    {
      "epoch": 6.354285714285714,
      "grad_norm": 7.494888782501221,
      "learning_rate": 6.811926605504588e-05,
      "loss": 5.2708,
      "step": 1390
    },
    {
      "epoch": 6.4,
      "grad_norm": 4.1815009117126465,
      "learning_rate": 6.788990825688074e-05,
      "loss": 5.3888,
      "step": 1400
    },
    {
      "epoch": 6.445714285714286,
      "grad_norm": 4.7130961418151855,
      "learning_rate": 6.76605504587156e-05,
      "loss": 5.266,
      "step": 1410
    },
    {
      "epoch": 6.491428571428571,
      "grad_norm": 4.662013053894043,
      "learning_rate": 6.743119266055046e-05,
      "loss": 5.2721,
      "step": 1420
    },
    {
      "epoch": 6.537142857142857,
      "grad_norm": 5.106797218322754,
      "learning_rate": 6.720183486238533e-05,
      "loss": 5.3283,
      "step": 1430
    },
    {
      "epoch": 6.582857142857143,
      "grad_norm": 4.141419887542725,
      "learning_rate": 6.697247706422018e-05,
      "loss": 5.4072,
      "step": 1440
    },
    {
      "epoch": 6.628571428571428,
      "grad_norm": 4.230240345001221,
      "learning_rate": 6.674311926605505e-05,
      "loss": 5.2139,
      "step": 1450
    },
    {
      "epoch": 6.674285714285714,
      "grad_norm": 4.531028747558594,
      "learning_rate": 6.651376146788991e-05,
      "loss": 5.3857,
      "step": 1460
    },
    {
      "epoch": 6.72,
      "grad_norm": 4.245660781860352,
      "learning_rate": 6.628440366972477e-05,
      "loss": 5.325,
      "step": 1470
    },
    {
      "epoch": 6.765714285714286,
      "grad_norm": 4.566542148590088,
      "learning_rate": 6.605504587155963e-05,
      "loss": 5.2203,
      "step": 1480
    },
    {
      "epoch": 6.811428571428571,
      "grad_norm": 4.889296054840088,
      "learning_rate": 6.58256880733945e-05,
      "loss": 5.2909,
      "step": 1490
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 4.3967461585998535,
      "learning_rate": 6.559633027522935e-05,
      "loss": 5.2485,
      "step": 1500
    },
    {
      "epoch": 6.902857142857143,
      "grad_norm": 4.33293342590332,
      "learning_rate": 6.536697247706422e-05,
      "loss": 5.2948,
      "step": 1510
    },
    {
      "epoch": 6.948571428571428,
      "grad_norm": 3.8168585300445557,
      "learning_rate": 6.513761467889909e-05,
      "loss": 5.334,
      "step": 1520
    },
    {
      "epoch": 6.994285714285715,
      "grad_norm": 4.673481464385986,
      "learning_rate": 6.490825688073395e-05,
      "loss": 5.2548,
      "step": 1530
    },
    {
      "epoch": 7.04,
      "grad_norm": 4.6100335121154785,
      "learning_rate": 6.46788990825688e-05,
      "loss": 5.2214,
      "step": 1540
    },
    {
      "epoch": 7.085714285714285,
      "grad_norm": 4.615877151489258,
      "learning_rate": 6.444954128440367e-05,
      "loss": 5.0937,
      "step": 1550
    },
    {
      "epoch": 7.131428571428572,
      "grad_norm": 3.698998212814331,
      "learning_rate": 6.422018348623854e-05,
      "loss": 5.223,
      "step": 1560
    },
    {
      "epoch": 7.177142857142857,
      "grad_norm": 4.019024848937988,
      "learning_rate": 6.39908256880734e-05,
      "loss": 5.3346,
      "step": 1570
    },
    {
      "epoch": 7.222857142857142,
      "grad_norm": 4.61644983291626,
      "learning_rate": 6.376146788990826e-05,
      "loss": 5.2726,
      "step": 1580
    },
    {
      "epoch": 7.268571428571429,
      "grad_norm": 4.076618194580078,
      "learning_rate": 6.353211009174312e-05,
      "loss": 5.3465,
      "step": 1590
    },
    {
      "epoch": 7.314285714285714,
      "grad_norm": 5.732125282287598,
      "learning_rate": 6.330275229357799e-05,
      "loss": 5.258,
      "step": 1600
    },
    {
      "epoch": 7.36,
      "grad_norm": 4.201084613800049,
      "learning_rate": 6.307339449541286e-05,
      "loss": 5.432,
      "step": 1610
    },
    {
      "epoch": 7.405714285714286,
      "grad_norm": 4.275938034057617,
      "learning_rate": 6.284403669724771e-05,
      "loss": 5.2621,
      "step": 1620
    },
    {
      "epoch": 7.451428571428571,
      "grad_norm": 4.59520149230957,
      "learning_rate": 6.261467889908257e-05,
      "loss": 5.3259,
      "step": 1630
    },
    {
      "epoch": 7.497142857142857,
      "grad_norm": 5.2159857749938965,
      "learning_rate": 6.238532110091744e-05,
      "loss": 5.3676,
      "step": 1640
    },
    {
      "epoch": 7.542857142857143,
      "grad_norm": 4.1771745681762695,
      "learning_rate": 6.21559633027523e-05,
      "loss": 5.3027,
      "step": 1650
    },
    {
      "epoch": 7.588571428571429,
      "grad_norm": 4.252683162689209,
      "learning_rate": 6.192660550458716e-05,
      "loss": 5.3682,
      "step": 1660
    },
    {
      "epoch": 7.634285714285714,
      "grad_norm": 5.26406192779541,
      "learning_rate": 6.169724770642203e-05,
      "loss": 5.3321,
      "step": 1670
    },
    {
      "epoch": 7.68,
      "grad_norm": 5.203461647033691,
      "learning_rate": 6.146788990825688e-05,
      "loss": 5.2392,
      "step": 1680
    },
    {
      "epoch": 7.725714285714286,
      "grad_norm": 4.4834113121032715,
      "learning_rate": 6.123853211009175e-05,
      "loss": 5.3254,
      "step": 1690
    },
    {
      "epoch": 7.771428571428571,
      "grad_norm": 4.454249382019043,
      "learning_rate": 6.1009174311926606e-05,
      "loss": 5.1809,
      "step": 1700
    },
    {
      "epoch": 7.817142857142857,
      "grad_norm": 5.104922771453857,
      "learning_rate": 6.0779816513761465e-05,
      "loss": 5.2666,
      "step": 1710
    },
    {
      "epoch": 7.862857142857143,
      "grad_norm": 4.96737813949585,
      "learning_rate": 6.055045871559634e-05,
      "loss": 5.3201,
      "step": 1720
    },
    {
      "epoch": 7.908571428571428,
      "grad_norm": 5.141938209533691,
      "learning_rate": 6.03211009174312e-05,
      "loss": 5.2646,
      "step": 1730
    },
    {
      "epoch": 7.954285714285715,
      "grad_norm": 3.6608171463012695,
      "learning_rate": 6.009174311926605e-05,
      "loss": 5.2647,
      "step": 1740
    },
    {
      "epoch": 8.0,
      "grad_norm": 4.874929904937744,
      "learning_rate": 5.9862385321100924e-05,
      "loss": 5.2641,
      "step": 1750
    },
    {
      "epoch": 8.045714285714286,
      "grad_norm": 4.400085926055908,
      "learning_rate": 5.9633027522935784e-05,
      "loss": 5.2777,
      "step": 1760
    },
    {
      "epoch": 8.09142857142857,
      "grad_norm": 5.638141632080078,
      "learning_rate": 5.940366972477065e-05,
      "loss": 5.2173,
      "step": 1770
    },
    {
      "epoch": 8.137142857142857,
      "grad_norm": 3.7130818367004395,
      "learning_rate": 5.917431192660551e-05,
      "loss": 5.3229,
      "step": 1780
    },
    {
      "epoch": 8.182857142857143,
      "grad_norm": 4.419607162475586,
      "learning_rate": 5.894495412844037e-05,
      "loss": 5.2832,
      "step": 1790
    },
    {
      "epoch": 8.228571428571428,
      "grad_norm": 3.929075241088867,
      "learning_rate": 5.8715596330275236e-05,
      "loss": 5.1703,
      "step": 1800
    },
    {
      "epoch": 8.274285714285714,
      "grad_norm": 5.030274391174316,
      "learning_rate": 5.8486238532110095e-05,
      "loss": 5.2745,
      "step": 1810
    },
    {
      "epoch": 8.32,
      "grad_norm": 4.319833755493164,
      "learning_rate": 5.8256880733944955e-05,
      "loss": 5.3517,
      "step": 1820
    },
    {
      "epoch": 8.365714285714287,
      "grad_norm": 5.030838489532471,
      "learning_rate": 5.802752293577982e-05,
      "loss": 5.2896,
      "step": 1830
    },
    {
      "epoch": 8.411428571428571,
      "grad_norm": 5.592815399169922,
      "learning_rate": 5.779816513761468e-05,
      "loss": 5.2015,
      "step": 1840
    },
    {
      "epoch": 8.457142857142857,
      "grad_norm": 4.264322280883789,
      "learning_rate": 5.756880733944955e-05,
      "loss": 5.1753,
      "step": 1850
    },
    {
      "epoch": 8.502857142857144,
      "grad_norm": 4.399652004241943,
      "learning_rate": 5.733944954128441e-05,
      "loss": 5.3013,
      "step": 1860
    },
    {
      "epoch": 8.548571428571428,
      "grad_norm": 7.350099563598633,
      "learning_rate": 5.7110091743119266e-05,
      "loss": 5.11,
      "step": 1870
    },
    {
      "epoch": 8.594285714285714,
      "grad_norm": 4.323237895965576,
      "learning_rate": 5.688073394495413e-05,
      "loss": 5.3068,
      "step": 1880
    },
    {
      "epoch": 8.64,
      "grad_norm": 5.034244060516357,
      "learning_rate": 5.665137614678899e-05,
      "loss": 5.3795,
      "step": 1890
    },
    {
      "epoch": 8.685714285714285,
      "grad_norm": 4.4084792137146,
      "learning_rate": 5.642201834862385e-05,
      "loss": 5.4032,
      "step": 1900
    },
    {
      "epoch": 8.731428571428571,
      "grad_norm": 4.876347064971924,
      "learning_rate": 5.619266055045872e-05,
      "loss": 5.1469,
      "step": 1910
    },
    {
      "epoch": 8.777142857142858,
      "grad_norm": 4.185166358947754,
      "learning_rate": 5.596330275229358e-05,
      "loss": 5.3347,
      "step": 1920
    },
    {
      "epoch": 8.822857142857142,
      "grad_norm": 5.185081481933594,
      "learning_rate": 5.5733944954128444e-05,
      "loss": 5.3156,
      "step": 1930
    },
    {
      "epoch": 8.868571428571428,
      "grad_norm": 5.61179256439209,
      "learning_rate": 5.5504587155963304e-05,
      "loss": 5.2013,
      "step": 1940
    },
    {
      "epoch": 8.914285714285715,
      "grad_norm": 4.7986531257629395,
      "learning_rate": 5.5275229357798164e-05,
      "loss": 5.2091,
      "step": 1950
    },
    {
      "epoch": 8.96,
      "grad_norm": 5.370541095733643,
      "learning_rate": 5.504587155963303e-05,
      "loss": 5.2513,
      "step": 1960
    },
    {
      "epoch": 9.005714285714285,
      "grad_norm": 4.118614673614502,
      "learning_rate": 5.481651376146789e-05,
      "loss": 5.312,
      "step": 1970
    },
    {
      "epoch": 9.051428571428572,
      "grad_norm": 4.910925388336182,
      "learning_rate": 5.458715596330275e-05,
      "loss": 5.2761,
      "step": 1980
    },
    {
      "epoch": 9.097142857142858,
      "grad_norm": 4.669849872589111,
      "learning_rate": 5.4357798165137616e-05,
      "loss": 5.2019,
      "step": 1990
    },
    {
      "epoch": 9.142857142857142,
      "grad_norm": 4.694509506225586,
      "learning_rate": 5.4128440366972475e-05,
      "loss": 5.3319,
      "step": 2000
    },
    {
      "epoch": 9.188571428571429,
      "grad_norm": 5.363883018493652,
      "learning_rate": 5.389908256880735e-05,
      "loss": 5.3187,
      "step": 2010
    },
    {
      "epoch": 9.234285714285715,
      "grad_norm": 5.432549476623535,
      "learning_rate": 5.36697247706422e-05,
      "loss": 5.2298,
      "step": 2020
    },
    {
      "epoch": 9.28,
      "grad_norm": 5.415426254272461,
      "learning_rate": 5.344036697247706e-05,
      "loss": 5.2003,
      "step": 2030
    },
    {
      "epoch": 9.325714285714286,
      "grad_norm": 4.834757328033447,
      "learning_rate": 5.3211009174311934e-05,
      "loss": 5.2347,
      "step": 2040
    },
    {
      "epoch": 9.371428571428572,
      "grad_norm": 5.279094219207764,
      "learning_rate": 5.2981651376146794e-05,
      "loss": 5.2428,
      "step": 2050
    },
    {
      "epoch": 9.417142857142856,
      "grad_norm": 5.5172929763793945,
      "learning_rate": 5.2752293577981646e-05,
      "loss": 5.2521,
      "step": 2060
    },
    {
      "epoch": 9.462857142857143,
      "grad_norm": 4.708383560180664,
      "learning_rate": 5.252293577981652e-05,
      "loss": 5.2775,
      "step": 2070
    },
    {
      "epoch": 9.508571428571429,
      "grad_norm": 4.565566062927246,
      "learning_rate": 5.229357798165138e-05,
      "loss": 5.2184,
      "step": 2080
    },
    {
      "epoch": 9.554285714285715,
      "grad_norm": 4.725283622741699,
      "learning_rate": 5.2064220183486246e-05,
      "loss": 5.1291,
      "step": 2090
    },
    {
      "epoch": 9.6,
      "grad_norm": 3.9641828536987305,
      "learning_rate": 5.1834862385321105e-05,
      "loss": 5.1546,
      "step": 2100
    },
    {
      "epoch": 9.645714285714286,
      "grad_norm": 4.935311317443848,
      "learning_rate": 5.1605504587155965e-05,
      "loss": 5.2029,
      "step": 2110
    },
    {
      "epoch": 9.691428571428572,
      "grad_norm": 4.152194023132324,
      "learning_rate": 5.137614678899083e-05,
      "loss": 5.3166,
      "step": 2120
    },
    {
      "epoch": 9.737142857142857,
      "grad_norm": 5.209307670593262,
      "learning_rate": 5.114678899082569e-05,
      "loss": 5.3191,
      "step": 2130
    },
    {
      "epoch": 9.782857142857143,
      "grad_norm": 4.670600891113281,
      "learning_rate": 5.091743119266055e-05,
      "loss": 5.1752,
      "step": 2140
    },
    {
      "epoch": 9.82857142857143,
      "grad_norm": 5.200076103210449,
      "learning_rate": 5.068807339449542e-05,
      "loss": 5.2054,
      "step": 2150
    },
    {
      "epoch": 9.874285714285714,
      "grad_norm": 5.106657981872559,
      "learning_rate": 5.0458715596330276e-05,
      "loss": 5.3059,
      "step": 2160
    },
    {
      "epoch": 9.92,
      "grad_norm": 4.3413238525390625,
      "learning_rate": 5.022935779816514e-05,
      "loss": 5.1636,
      "step": 2170
    },
    {
      "epoch": 9.965714285714286,
      "grad_norm": 4.873195648193359,
      "learning_rate": 5e-05,
      "loss": 5.2602,
      "step": 2180
    },
    {
      "epoch": 10.01142857142857,
      "grad_norm": 3.5784363746643066,
      "learning_rate": 4.977064220183487e-05,
      "loss": 5.3976,
      "step": 2190
    },
    {
      "epoch": 10.057142857142857,
      "grad_norm": 4.772542953491211,
      "learning_rate": 4.954128440366973e-05,
      "loss": 5.1849,
      "step": 2200
    },
    {
      "epoch": 10.102857142857143,
      "grad_norm": 4.7702813148498535,
      "learning_rate": 4.931192660550459e-05,
      "loss": 5.2931,
      "step": 2210
    },
    {
      "epoch": 10.14857142857143,
      "grad_norm": 4.889726161956787,
      "learning_rate": 4.9082568807339454e-05,
      "loss": 5.2108,
      "step": 2220
    },
    {
      "epoch": 10.194285714285714,
      "grad_norm": 5.19484281539917,
      "learning_rate": 4.8853211009174314e-05,
      "loss": 5.3155,
      "step": 2230
    },
    {
      "epoch": 10.24,
      "grad_norm": 5.349625110626221,
      "learning_rate": 4.862385321100918e-05,
      "loss": 5.1412,
      "step": 2240
    },
    {
      "epoch": 10.285714285714286,
      "grad_norm": 4.772522926330566,
      "learning_rate": 4.839449541284404e-05,
      "loss": 5.2467,
      "step": 2250
    },
    {
      "epoch": 10.331428571428571,
      "grad_norm": 4.850603103637695,
      "learning_rate": 4.81651376146789e-05,
      "loss": 5.2535,
      "step": 2260
    },
    {
      "epoch": 10.377142857142857,
      "grad_norm": 5.347822189331055,
      "learning_rate": 4.7935779816513766e-05,
      "loss": 5.2611,
      "step": 2270
    },
    {
      "epoch": 10.422857142857143,
      "grad_norm": 5.0148725509643555,
      "learning_rate": 4.7706422018348626e-05,
      "loss": 5.3345,
      "step": 2280
    },
    {
      "epoch": 10.468571428571428,
      "grad_norm": 4.567194938659668,
      "learning_rate": 4.7477064220183485e-05,
      "loss": 5.1315,
      "step": 2290
    },
    {
      "epoch": 10.514285714285714,
      "grad_norm": 4.57831335067749,
      "learning_rate": 4.724770642201835e-05,
      "loss": 5.2218,
      "step": 2300
    },
    {
      "epoch": 10.56,
      "grad_norm": 5.213657855987549,
      "learning_rate": 4.701834862385321e-05,
      "loss": 5.3227,
      "step": 2310
    },
    {
      "epoch": 10.605714285714285,
      "grad_norm": 5.036167144775391,
      "learning_rate": 4.678899082568808e-05,
      "loss": 5.2916,
      "step": 2320
    },
    {
      "epoch": 10.651428571428571,
      "grad_norm": 5.2843918800354,
      "learning_rate": 4.655963302752294e-05,
      "loss": 5.2611,
      "step": 2330
    },
    {
      "epoch": 10.697142857142858,
      "grad_norm": 5.020141124725342,
      "learning_rate": 4.6330275229357804e-05,
      "loss": 5.2112,
      "step": 2340
    },
    {
      "epoch": 10.742857142857144,
      "grad_norm": 4.451298236846924,
      "learning_rate": 4.610091743119266e-05,
      "loss": 5.1973,
      "step": 2350
    },
    {
      "epoch": 10.788571428571428,
      "grad_norm": 6.325306415557861,
      "learning_rate": 4.587155963302753e-05,
      "loss": 5.1915,
      "step": 2360
    },
    {
      "epoch": 10.834285714285715,
      "grad_norm": 4.713601589202881,
      "learning_rate": 4.564220183486239e-05,
      "loss": 5.1108,
      "step": 2370
    },
    {
      "epoch": 10.88,
      "grad_norm": 5.91445255279541,
      "learning_rate": 4.541284403669725e-05,
      "loss": 5.1492,
      "step": 2380
    },
    {
      "epoch": 10.925714285714285,
      "grad_norm": 5.481832981109619,
      "learning_rate": 4.5183486238532115e-05,
      "loss": 5.2591,
      "step": 2390
    },
    {
      "epoch": 10.971428571428572,
      "grad_norm": 6.217349529266357,
      "learning_rate": 4.4954128440366975e-05,
      "loss": 5.117,
      "step": 2400
    }
  ],
  "logging_steps": 10,
  "max_steps": 4360,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
