{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.0,
  "eval_steps": 500,
  "global_step": 6561,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013717421124828532,
      "grad_norm": 7.102995872497559,
      "learning_rate": 0.003994513031550069,
      "loss": 7.909,
      "step": 10
    },
    {
      "epoch": 0.027434842249657063,
      "grad_norm": 5.449845790863037,
      "learning_rate": 0.003989026063100137,
      "loss": 4.2655,
      "step": 20
    },
    {
      "epoch": 0.0411522633744856,
      "grad_norm": 9.055180549621582,
      "learning_rate": 0.0039835390946502056,
      "loss": 3.5615,
      "step": 30
    },
    {
      "epoch": 0.05486968449931413,
      "grad_norm": 9.314657211303711,
      "learning_rate": 0.003978052126200274,
      "loss": 3.0856,
      "step": 40
    },
    {
      "epoch": 0.06858710562414266,
      "grad_norm": 19.166606903076172,
      "learning_rate": 0.003972565157750343,
      "loss": 2.6442,
      "step": 50
    },
    {
      "epoch": 0.0823045267489712,
      "grad_norm": 13.384329795837402,
      "learning_rate": 0.003967078189300412,
      "loss": 2.2823,
      "step": 60
    },
    {
      "epoch": 0.09602194787379972,
      "grad_norm": 16.77313995361328,
      "learning_rate": 0.00396159122085048,
      "loss": 1.9177,
      "step": 70
    },
    {
      "epoch": 0.10973936899862825,
      "grad_norm": 12.387334823608398,
      "learning_rate": 0.003956104252400549,
      "loss": 1.6896,
      "step": 80
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 14.599374771118164,
      "learning_rate": 0.003950617283950617,
      "loss": 1.6219,
      "step": 90
    },
    {
      "epoch": 0.13717421124828533,
      "grad_norm": 12.941634178161621,
      "learning_rate": 0.003945130315500686,
      "loss": 1.4573,
      "step": 100
    },
    {
      "epoch": 0.15089163237311384,
      "grad_norm": 14.008685111999512,
      "learning_rate": 0.003939643347050755,
      "loss": 1.1522,
      "step": 110
    },
    {
      "epoch": 0.1646090534979424,
      "grad_norm": 16.199888229370117,
      "learning_rate": 0.003934156378600823,
      "loss": 1.0397,
      "step": 120
    },
    {
      "epoch": 0.17832647462277093,
      "grad_norm": 18.411029815673828,
      "learning_rate": 0.003928669410150892,
      "loss": 1.1183,
      "step": 130
    },
    {
      "epoch": 0.19204389574759945,
      "grad_norm": 21.276185989379883,
      "learning_rate": 0.00392318244170096,
      "loss": 0.8733,
      "step": 140
    },
    {
      "epoch": 0.205761316872428,
      "grad_norm": 16.384662628173828,
      "learning_rate": 0.003917695473251029,
      "loss": 0.8043,
      "step": 150
    },
    {
      "epoch": 0.2194787379972565,
      "grad_norm": 17.66779327392578,
      "learning_rate": 0.003912208504801098,
      "loss": 0.6612,
      "step": 160
    },
    {
      "epoch": 0.23319615912208505,
      "grad_norm": 12.729955673217773,
      "learning_rate": 0.003906721536351166,
      "loss": 0.6627,
      "step": 170
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 10.72763442993164,
      "learning_rate": 0.0039012345679012347,
      "loss": 0.6115,
      "step": 180
    },
    {
      "epoch": 0.2606310013717421,
      "grad_norm": 19.470672607421875,
      "learning_rate": 0.0038957475994513035,
      "loss": 0.6592,
      "step": 190
    },
    {
      "epoch": 0.27434842249657065,
      "grad_norm": 9.79128360748291,
      "learning_rate": 0.003890260631001372,
      "loss": 0.4412,
      "step": 200
    },
    {
      "epoch": 0.2880658436213992,
      "grad_norm": 17.319538116455078,
      "learning_rate": 0.0038847736625514406,
      "loss": 0.4442,
      "step": 210
    },
    {
      "epoch": 0.3017832647462277,
      "grad_norm": 20.70102310180664,
      "learning_rate": 0.003879286694101509,
      "loss": 0.4374,
      "step": 220
    },
    {
      "epoch": 0.31550068587105623,
      "grad_norm": 11.593032836914062,
      "learning_rate": 0.0038737997256515777,
      "loss": 0.3548,
      "step": 230
    },
    {
      "epoch": 0.3292181069958848,
      "grad_norm": 10.380108833312988,
      "learning_rate": 0.0038683127572016465,
      "loss": 0.3424,
      "step": 240
    },
    {
      "epoch": 0.3429355281207133,
      "grad_norm": 9.524140357971191,
      "learning_rate": 0.003862825788751715,
      "loss": 0.3228,
      "step": 250
    },
    {
      "epoch": 0.35665294924554186,
      "grad_norm": 9.343277931213379,
      "learning_rate": 0.0038573388203017836,
      "loss": 0.2435,
      "step": 260
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 13.622091293334961,
      "learning_rate": 0.0038518518518518515,
      "loss": 0.2275,
      "step": 270
    },
    {
      "epoch": 0.3840877914951989,
      "grad_norm": 14.499710083007812,
      "learning_rate": 0.0038463648834019203,
      "loss": 0.3775,
      "step": 280
    },
    {
      "epoch": 0.39780521262002744,
      "grad_norm": 16.33543586730957,
      "learning_rate": 0.003840877914951989,
      "loss": 0.263,
      "step": 290
    },
    {
      "epoch": 0.411522633744856,
      "grad_norm": 10.830759048461914,
      "learning_rate": 0.0038353909465020574,
      "loss": 0.1856,
      "step": 300
    },
    {
      "epoch": 0.4252400548696845,
      "grad_norm": 6.924062728881836,
      "learning_rate": 0.0038299039780521262,
      "loss": 0.152,
      "step": 310
    },
    {
      "epoch": 0.438957475994513,
      "grad_norm": 6.442953109741211,
      "learning_rate": 0.003824417009602195,
      "loss": 0.1087,
      "step": 320
    },
    {
      "epoch": 0.45267489711934156,
      "grad_norm": 8.133803367614746,
      "learning_rate": 0.0038189300411522633,
      "loss": 0.1757,
      "step": 330
    },
    {
      "epoch": 0.4663923182441701,
      "grad_norm": 13.79932689666748,
      "learning_rate": 0.003813443072702332,
      "loss": 0.1208,
      "step": 340
    },
    {
      "epoch": 0.48010973936899864,
      "grad_norm": 10.184813499450684,
      "learning_rate": 0.0038079561042524005,
      "loss": 0.1287,
      "step": 350
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 3.2577786445617676,
      "learning_rate": 0.0038024691358024693,
      "loss": 0.1784,
      "step": 360
    },
    {
      "epoch": 0.5075445816186557,
      "grad_norm": 4.324190616607666,
      "learning_rate": 0.003796982167352538,
      "loss": 0.112,
      "step": 370
    },
    {
      "epoch": 0.5212620027434842,
      "grad_norm": 3.94382905960083,
      "learning_rate": 0.0037914951989026064,
      "loss": 0.1031,
      "step": 380
    },
    {
      "epoch": 0.5349794238683128,
      "grad_norm": 4.351624011993408,
      "learning_rate": 0.003786008230452675,
      "loss": 0.0946,
      "step": 390
    },
    {
      "epoch": 0.5486968449931413,
      "grad_norm": 0.9790947437286377,
      "learning_rate": 0.0037805212620027435,
      "loss": 0.1055,
      "step": 400
    },
    {
      "epoch": 0.5624142661179699,
      "grad_norm": 11.866278648376465,
      "learning_rate": 0.0037750342935528123,
      "loss": 0.0813,
      "step": 410
    },
    {
      "epoch": 0.5761316872427984,
      "grad_norm": 4.954287052154541,
      "learning_rate": 0.003769547325102881,
      "loss": 0.1122,
      "step": 420
    },
    {
      "epoch": 0.5898491083676269,
      "grad_norm": 3.6281094551086426,
      "learning_rate": 0.0037640603566529494,
      "loss": 0.0781,
      "step": 430
    },
    {
      "epoch": 0.6035665294924554,
      "grad_norm": 4.920687675476074,
      "learning_rate": 0.0037585733882030178,
      "loss": 0.1171,
      "step": 440
    },
    {
      "epoch": 0.6172839506172839,
      "grad_norm": 7.36521577835083,
      "learning_rate": 0.003753086419753086,
      "loss": 0.1346,
      "step": 450
    },
    {
      "epoch": 0.6310013717421125,
      "grad_norm": 3.5788347721099854,
      "learning_rate": 0.003747599451303155,
      "loss": 0.088,
      "step": 460
    },
    {
      "epoch": 0.644718792866941,
      "grad_norm": 3.4002041816711426,
      "learning_rate": 0.0037421124828532237,
      "loss": 0.0826,
      "step": 470
    },
    {
      "epoch": 0.6584362139917695,
      "grad_norm": 2.4556453227996826,
      "learning_rate": 0.003736625514403292,
      "loss": 0.0534,
      "step": 480
    },
    {
      "epoch": 0.6721536351165981,
      "grad_norm": 2.1229560375213623,
      "learning_rate": 0.003731138545953361,
      "loss": 0.0935,
      "step": 490
    },
    {
      "epoch": 0.6858710562414266,
      "grad_norm": 1.7936378717422485,
      "learning_rate": 0.0037256515775034296,
      "loss": 0.074,
      "step": 500
    },
    {
      "epoch": 0.6995884773662552,
      "grad_norm": 4.88102388381958,
      "learning_rate": 0.003720164609053498,
      "loss": 0.0567,
      "step": 510
    },
    {
      "epoch": 0.7133058984910837,
      "grad_norm": 2.9650697708129883,
      "learning_rate": 0.0037146776406035667,
      "loss": 0.042,
      "step": 520
    },
    {
      "epoch": 0.7270233196159122,
      "grad_norm": 3.334686040878296,
      "learning_rate": 0.003709190672153635,
      "loss": 0.085,
      "step": 530
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 10.097945213317871,
      "learning_rate": 0.003703703703703704,
      "loss": 0.0608,
      "step": 540
    },
    {
      "epoch": 0.7544581618655692,
      "grad_norm": 5.526248931884766,
      "learning_rate": 0.0036982167352537726,
      "loss": 0.0475,
      "step": 550
    },
    {
      "epoch": 0.7681755829903978,
      "grad_norm": 2.3787221908569336,
      "learning_rate": 0.003692729766803841,
      "loss": 0.0489,
      "step": 560
    },
    {
      "epoch": 0.7818930041152263,
      "grad_norm": 9.216018676757812,
      "learning_rate": 0.0036872427983539098,
      "loss": 0.2081,
      "step": 570
    },
    {
      "epoch": 0.7956104252400549,
      "grad_norm": 3.5417046546936035,
      "learning_rate": 0.003681755829903978,
      "loss": 0.0483,
      "step": 580
    },
    {
      "epoch": 0.8093278463648834,
      "grad_norm": 2.9877257347106934,
      "learning_rate": 0.003676268861454047,
      "loss": 0.0465,
      "step": 590
    },
    {
      "epoch": 0.823045267489712,
      "grad_norm": 2.1434154510498047,
      "learning_rate": 0.0036707818930041157,
      "loss": 0.0446,
      "step": 600
    },
    {
      "epoch": 0.8367626886145405,
      "grad_norm": 0.45098498463630676,
      "learning_rate": 0.0036652949245541836,
      "loss": 0.0276,
      "step": 610
    },
    {
      "epoch": 0.850480109739369,
      "grad_norm": 1.5228772163391113,
      "learning_rate": 0.0036598079561042524,
      "loss": 0.0393,
      "step": 620
    },
    {
      "epoch": 0.8641975308641975,
      "grad_norm": 3.817784070968628,
      "learning_rate": 0.0036543209876543207,
      "loss": 0.0506,
      "step": 630
    },
    {
      "epoch": 0.877914951989026,
      "grad_norm": 1.6846778392791748,
      "learning_rate": 0.0036488340192043895,
      "loss": 0.0483,
      "step": 640
    },
    {
      "epoch": 0.8916323731138546,
      "grad_norm": 0.6752075552940369,
      "learning_rate": 0.0036433470507544583,
      "loss": 0.0302,
      "step": 650
    },
    {
      "epoch": 0.9053497942386831,
      "grad_norm": 0.5913817882537842,
      "learning_rate": 0.0036378600823045266,
      "loss": 0.0677,
      "step": 660
    },
    {
      "epoch": 0.9190672153635117,
      "grad_norm": 0.4712817370891571,
      "learning_rate": 0.0036323731138545954,
      "loss": 0.0311,
      "step": 670
    },
    {
      "epoch": 0.9327846364883402,
      "grad_norm": 0.5882495641708374,
      "learning_rate": 0.003626886145404664,
      "loss": 0.0364,
      "step": 680
    },
    {
      "epoch": 0.9465020576131687,
      "grad_norm": 7.433635234832764,
      "learning_rate": 0.0036213991769547325,
      "loss": 0.0494,
      "step": 690
    },
    {
      "epoch": 0.9602194787379973,
      "grad_norm": 3.19130802154541,
      "learning_rate": 0.0036159122085048013,
      "loss": 0.0379,
      "step": 700
    },
    {
      "epoch": 0.9739368998628258,
      "grad_norm": 1.097072720527649,
      "learning_rate": 0.0036104252400548697,
      "loss": 0.0248,
      "step": 710
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 3.953758955001831,
      "learning_rate": 0.0036049382716049384,
      "loss": 0.0362,
      "step": 720
    },
    {
      "epoch": 1.0013717421124828,
      "grad_norm": 1.9627350568771362,
      "learning_rate": 0.003599451303155007,
      "loss": 0.0169,
      "step": 730
    },
    {
      "epoch": 1.0150891632373114,
      "grad_norm": 7.423328876495361,
      "learning_rate": 0.0035939643347050756,
      "loss": 0.0419,
      "step": 740
    },
    {
      "epoch": 1.02880658436214,
      "grad_norm": 1.5784693956375122,
      "learning_rate": 0.0035884773662551443,
      "loss": 0.0295,
      "step": 750
    },
    {
      "epoch": 1.0425240054869684,
      "grad_norm": 1.6370735168457031,
      "learning_rate": 0.0035829903978052127,
      "loss": 0.0295,
      "step": 760
    },
    {
      "epoch": 1.056241426611797,
      "grad_norm": 1.974791407585144,
      "learning_rate": 0.0035775034293552815,
      "loss": 0.0357,
      "step": 770
    },
    {
      "epoch": 1.0699588477366255,
      "grad_norm": 3.13201904296875,
      "learning_rate": 0.0035720164609053503,
      "loss": 0.0297,
      "step": 780
    },
    {
      "epoch": 1.083676268861454,
      "grad_norm": 2.4929585456848145,
      "learning_rate": 0.003566529492455418,
      "loss": 0.0502,
      "step": 790
    },
    {
      "epoch": 1.0973936899862826,
      "grad_norm": 2.195676803588867,
      "learning_rate": 0.003561042524005487,
      "loss": 0.0333,
      "step": 800
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 3.32153582572937,
      "learning_rate": 0.0035555555555555553,
      "loss": 0.0288,
      "step": 810
    },
    {
      "epoch": 1.1248285322359397,
      "grad_norm": 0.33802416920661926,
      "learning_rate": 0.003550068587105624,
      "loss": 0.0299,
      "step": 820
    },
    {
      "epoch": 1.1385459533607682,
      "grad_norm": 3.0308303833007812,
      "learning_rate": 0.003544581618655693,
      "loss": 0.0136,
      "step": 830
    },
    {
      "epoch": 1.1522633744855968,
      "grad_norm": 0.5001335144042969,
      "learning_rate": 0.003539094650205761,
      "loss": 0.0166,
      "step": 840
    },
    {
      "epoch": 1.1659807956104253,
      "grad_norm": 1.3040770292282104,
      "learning_rate": 0.00353360768175583,
      "loss": 0.0221,
      "step": 850
    },
    {
      "epoch": 1.1796982167352539,
      "grad_norm": 1.849737286567688,
      "learning_rate": 0.0035281207133058983,
      "loss": 0.0176,
      "step": 860
    },
    {
      "epoch": 1.1934156378600824,
      "grad_norm": 3.2244155406951904,
      "learning_rate": 0.003522633744855967,
      "loss": 0.0261,
      "step": 870
    },
    {
      "epoch": 1.2071330589849107,
      "grad_norm": 5.836394786834717,
      "learning_rate": 0.003517146776406036,
      "loss": 0.0257,
      "step": 880
    },
    {
      "epoch": 1.2208504801097393,
      "grad_norm": 1.5565156936645508,
      "learning_rate": 0.0035116598079561042,
      "loss": 0.0308,
      "step": 890
    },
    {
      "epoch": 1.2345679012345678,
      "grad_norm": 0.5373246669769287,
      "learning_rate": 0.003506172839506173,
      "loss": 0.0073,
      "step": 900
    },
    {
      "epoch": 1.2482853223593964,
      "grad_norm": 0.11659617722034454,
      "learning_rate": 0.003500685871056242,
      "loss": 0.0035,
      "step": 910
    },
    {
      "epoch": 1.262002743484225,
      "grad_norm": 0.24629347026348114,
      "learning_rate": 0.00349519890260631,
      "loss": 0.0041,
      "step": 920
    },
    {
      "epoch": 1.2757201646090535,
      "grad_norm": 0.1418679803609848,
      "learning_rate": 0.003489711934156379,
      "loss": 0.003,
      "step": 930
    },
    {
      "epoch": 1.289437585733882,
      "grad_norm": 2.758559465408325,
      "learning_rate": 0.0034842249657064473,
      "loss": 0.0081,
      "step": 940
    },
    {
      "epoch": 1.3031550068587106,
      "grad_norm": 4.704490661621094,
      "learning_rate": 0.003478737997256516,
      "loss": 0.0327,
      "step": 950
    },
    {
      "epoch": 1.316872427983539,
      "grad_norm": 1.6501725912094116,
      "learning_rate": 0.0034732510288065844,
      "loss": 0.0142,
      "step": 960
    },
    {
      "epoch": 1.3305898491083676,
      "grad_norm": 3.161900043487549,
      "learning_rate": 0.0034677640603566528,
      "loss": 0.0149,
      "step": 970
    },
    {
      "epoch": 1.3443072702331962,
      "grad_norm": 2.3209738731384277,
      "learning_rate": 0.0034622770919067215,
      "loss": 0.0193,
      "step": 980
    },
    {
      "epoch": 1.3580246913580247,
      "grad_norm": 0.1960245668888092,
      "learning_rate": 0.00345679012345679,
      "loss": 0.0086,
      "step": 990
    },
    {
      "epoch": 1.3717421124828533,
      "grad_norm": 1.492215633392334,
      "learning_rate": 0.0034513031550068587,
      "loss": 0.0107,
      "step": 1000
    },
    {
      "epoch": 1.3854595336076818,
      "grad_norm": 0.2733677327632904,
      "learning_rate": 0.0034458161865569274,
      "loss": 0.0117,
      "step": 1010
    },
    {
      "epoch": 1.3991769547325104,
      "grad_norm": 1.0395753383636475,
      "learning_rate": 0.003440329218106996,
      "loss": 0.0105,
      "step": 1020
    },
    {
      "epoch": 1.412894375857339,
      "grad_norm": 2.691480875015259,
      "learning_rate": 0.0034348422496570646,
      "loss": 0.0158,
      "step": 1030
    },
    {
      "epoch": 1.4266117969821672,
      "grad_norm": 2.866596221923828,
      "learning_rate": 0.003429355281207133,
      "loss": 0.0213,
      "step": 1040
    },
    {
      "epoch": 1.4403292181069958,
      "grad_norm": 0.7269473671913147,
      "learning_rate": 0.0034238683127572017,
      "loss": 0.0124,
      "step": 1050
    },
    {
      "epoch": 1.4540466392318243,
      "grad_norm": 3.53381609916687,
      "learning_rate": 0.0034183813443072705,
      "loss": 0.0154,
      "step": 1060
    },
    {
      "epoch": 1.4677640603566529,
      "grad_norm": 5.748310565948486,
      "learning_rate": 0.003412894375857339,
      "loss": 0.0259,
      "step": 1070
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 2.57140851020813,
      "learning_rate": 0.0034074074074074076,
      "loss": 0.0155,
      "step": 1080
    },
    {
      "epoch": 1.49519890260631,
      "grad_norm": 1.7304370403289795,
      "learning_rate": 0.0034019204389574764,
      "loss": 0.0244,
      "step": 1090
    },
    {
      "epoch": 1.5089163237311385,
      "grad_norm": 0.2701631486415863,
      "learning_rate": 0.0033964334705075447,
      "loss": 0.0079,
      "step": 1100
    },
    {
      "epoch": 1.522633744855967,
      "grad_norm": 0.058133356273174286,
      "learning_rate": 0.0033909465020576135,
      "loss": 0.006,
      "step": 1110
    },
    {
      "epoch": 1.5363511659807956,
      "grad_norm": 0.13815462589263916,
      "learning_rate": 0.003385459533607682,
      "loss": 0.0072,
      "step": 1120
    },
    {
      "epoch": 1.5500685871056241,
      "grad_norm": 0.10726580023765564,
      "learning_rate": 0.00337997256515775,
      "loss": 0.003,
      "step": 1130
    },
    {
      "epoch": 1.5637860082304527,
      "grad_norm": 0.07423245161771774,
      "learning_rate": 0.003374485596707819,
      "loss": 0.0025,
      "step": 1140
    },
    {
      "epoch": 1.5775034293552812,
      "grad_norm": 0.042405132204294205,
      "learning_rate": 0.0033689986282578873,
      "loss": 0.0029,
      "step": 1150
    },
    {
      "epoch": 1.5912208504801097,
      "grad_norm": 0.15457452833652496,
      "learning_rate": 0.003363511659807956,
      "loss": 0.0038,
      "step": 1160
    },
    {
      "epoch": 1.6049382716049383,
      "grad_norm": 0.19390183687210083,
      "learning_rate": 0.0033580246913580245,
      "loss": 0.0037,
      "step": 1170
    },
    {
      "epoch": 1.6186556927297668,
      "grad_norm": 0.5642995834350586,
      "learning_rate": 0.0033525377229080932,
      "loss": 0.0021,
      "step": 1180
    },
    {
      "epoch": 1.6323731138545954,
      "grad_norm": 0.041702333837747574,
      "learning_rate": 0.003347050754458162,
      "loss": 0.0014,
      "step": 1190
    },
    {
      "epoch": 1.646090534979424,
      "grad_norm": 1.3787133693695068,
      "learning_rate": 0.0033415637860082304,
      "loss": 0.0023,
      "step": 1200
    },
    {
      "epoch": 1.6598079561042525,
      "grad_norm": 0.3663009703159332,
      "learning_rate": 0.003336076817558299,
      "loss": 0.0022,
      "step": 1210
    },
    {
      "epoch": 1.673525377229081,
      "grad_norm": 0.029018864035606384,
      "learning_rate": 0.0033305898491083675,
      "loss": 0.0012,
      "step": 1220
    },
    {
      "epoch": 1.6872427983539096,
      "grad_norm": 0.030573470517992973,
      "learning_rate": 0.0033251028806584363,
      "loss": 0.0011,
      "step": 1230
    },
    {
      "epoch": 1.700960219478738,
      "grad_norm": 0.019351551309227943,
      "learning_rate": 0.003319615912208505,
      "loss": 0.0008,
      "step": 1240
    },
    {
      "epoch": 1.7146776406035666,
      "grad_norm": 0.06518267095088959,
      "learning_rate": 0.0033141289437585734,
      "loss": 0.0007,
      "step": 1250
    },
    {
      "epoch": 1.7283950617283952,
      "grad_norm": 0.007179753389209509,
      "learning_rate": 0.003308641975308642,
      "loss": 0.0005,
      "step": 1260
    },
    {
      "epoch": 1.7421124828532237,
      "grad_norm": 0.004783597309142351,
      "learning_rate": 0.003303155006858711,
      "loss": 0.0005,
      "step": 1270
    },
    {
      "epoch": 1.7558299039780523,
      "grad_norm": 0.006074214819818735,
      "learning_rate": 0.0032976680384087793,
      "loss": 0.0004,
      "step": 1280
    },
    {
      "epoch": 1.7695473251028808,
      "grad_norm": 0.21831804513931274,
      "learning_rate": 0.003292181069958848,
      "loss": 0.0006,
      "step": 1290
    },
    {
      "epoch": 1.7832647462277091,
      "grad_norm": 0.008676720783114433,
      "learning_rate": 0.0032866941015089165,
      "loss": 0.0005,
      "step": 1300
    },
    {
      "epoch": 1.7969821673525377,
      "grad_norm": 0.018993107602000237,
      "learning_rate": 0.003281207133058985,
      "loss": 0.0004,
      "step": 1310
    },
    {
      "epoch": 1.8106995884773662,
      "grad_norm": 0.01370448712259531,
      "learning_rate": 0.0032757201646090536,
      "loss": 0.0004,
      "step": 1320
    },
    {
      "epoch": 1.8244170096021948,
      "grad_norm": 0.0071778870187699795,
      "learning_rate": 0.003270233196159122,
      "loss": 0.0004,
      "step": 1330
    },
    {
      "epoch": 1.8381344307270233,
      "grad_norm": 0.004453766159713268,
      "learning_rate": 0.0032647462277091907,
      "loss": 0.0005,
      "step": 1340
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 0.010501434095203876,
      "learning_rate": 0.003259259259259259,
      "loss": 0.0003,
      "step": 1350
    },
    {
      "epoch": 1.8655692729766804,
      "grad_norm": 0.003696098690852523,
      "learning_rate": 0.003253772290809328,
      "loss": 0.0003,
      "step": 1360
    },
    {
      "epoch": 1.879286694101509,
      "grad_norm": 0.006534223910421133,
      "learning_rate": 0.0032482853223593966,
      "loss": 0.0059,
      "step": 1370
    },
    {
      "epoch": 1.8930041152263375,
      "grad_norm": 0.1527172178030014,
      "learning_rate": 0.003242798353909465,
      "loss": 0.0052,
      "step": 1380
    },
    {
      "epoch": 1.906721536351166,
      "grad_norm": 0.31320658326148987,
      "learning_rate": 0.0032373113854595337,
      "loss": 0.0043,
      "step": 1390
    },
    {
      "epoch": 1.9204389574759944,
      "grad_norm": 2.9821789264678955,
      "learning_rate": 0.003231824417009602,
      "loss": 0.0075,
      "step": 1400
    },
    {
      "epoch": 1.934156378600823,
      "grad_norm": 0.3538370430469513,
      "learning_rate": 0.003226337448559671,
      "loss": 0.0082,
      "step": 1410
    },
    {
      "epoch": 1.9478737997256514,
      "grad_norm": 0.32395270466804504,
      "learning_rate": 0.0032208504801097397,
      "loss": 0.0035,
      "step": 1420
    },
    {
      "epoch": 1.96159122085048,
      "grad_norm": 2.4334521293640137,
      "learning_rate": 0.003215363511659808,
      "loss": 0.003,
      "step": 1430
    },
    {
      "epoch": 1.9753086419753085,
      "grad_norm": 17.39132308959961,
      "learning_rate": 0.0032098765432098768,
      "loss": 0.0242,
      "step": 1440
    },
    {
      "epoch": 1.989026063100137,
      "grad_norm": 3.919358491897583,
      "learning_rate": 0.0032043895747599456,
      "loss": 0.0182,
      "step": 1450
    },
    {
      "epoch": 2.0027434842249656,
      "grad_norm": 0.43184858560562134,
      "learning_rate": 0.003198902606310014,
      "loss": 0.0226,
      "step": 1460
    },
    {
      "epoch": 2.016460905349794,
      "grad_norm": 3.5691072940826416,
      "learning_rate": 0.0031934156378600827,
      "loss": 0.0195,
      "step": 1470
    },
    {
      "epoch": 2.0301783264746227,
      "grad_norm": 0.7574926018714905,
      "learning_rate": 0.0031879286694101506,
      "loss": 0.0098,
      "step": 1480
    },
    {
      "epoch": 2.0438957475994513,
      "grad_norm": 0.1012134924530983,
      "learning_rate": 0.0031824417009602194,
      "loss": 0.0075,
      "step": 1490
    },
    {
      "epoch": 2.05761316872428,
      "grad_norm": 0.6531967520713806,
      "learning_rate": 0.003176954732510288,
      "loss": 0.0033,
      "step": 1500
    },
    {
      "epoch": 2.0713305898491083,
      "grad_norm": 0.038570135831832886,
      "learning_rate": 0.0031714677640603565,
      "loss": 0.0029,
      "step": 1510
    },
    {
      "epoch": 2.085048010973937,
      "grad_norm": 0.11704251170158386,
      "learning_rate": 0.0031659807956104253,
      "loss": 0.0019,
      "step": 1520
    },
    {
      "epoch": 2.0987654320987654,
      "grad_norm": 0.02710708975791931,
      "learning_rate": 0.0031604938271604936,
      "loss": 0.0028,
      "step": 1530
    },
    {
      "epoch": 2.112482853223594,
      "grad_norm": 0.0403568297624588,
      "learning_rate": 0.0031550068587105624,
      "loss": 0.0024,
      "step": 1540
    },
    {
      "epoch": 2.1262002743484225,
      "grad_norm": 0.1495818793773651,
      "learning_rate": 0.003149519890260631,
      "loss": 0.0046,
      "step": 1550
    },
    {
      "epoch": 2.139917695473251,
      "grad_norm": 0.26200664043426514,
      "learning_rate": 0.0031440329218106996,
      "loss": 0.0067,
      "step": 1560
    },
    {
      "epoch": 2.1536351165980796,
      "grad_norm": 0.08924920856952667,
      "learning_rate": 0.0031385459533607683,
      "loss": 0.003,
      "step": 1570
    },
    {
      "epoch": 2.167352537722908,
      "grad_norm": 0.6207463145256042,
      "learning_rate": 0.0031330589849108367,
      "loss": 0.0021,
      "step": 1580
    },
    {
      "epoch": 2.1810699588477367,
      "grad_norm": 0.021208368241786957,
      "learning_rate": 0.0031275720164609055,
      "loss": 0.0014,
      "step": 1590
    },
    {
      "epoch": 2.1947873799725652,
      "grad_norm": 0.015034540556371212,
      "learning_rate": 0.0031220850480109742,
      "loss": 0.0015,
      "step": 1600
    },
    {
      "epoch": 2.2085048010973938,
      "grad_norm": 0.022754216566681862,
      "learning_rate": 0.0031165980795610426,
      "loss": 0.0012,
      "step": 1610
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.1630395650863647,
      "learning_rate": 0.0031111111111111114,
      "loss": 0.0116,
      "step": 1620
    },
    {
      "epoch": 2.235939643347051,
      "grad_norm": 0.09275396913290024,
      "learning_rate": 0.0031056241426611797,
      "loss": 0.0065,
      "step": 1630
    },
    {
      "epoch": 2.2496570644718794,
      "grad_norm": 3.0868425369262695,
      "learning_rate": 0.0031001371742112485,
      "loss": 0.0055,
      "step": 1640
    },
    {
      "epoch": 2.263374485596708,
      "grad_norm": 0.17457349598407745,
      "learning_rate": 0.0030946502057613173,
      "loss": 0.0046,
      "step": 1650
    },
    {
      "epoch": 2.2770919067215365,
      "grad_norm": 0.2302320897579193,
      "learning_rate": 0.003089163237311385,
      "loss": 0.0071,
      "step": 1660
    },
    {
      "epoch": 2.290809327846365,
      "grad_norm": 2.841557025909424,
      "learning_rate": 0.003083676268861454,
      "loss": 0.0084,
      "step": 1670
    },
    {
      "epoch": 2.3045267489711936,
      "grad_norm": 0.07868708670139313,
      "learning_rate": 0.0030781893004115228,
      "loss": 0.0057,
      "step": 1680
    },
    {
      "epoch": 2.318244170096022,
      "grad_norm": 2.7797863483428955,
      "learning_rate": 0.003072702331961591,
      "loss": 0.0048,
      "step": 1690
    },
    {
      "epoch": 2.3319615912208507,
      "grad_norm": 0.32815805077552795,
      "learning_rate": 0.00306721536351166,
      "loss": 0.0038,
      "step": 1700
    },
    {
      "epoch": 2.3456790123456788,
      "grad_norm": 2.8385255336761475,
      "learning_rate": 0.0030617283950617282,
      "loss": 0.0069,
      "step": 1710
    },
    {
      "epoch": 2.3593964334705078,
      "grad_norm": 0.1052158772945404,
      "learning_rate": 0.003056241426611797,
      "loss": 0.0093,
      "step": 1720
    },
    {
      "epoch": 2.373113854595336,
      "grad_norm": 0.16486582159996033,
      "learning_rate": 0.003050754458161866,
      "loss": 0.0044,
      "step": 1730
    },
    {
      "epoch": 2.386831275720165,
      "grad_norm": 1.8320555686950684,
      "learning_rate": 0.003045267489711934,
      "loss": 0.0034,
      "step": 1740
    },
    {
      "epoch": 2.400548696844993,
      "grad_norm": 0.06956902891397476,
      "learning_rate": 0.003039780521262003,
      "loss": 0.0036,
      "step": 1750
    },
    {
      "epoch": 2.4142661179698215,
      "grad_norm": 0.04147619009017944,
      "learning_rate": 0.0030342935528120713,
      "loss": 0.0044,
      "step": 1760
    },
    {
      "epoch": 2.42798353909465,
      "grad_norm": 0.1378372609615326,
      "learning_rate": 0.00302880658436214,
      "loss": 0.0064,
      "step": 1770
    },
    {
      "epoch": 2.4417009602194786,
      "grad_norm": 0.2938307225704193,
      "learning_rate": 0.003023319615912209,
      "loss": 0.0102,
      "step": 1780
    },
    {
      "epoch": 2.455418381344307,
      "grad_norm": 0.07331833243370056,
      "learning_rate": 0.003017832647462277,
      "loss": 0.0063,
      "step": 1790
    },
    {
      "epoch": 2.4691358024691357,
      "grad_norm": 3.1299517154693604,
      "learning_rate": 0.003012345679012346,
      "loss": 0.0295,
      "step": 1800
    },
    {
      "epoch": 2.482853223593964,
      "grad_norm": 0.24580276012420654,
      "learning_rate": 0.0030068587105624143,
      "loss": 0.0417,
      "step": 1810
    },
    {
      "epoch": 2.4965706447187928,
      "grad_norm": 0.6949389576911926,
      "learning_rate": 0.003001371742112483,
      "loss": 0.0104,
      "step": 1820
    },
    {
      "epoch": 2.5102880658436213,
      "grad_norm": 0.44596511125564575,
      "learning_rate": 0.0029958847736625514,
      "loss": 0.0055,
      "step": 1830
    },
    {
      "epoch": 2.52400548696845,
      "grad_norm": 0.3634553849697113,
      "learning_rate": 0.0029903978052126198,
      "loss": 0.0064,
      "step": 1840
    },
    {
      "epoch": 2.5377229080932784,
      "grad_norm": 0.30697545409202576,
      "learning_rate": 0.0029849108367626886,
      "loss": 0.0071,
      "step": 1850
    },
    {
      "epoch": 2.551440329218107,
      "grad_norm": 0.09807620942592621,
      "learning_rate": 0.0029794238683127573,
      "loss": 0.0027,
      "step": 1860
    },
    {
      "epoch": 2.5651577503429355,
      "grad_norm": 0.023437203839421272,
      "learning_rate": 0.0029739368998628257,
      "loss": 0.0015,
      "step": 1870
    },
    {
      "epoch": 2.578875171467764,
      "grad_norm": 0.02568647265434265,
      "learning_rate": 0.0029684499314128945,
      "loss": 0.0012,
      "step": 1880
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 0.011096304282546043,
      "learning_rate": 0.002962962962962963,
      "loss": 0.0008,
      "step": 1890
    },
    {
      "epoch": 2.606310013717421,
      "grad_norm": 0.012583097442984581,
      "learning_rate": 0.0029574759945130316,
      "loss": 0.0007,
      "step": 1900
    },
    {
      "epoch": 2.6200274348422496,
      "grad_norm": 4.343296527862549,
      "learning_rate": 0.0029519890260631004,
      "loss": 0.0062,
      "step": 1910
    },
    {
      "epoch": 2.633744855967078,
      "grad_norm": 0.1520501673221588,
      "learning_rate": 0.0029465020576131687,
      "loss": 0.0012,
      "step": 1920
    },
    {
      "epoch": 2.6474622770919067,
      "grad_norm": 0.2666127383708954,
      "learning_rate": 0.0029410150891632375,
      "loss": 0.0014,
      "step": 1930
    },
    {
      "epoch": 2.6611796982167353,
      "grad_norm": 0.029684538021683693,
      "learning_rate": 0.002935528120713306,
      "loss": 0.0009,
      "step": 1940
    },
    {
      "epoch": 2.674897119341564,
      "grad_norm": 0.010009915567934513,
      "learning_rate": 0.0029300411522633746,
      "loss": 0.0009,
      "step": 1950
    },
    {
      "epoch": 2.6886145404663924,
      "grad_norm": 0.011954572051763535,
      "learning_rate": 0.0029245541838134434,
      "loss": 0.0005,
      "step": 1960
    },
    {
      "epoch": 2.702331961591221,
      "grad_norm": 0.005731647834181786,
      "learning_rate": 0.0029190672153635118,
      "loss": 0.0005,
      "step": 1970
    },
    {
      "epoch": 2.7160493827160495,
      "grad_norm": 0.010606400668621063,
      "learning_rate": 0.0029135802469135805,
      "loss": 0.0012,
      "step": 1980
    },
    {
      "epoch": 2.729766803840878,
      "grad_norm": 0.1387273520231247,
      "learning_rate": 0.002908093278463649,
      "loss": 0.0018,
      "step": 1990
    },
    {
      "epoch": 2.7434842249657065,
      "grad_norm": 0.06201981380581856,
      "learning_rate": 0.0029026063100137172,
      "loss": 0.0015,
      "step": 2000
    },
    {
      "epoch": 2.757201646090535,
      "grad_norm": 0.16873539984226227,
      "learning_rate": 0.002897119341563786,
      "loss": 0.0068,
      "step": 2010
    },
    {
      "epoch": 2.7709190672153636,
      "grad_norm": 0.07312056422233582,
      "learning_rate": 0.0028916323731138544,
      "loss": 0.0065,
      "step": 2020
    },
    {
      "epoch": 2.784636488340192,
      "grad_norm": 1.4947420358657837,
      "learning_rate": 0.002886145404663923,
      "loss": 0.0151,
      "step": 2030
    },
    {
      "epoch": 2.7983539094650207,
      "grad_norm": 0.07982174307107925,
      "learning_rate": 0.002880658436213992,
      "loss": 0.003,
      "step": 2040
    },
    {
      "epoch": 2.8120713305898493,
      "grad_norm": 0.6236079931259155,
      "learning_rate": 0.0028751714677640603,
      "loss": 0.0025,
      "step": 2050
    },
    {
      "epoch": 2.825788751714678,
      "grad_norm": 0.03273460641503334,
      "learning_rate": 0.002869684499314129,
      "loss": 0.0022,
      "step": 2060
    },
    {
      "epoch": 2.8395061728395063,
      "grad_norm": 0.015444724820554256,
      "learning_rate": 0.0028641975308641974,
      "loss": 0.0011,
      "step": 2070
    },
    {
      "epoch": 2.8532235939643344,
      "grad_norm": 0.019327662885189056,
      "learning_rate": 0.002858710562414266,
      "loss": 0.0012,
      "step": 2080
    },
    {
      "epoch": 2.8669410150891634,
      "grad_norm": 0.008190714754164219,
      "learning_rate": 0.002853223593964335,
      "loss": 0.0009,
      "step": 2090
    },
    {
      "epoch": 2.8806584362139915,
      "grad_norm": 0.2650699317455292,
      "learning_rate": 0.0028477366255144033,
      "loss": 0.0017,
      "step": 2100
    },
    {
      "epoch": 2.8943758573388205,
      "grad_norm": 0.5351266860961914,
      "learning_rate": 0.002842249657064472,
      "loss": 0.0072,
      "step": 2110
    },
    {
      "epoch": 2.9080932784636486,
      "grad_norm": 0.1880808174610138,
      "learning_rate": 0.0028367626886145404,
      "loss": 0.0027,
      "step": 2120
    },
    {
      "epoch": 2.9218106995884776,
      "grad_norm": 6.402782440185547,
      "learning_rate": 0.0028312757201646092,
      "loss": 0.0059,
      "step": 2130
    },
    {
      "epoch": 2.9355281207133057,
      "grad_norm": 0.1735450029373169,
      "learning_rate": 0.002825788751714678,
      "loss": 0.0084,
      "step": 2140
    },
    {
      "epoch": 2.9492455418381347,
      "grad_norm": 0.06327307969331741,
      "learning_rate": 0.0028203017832647463,
      "loss": 0.0026,
      "step": 2150
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 2.2099366188049316,
      "learning_rate": 0.002814814814814815,
      "loss": 0.0036,
      "step": 2160
    },
    {
      "epoch": 2.9766803840877913,
      "grad_norm": 0.06577832996845245,
      "learning_rate": 0.002809327846364883,
      "loss": 0.0017,
      "step": 2170
    },
    {
      "epoch": 2.99039780521262,
      "grad_norm": 0.034665368497371674,
      "learning_rate": 0.002803840877914952,
      "loss": 0.0013,
      "step": 2180
    },
    {
      "epoch": 3.0041152263374484,
      "grad_norm": 0.07215996831655502,
      "learning_rate": 0.0027983539094650206,
      "loss": 0.0029,
      "step": 2190
    },
    {
      "epoch": 3.017832647462277,
      "grad_norm": 0.13844533264636993,
      "learning_rate": 0.002792866941015089,
      "loss": 0.0032,
      "step": 2200
    },
    {
      "epoch": 3.0315500685871055,
      "grad_norm": 1.9053531885147095,
      "learning_rate": 0.0027873799725651577,
      "loss": 0.0171,
      "step": 2210
    },
    {
      "epoch": 3.045267489711934,
      "grad_norm": 1.5286877155303955,
      "learning_rate": 0.0027818930041152265,
      "loss": 0.0075,
      "step": 2220
    },
    {
      "epoch": 3.0589849108367626,
      "grad_norm": 0.037277668714523315,
      "learning_rate": 0.002776406035665295,
      "loss": 0.0107,
      "step": 2230
    },
    {
      "epoch": 3.072702331961591,
      "grad_norm": 0.6392796039581299,
      "learning_rate": 0.0027709190672153636,
      "loss": 0.0046,
      "step": 2240
    },
    {
      "epoch": 3.0864197530864197,
      "grad_norm": 0.06115949898958206,
      "learning_rate": 0.002765432098765432,
      "loss": 0.0029,
      "step": 2250
    },
    {
      "epoch": 3.1001371742112482,
      "grad_norm": 0.02142229676246643,
      "learning_rate": 0.0027599451303155008,
      "loss": 0.0025,
      "step": 2260
    },
    {
      "epoch": 3.113854595336077,
      "grad_norm": 0.7401732206344604,
      "learning_rate": 0.0027544581618655696,
      "loss": 0.003,
      "step": 2270
    },
    {
      "epoch": 3.1275720164609053,
      "grad_norm": 0.13516952097415924,
      "learning_rate": 0.002748971193415638,
      "loss": 0.0027,
      "step": 2280
    },
    {
      "epoch": 3.141289437585734,
      "grad_norm": 0.025928517803549767,
      "learning_rate": 0.0027434842249657067,
      "loss": 0.0029,
      "step": 2290
    },
    {
      "epoch": 3.1550068587105624,
      "grad_norm": 0.06315171718597412,
      "learning_rate": 0.002737997256515775,
      "loss": 0.0112,
      "step": 2300
    },
    {
      "epoch": 3.168724279835391,
      "grad_norm": 0.11292346566915512,
      "learning_rate": 0.002732510288065844,
      "loss": 0.0075,
      "step": 2310
    },
    {
      "epoch": 3.1824417009602195,
      "grad_norm": 0.4172564744949341,
      "learning_rate": 0.0027270233196159126,
      "loss": 0.0024,
      "step": 2320
    },
    {
      "epoch": 3.196159122085048,
      "grad_norm": 1.852839708328247,
      "learning_rate": 0.002721536351165981,
      "loss": 0.006,
      "step": 2330
    },
    {
      "epoch": 3.2098765432098766,
      "grad_norm": 0.030120542272925377,
      "learning_rate": 0.0027160493827160497,
      "loss": 0.004,
      "step": 2340
    },
    {
      "epoch": 3.223593964334705,
      "grad_norm": 0.11482139676809311,
      "learning_rate": 0.0027105624142661176,
      "loss": 0.004,
      "step": 2350
    },
    {
      "epoch": 3.2373113854595337,
      "grad_norm": 0.06720826029777527,
      "learning_rate": 0.0027050754458161864,
      "loss": 0.0021,
      "step": 2360
    },
    {
      "epoch": 3.251028806584362,
      "grad_norm": 0.02401961386203766,
      "learning_rate": 0.002699588477366255,
      "loss": 0.0015,
      "step": 2370
    },
    {
      "epoch": 3.2647462277091908,
      "grad_norm": 0.02388683333992958,
      "learning_rate": 0.0026941015089163235,
      "loss": 0.0012,
      "step": 2380
    },
    {
      "epoch": 3.2784636488340193,
      "grad_norm": 0.00940149836242199,
      "learning_rate": 0.0026886145404663923,
      "loss": 0.0008,
      "step": 2390
    },
    {
      "epoch": 3.292181069958848,
      "grad_norm": 0.08504175394773483,
      "learning_rate": 0.002683127572016461,
      "loss": 0.0006,
      "step": 2400
    },
    {
      "epoch": 3.3058984910836764,
      "grad_norm": 0.11855009198188782,
      "learning_rate": 0.0026776406035665294,
      "loss": 0.0006,
      "step": 2410
    },
    {
      "epoch": 3.319615912208505,
      "grad_norm": 0.00688385684043169,
      "learning_rate": 0.0026721536351165982,
      "loss": 0.0004,
      "step": 2420
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.005145902745425701,
      "learning_rate": 0.0026666666666666666,
      "loss": 0.0004,
      "step": 2430
    },
    {
      "epoch": 3.347050754458162,
      "grad_norm": 0.00445746211335063,
      "learning_rate": 0.0026611796982167354,
      "loss": 0.0004,
      "step": 2440
    },
    {
      "epoch": 3.3607681755829906,
      "grad_norm": 0.010125778615474701,
      "learning_rate": 0.002655692729766804,
      "loss": 0.0004,
      "step": 2450
    },
    {
      "epoch": 3.374485596707819,
      "grad_norm": 0.002951627131551504,
      "learning_rate": 0.0026502057613168725,
      "loss": 0.0004,
      "step": 2460
    },
    {
      "epoch": 3.388203017832647,
      "grad_norm": 0.002508001634851098,
      "learning_rate": 0.0026447187928669413,
      "loss": 0.0003,
      "step": 2470
    },
    {
      "epoch": 3.401920438957476,
      "grad_norm": 0.0026546609587967396,
      "learning_rate": 0.0026392318244170096,
      "loss": 0.0003,
      "step": 2480
    },
    {
      "epoch": 3.4156378600823043,
      "grad_norm": 0.004047069698572159,
      "learning_rate": 0.0026337448559670784,
      "loss": 0.0003,
      "step": 2490
    },
    {
      "epoch": 3.4293552812071333,
      "grad_norm": 0.00400233268737793,
      "learning_rate": 0.002628257887517147,
      "loss": 0.0003,
      "step": 2500
    },
    {
      "epoch": 3.4430727023319614,
      "grad_norm": 0.0021905070170760155,
      "learning_rate": 0.0026227709190672155,
      "loss": 0.0003,
      "step": 2510
    },
    {
      "epoch": 3.45679012345679,
      "grad_norm": 0.002513401908800006,
      "learning_rate": 0.002617283950617284,
      "loss": 0.0003,
      "step": 2520
    },
    {
      "epoch": 3.4705075445816185,
      "grad_norm": 0.002495032735168934,
      "learning_rate": 0.0026117969821673522,
      "loss": 0.0003,
      "step": 2530
    },
    {
      "epoch": 3.484224965706447,
      "grad_norm": 0.002583242254331708,
      "learning_rate": 0.002606310013717421,
      "loss": 0.0002,
      "step": 2540
    },
    {
      "epoch": 3.4979423868312756,
      "grad_norm": 0.002090156776830554,
      "learning_rate": 0.0026008230452674898,
      "loss": 0.0003,
      "step": 2550
    },
    {
      "epoch": 3.511659807956104,
      "grad_norm": 0.007597356103360653,
      "learning_rate": 0.002595336076817558,
      "loss": 0.0002,
      "step": 2560
    },
    {
      "epoch": 3.5253772290809327,
      "grad_norm": 0.0020055435597896576,
      "learning_rate": 0.002589849108367627,
      "loss": 0.0002,
      "step": 2570
    },
    {
      "epoch": 3.539094650205761,
      "grad_norm": 0.00487479055300355,
      "learning_rate": 0.0025843621399176953,
      "loss": 0.0002,
      "step": 2580
    },
    {
      "epoch": 3.5528120713305897,
      "grad_norm": 0.0018157637678086758,
      "learning_rate": 0.002578875171467764,
      "loss": 0.0003,
      "step": 2590
    },
    {
      "epoch": 3.5665294924554183,
      "grad_norm": 0.017930366098880768,
      "learning_rate": 0.002573388203017833,
      "loss": 0.0003,
      "step": 2600
    },
    {
      "epoch": 3.580246913580247,
      "grad_norm": 0.005410470068454742,
      "learning_rate": 0.002567901234567901,
      "loss": 0.0002,
      "step": 2610
    },
    {
      "epoch": 3.5939643347050754,
      "grad_norm": 0.002658593701198697,
      "learning_rate": 0.00256241426611797,
      "loss": 0.0002,
      "step": 2620
    },
    {
      "epoch": 3.607681755829904,
      "grad_norm": 0.008872374892234802,
      "learning_rate": 0.0025569272976680387,
      "loss": 0.0003,
      "step": 2630
    },
    {
      "epoch": 3.6213991769547325,
      "grad_norm": 0.031665947288274765,
      "learning_rate": 0.002551440329218107,
      "loss": 0.0003,
      "step": 2640
    },
    {
      "epoch": 3.635116598079561,
      "grad_norm": 0.0031069600954651833,
      "learning_rate": 0.002545953360768176,
      "loss": 0.0002,
      "step": 2650
    },
    {
      "epoch": 3.6488340192043895,
      "grad_norm": 0.00202539237216115,
      "learning_rate": 0.002540466392318244,
      "loss": 0.0002,
      "step": 2660
    },
    {
      "epoch": 3.662551440329218,
      "grad_norm": 0.0023754937574267387,
      "learning_rate": 0.002534979423868313,
      "loss": 0.0005,
      "step": 2670
    },
    {
      "epoch": 3.6762688614540466,
      "grad_norm": 0.0036066994071006775,
      "learning_rate": 0.0025294924554183818,
      "loss": 0.0003,
      "step": 2680
    },
    {
      "epoch": 3.689986282578875,
      "grad_norm": 0.0048004393465816975,
      "learning_rate": 0.0025240054869684497,
      "loss": 0.0002,
      "step": 2690
    },
    {
      "epoch": 3.7037037037037037,
      "grad_norm": 0.0022084855008870363,
      "learning_rate": 0.0025185185185185185,
      "loss": 0.0002,
      "step": 2700
    },
    {
      "epoch": 3.7174211248285323,
      "grad_norm": 0.002644112566486001,
      "learning_rate": 0.002513031550068587,
      "loss": 0.0002,
      "step": 2710
    },
    {
      "epoch": 3.731138545953361,
      "grad_norm": 0.004102760925889015,
      "learning_rate": 0.0025075445816186556,
      "loss": 0.0002,
      "step": 2720
    },
    {
      "epoch": 3.7448559670781894,
      "grad_norm": 0.004511853214353323,
      "learning_rate": 0.0025020576131687244,
      "loss": 0.0002,
      "step": 2730
    },
    {
      "epoch": 3.758573388203018,
      "grad_norm": 0.004910784773528576,
      "learning_rate": 0.0024965706447187927,
      "loss": 0.0002,
      "step": 2740
    },
    {
      "epoch": 3.7722908093278464,
      "grad_norm": 0.0016586155397817492,
      "learning_rate": 0.0024910836762688615,
      "loss": 0.0002,
      "step": 2750
    },
    {
      "epoch": 3.786008230452675,
      "grad_norm": 0.004278423264622688,
      "learning_rate": 0.00248559670781893,
      "loss": 0.0002,
      "step": 2760
    },
    {
      "epoch": 3.7997256515775035,
      "grad_norm": 0.001237638178281486,
      "learning_rate": 0.0024801097393689986,
      "loss": 0.0002,
      "step": 2770
    },
    {
      "epoch": 3.813443072702332,
      "grad_norm": 0.0033684903755784035,
      "learning_rate": 0.0024746227709190674,
      "loss": 0.0002,
      "step": 2780
    },
    {
      "epoch": 3.8271604938271606,
      "grad_norm": 0.0025746922474354506,
      "learning_rate": 0.0024691358024691358,
      "loss": 0.0002,
      "step": 2790
    },
    {
      "epoch": 3.840877914951989,
      "grad_norm": 0.0012643272057175636,
      "learning_rate": 0.0024636488340192045,
      "loss": 0.0002,
      "step": 2800
    },
    {
      "epoch": 3.8545953360768177,
      "grad_norm": 0.0020842733792960644,
      "learning_rate": 0.0024581618655692733,
      "loss": 0.0002,
      "step": 2810
    },
    {
      "epoch": 3.8683127572016462,
      "grad_norm": 0.001501398510299623,
      "learning_rate": 0.0024526748971193417,
      "loss": 0.0002,
      "step": 2820
    },
    {
      "epoch": 3.882030178326475,
      "grad_norm": 0.0027897374238818884,
      "learning_rate": 0.0024471879286694104,
      "loss": 0.0002,
      "step": 2830
    },
    {
      "epoch": 3.895747599451303,
      "grad_norm": 0.003549241228029132,
      "learning_rate": 0.002441700960219479,
      "loss": 0.0002,
      "step": 2840
    },
    {
      "epoch": 3.909465020576132,
      "grad_norm": 0.0017101753037422895,
      "learning_rate": 0.0024362139917695476,
      "loss": 0.0002,
      "step": 2850
    },
    {
      "epoch": 3.92318244170096,
      "grad_norm": 0.0019579664804041386,
      "learning_rate": 0.0024307270233196164,
      "loss": 0.0002,
      "step": 2860
    },
    {
      "epoch": 3.936899862825789,
      "grad_norm": 0.0023383598309010267,
      "learning_rate": 0.0024252400548696843,
      "loss": 0.0001,
      "step": 2870
    },
    {
      "epoch": 3.950617283950617,
      "grad_norm": 0.0015113148838281631,
      "learning_rate": 0.002419753086419753,
      "loss": 0.0002,
      "step": 2880
    },
    {
      "epoch": 3.964334705075446,
      "grad_norm": 0.005561290308833122,
      "learning_rate": 0.0024142661179698214,
      "loss": 0.0001,
      "step": 2890
    },
    {
      "epoch": 3.978052126200274,
      "grad_norm": 0.001230288646183908,
      "learning_rate": 0.00240877914951989,
      "loss": 0.0002,
      "step": 2900
    },
    {
      "epoch": 3.991769547325103,
      "grad_norm": 0.0015020390274003148,
      "learning_rate": 0.002403292181069959,
      "loss": 0.0001,
      "step": 2910
    },
    {
      "epoch": 4.005486968449931,
      "grad_norm": 0.0017874010372906923,
      "learning_rate": 0.0023978052126200273,
      "loss": 0.0001,
      "step": 2920
    },
    {
      "epoch": 4.01920438957476,
      "grad_norm": 0.0011951122432947159,
      "learning_rate": 0.002392318244170096,
      "loss": 0.0002,
      "step": 2930
    },
    {
      "epoch": 4.032921810699588,
      "grad_norm": 0.0010632078628987074,
      "learning_rate": 0.0023868312757201644,
      "loss": 0.0002,
      "step": 2940
    },
    {
      "epoch": 4.046639231824417,
      "grad_norm": 0.0015212746802717447,
      "learning_rate": 0.002381344307270233,
      "loss": 0.0001,
      "step": 2950
    },
    {
      "epoch": 4.060356652949245,
      "grad_norm": 0.002663977211341262,
      "learning_rate": 0.002375857338820302,
      "loss": 0.0002,
      "step": 2960
    },
    {
      "epoch": 4.074074074074074,
      "grad_norm": 0.0016571758314967155,
      "learning_rate": 0.0023703703703703703,
      "loss": 0.0001,
      "step": 2970
    },
    {
      "epoch": 4.0877914951989025,
      "grad_norm": 0.0014706564834341407,
      "learning_rate": 0.002364883401920439,
      "loss": 0.0002,
      "step": 2980
    },
    {
      "epoch": 4.1015089163237315,
      "grad_norm": 0.0011087526800110936,
      "learning_rate": 0.002359396433470508,
      "loss": 0.0002,
      "step": 2990
    },
    {
      "epoch": 4.11522633744856,
      "grad_norm": 0.0018552889814600348,
      "learning_rate": 0.0023539094650205762,
      "loss": 0.0002,
      "step": 3000
    },
    {
      "epoch": 4.128943758573389,
      "grad_norm": 0.0018833172507584095,
      "learning_rate": 0.002348422496570645,
      "loss": 0.0001,
      "step": 3010
    },
    {
      "epoch": 4.142661179698217,
      "grad_norm": 0.0014493359485641122,
      "learning_rate": 0.0023429355281207134,
      "loss": 0.0002,
      "step": 3020
    },
    {
      "epoch": 4.156378600823046,
      "grad_norm": 0.0011537574464455247,
      "learning_rate": 0.002337448559670782,
      "loss": 0.0001,
      "step": 3030
    },
    {
      "epoch": 4.170096021947874,
      "grad_norm": 0.0013885247753933072,
      "learning_rate": 0.0023319615912208505,
      "loss": 0.0002,
      "step": 3040
    },
    {
      "epoch": 4.183813443072703,
      "grad_norm": 0.005427801050245762,
      "learning_rate": 0.002326474622770919,
      "loss": 0.0001,
      "step": 3050
    },
    {
      "epoch": 4.197530864197531,
      "grad_norm": 0.0011422134703025222,
      "learning_rate": 0.0023209876543209876,
      "loss": 0.0001,
      "step": 3060
    },
    {
      "epoch": 4.211248285322359,
      "grad_norm": 0.003809132380411029,
      "learning_rate": 0.002315500685871056,
      "loss": 0.0001,
      "step": 3070
    },
    {
      "epoch": 4.224965706447188,
      "grad_norm": 0.0016624514246359468,
      "learning_rate": 0.0023100137174211248,
      "loss": 0.0001,
      "step": 3080
    },
    {
      "epoch": 4.238683127572016,
      "grad_norm": 0.0012955807615071535,
      "learning_rate": 0.0023045267489711935,
      "loss": 0.0001,
      "step": 3090
    },
    {
      "epoch": 4.252400548696845,
      "grad_norm": 0.0011392554733902216,
      "learning_rate": 0.002299039780521262,
      "loss": 0.0001,
      "step": 3100
    },
    {
      "epoch": 4.266117969821673,
      "grad_norm": 0.0010633831843733788,
      "learning_rate": 0.0022935528120713307,
      "loss": 0.0001,
      "step": 3110
    },
    {
      "epoch": 4.279835390946502,
      "grad_norm": 0.0007296721450984478,
      "learning_rate": 0.002288065843621399,
      "loss": 0.0001,
      "step": 3120
    },
    {
      "epoch": 4.29355281207133,
      "grad_norm": 0.0009504772606305778,
      "learning_rate": 0.002282578875171468,
      "loss": 0.0001,
      "step": 3130
    },
    {
      "epoch": 4.307270233196159,
      "grad_norm": 0.0019498406909406185,
      "learning_rate": 0.0022770919067215366,
      "loss": 0.0001,
      "step": 3140
    },
    {
      "epoch": 4.320987654320987,
      "grad_norm": 0.002592818345874548,
      "learning_rate": 0.002271604938271605,
      "loss": 0.0001,
      "step": 3150
    },
    {
      "epoch": 4.334705075445816,
      "grad_norm": 0.001106193638406694,
      "learning_rate": 0.0022661179698216737,
      "loss": 0.0001,
      "step": 3160
    },
    {
      "epoch": 4.348422496570644,
      "grad_norm": 0.0013510981807485223,
      "learning_rate": 0.0022606310013717425,
      "loss": 0.0001,
      "step": 3170
    },
    {
      "epoch": 4.362139917695473,
      "grad_norm": 0.001131572644226253,
      "learning_rate": 0.002255144032921811,
      "loss": 0.0001,
      "step": 3180
    },
    {
      "epoch": 4.3758573388203015,
      "grad_norm": 0.001454192795790732,
      "learning_rate": 0.0022496570644718796,
      "loss": 0.0001,
      "step": 3190
    },
    {
      "epoch": 4.3895747599451305,
      "grad_norm": 0.0006763855344615877,
      "learning_rate": 0.002244170096021948,
      "loss": 0.0001,
      "step": 3200
    },
    {
      "epoch": 4.403292181069959,
      "grad_norm": 0.0014144409215077758,
      "learning_rate": 0.0022386831275720167,
      "loss": 0.0001,
      "step": 3210
    },
    {
      "epoch": 4.4170096021947876,
      "grad_norm": 0.0011557945981621742,
      "learning_rate": 0.002233196159122085,
      "loss": 0.0001,
      "step": 3220
    },
    {
      "epoch": 4.430727023319616,
      "grad_norm": 0.0010556734632700682,
      "learning_rate": 0.0022277091906721534,
      "loss": 0.0001,
      "step": 3230
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.0010192819172516465,
      "learning_rate": 0.0022222222222222222,
      "loss": 0.0001,
      "step": 3240
    },
    {
      "epoch": 4.458161865569273,
      "grad_norm": 0.0016562347300350666,
      "learning_rate": 0.0022167352537722906,
      "loss": 0.0001,
      "step": 3250
    },
    {
      "epoch": 4.471879286694102,
      "grad_norm": 0.0013456550659611821,
      "learning_rate": 0.0022112482853223593,
      "loss": 0.0001,
      "step": 3260
    },
    {
      "epoch": 4.48559670781893,
      "grad_norm": 0.0017605352913960814,
      "learning_rate": 0.002205761316872428,
      "loss": 0.0001,
      "step": 3270
    },
    {
      "epoch": 4.499314128943759,
      "grad_norm": 0.000689368462190032,
      "learning_rate": 0.0022002743484224965,
      "loss": 0.0001,
      "step": 3280
    },
    {
      "epoch": 4.513031550068587,
      "grad_norm": 0.0013552384916692972,
      "learning_rate": 0.0021947873799725653,
      "loss": 0.0001,
      "step": 3290
    },
    {
      "epoch": 4.526748971193416,
      "grad_norm": 0.0007789325900375843,
      "learning_rate": 0.0021893004115226336,
      "loss": 0.0001,
      "step": 3300
    },
    {
      "epoch": 4.540466392318244,
      "grad_norm": 0.002561878878623247,
      "learning_rate": 0.0021838134430727024,
      "loss": 0.0001,
      "step": 3310
    },
    {
      "epoch": 4.554183813443073,
      "grad_norm": 0.0015518440632149577,
      "learning_rate": 0.002178326474622771,
      "loss": 0.0001,
      "step": 3320
    },
    {
      "epoch": 4.567901234567901,
      "grad_norm": 0.0011058344971388578,
      "learning_rate": 0.0021728395061728395,
      "loss": 0.0001,
      "step": 3330
    },
    {
      "epoch": 4.58161865569273,
      "grad_norm": 0.0008706008084118366,
      "learning_rate": 0.0021673525377229083,
      "loss": 0.0001,
      "step": 3340
    },
    {
      "epoch": 4.595336076817558,
      "grad_norm": 0.0010210041655227542,
      "learning_rate": 0.0021618655692729766,
      "loss": 0.0001,
      "step": 3350
    },
    {
      "epoch": 4.609053497942387,
      "grad_norm": 0.0025373089592903852,
      "learning_rate": 0.0021563786008230454,
      "loss": 0.0001,
      "step": 3360
    },
    {
      "epoch": 4.622770919067215,
      "grad_norm": 0.001905267359688878,
      "learning_rate": 0.002150891632373114,
      "loss": 0.0001,
      "step": 3370
    },
    {
      "epoch": 4.636488340192044,
      "grad_norm": 0.0038350936956703663,
      "learning_rate": 0.0021454046639231826,
      "loss": 0.0011,
      "step": 3380
    },
    {
      "epoch": 4.650205761316872,
      "grad_norm": 0.03215411677956581,
      "learning_rate": 0.002139917695473251,
      "loss": 0.0015,
      "step": 3390
    },
    {
      "epoch": 4.663923182441701,
      "grad_norm": 0.050950098782777786,
      "learning_rate": 0.0021344307270233197,
      "loss": 0.0114,
      "step": 3400
    },
    {
      "epoch": 4.677640603566529,
      "grad_norm": 0.22245721518993378,
      "learning_rate": 0.002128943758573388,
      "loss": 0.0079,
      "step": 3410
    },
    {
      "epoch": 4.6913580246913575,
      "grad_norm": 0.05828854441642761,
      "learning_rate": 0.002123456790123457,
      "loss": 0.0045,
      "step": 3420
    },
    {
      "epoch": 4.7050754458161865,
      "grad_norm": 0.17242659628391266,
      "learning_rate": 0.002117969821673525,
      "loss": 0.0037,
      "step": 3430
    },
    {
      "epoch": 4.7187928669410155,
      "grad_norm": 0.3634680509567261,
      "learning_rate": 0.002112482853223594,
      "loss": 0.0029,
      "step": 3440
    },
    {
      "epoch": 4.732510288065844,
      "grad_norm": 0.028742166236042976,
      "learning_rate": 0.0021069958847736627,
      "loss": 0.0015,
      "step": 3450
    },
    {
      "epoch": 4.746227709190672,
      "grad_norm": 0.20705001056194305,
      "learning_rate": 0.002101508916323731,
      "loss": 0.0011,
      "step": 3460
    },
    {
      "epoch": 4.759945130315501,
      "grad_norm": 0.02369321510195732,
      "learning_rate": 0.0020960219478738,
      "loss": 0.0006,
      "step": 3470
    },
    {
      "epoch": 4.77366255144033,
      "grad_norm": 0.004729756619781256,
      "learning_rate": 0.002090534979423868,
      "loss": 0.0004,
      "step": 3480
    },
    {
      "epoch": 4.787379972565158,
      "grad_norm": 0.004917320795357227,
      "learning_rate": 0.002085048010973937,
      "loss": 0.0005,
      "step": 3490
    },
    {
      "epoch": 4.801097393689986,
      "grad_norm": 0.018097400665283203,
      "learning_rate": 0.0020795610425240058,
      "loss": 0.0003,
      "step": 3500
    },
    {
      "epoch": 4.814814814814815,
      "grad_norm": 0.0038112145848572254,
      "learning_rate": 0.002074074074074074,
      "loss": 0.0003,
      "step": 3510
    },
    {
      "epoch": 4.828532235939643,
      "grad_norm": 0.002780374838039279,
      "learning_rate": 0.002068587105624143,
      "loss": 0.0004,
      "step": 3520
    },
    {
      "epoch": 4.842249657064472,
      "grad_norm": 0.05895451828837395,
      "learning_rate": 0.0020631001371742112,
      "loss": 0.0015,
      "step": 3530
    },
    {
      "epoch": 4.8559670781893,
      "grad_norm": 0.0640050619840622,
      "learning_rate": 0.00205761316872428,
      "loss": 0.0013,
      "step": 3540
    },
    {
      "epoch": 4.869684499314129,
      "grad_norm": 0.03705447539687157,
      "learning_rate": 0.002052126200274349,
      "loss": 0.0018,
      "step": 3550
    },
    {
      "epoch": 4.883401920438957,
      "grad_norm": 0.008830877020955086,
      "learning_rate": 0.0020466392318244167,
      "loss": 0.001,
      "step": 3560
    },
    {
      "epoch": 4.897119341563786,
      "grad_norm": 0.018128203228116035,
      "learning_rate": 0.0020411522633744855,
      "loss": 0.0005,
      "step": 3570
    },
    {
      "epoch": 4.910836762688614,
      "grad_norm": 0.030765797942876816,
      "learning_rate": 0.0020356652949245543,
      "loss": 0.0008,
      "step": 3580
    },
    {
      "epoch": 4.924554183813443,
      "grad_norm": 0.45671403408050537,
      "learning_rate": 0.0020301783264746226,
      "loss": 0.0009,
      "step": 3590
    },
    {
      "epoch": 4.938271604938271,
      "grad_norm": 0.028181204572319984,
      "learning_rate": 0.0020246913580246914,
      "loss": 0.0006,
      "step": 3600
    },
    {
      "epoch": 4.9519890260631,
      "grad_norm": 0.040401920676231384,
      "learning_rate": 0.0020192043895747597,
      "loss": 0.0005,
      "step": 3610
    },
    {
      "epoch": 4.965706447187928,
      "grad_norm": 0.03618493676185608,
      "learning_rate": 0.0020137174211248285,
      "loss": 0.0009,
      "step": 3620
    },
    {
      "epoch": 4.979423868312757,
      "grad_norm": 0.035240333527326584,
      "learning_rate": 0.0020082304526748973,
      "loss": 0.001,
      "step": 3630
    },
    {
      "epoch": 4.9931412894375855,
      "grad_norm": 0.741183876991272,
      "learning_rate": 0.0020027434842249657,
      "loss": 0.0018,
      "step": 3640
    },
    {
      "epoch": 5.0068587105624145,
      "grad_norm": 0.04091452434659004,
      "learning_rate": 0.0019972565157750344,
      "loss": 0.0166,
      "step": 3650
    },
    {
      "epoch": 5.020576131687243,
      "grad_norm": 3.2058873176574707,
      "learning_rate": 0.0019917695473251028,
      "loss": 0.0101,
      "step": 3660
    },
    {
      "epoch": 5.034293552812072,
      "grad_norm": 0.7214604020118713,
      "learning_rate": 0.0019862825788751716,
      "loss": 0.0071,
      "step": 3670
    },
    {
      "epoch": 5.0480109739369,
      "grad_norm": 0.09746865183115005,
      "learning_rate": 0.00198079561042524,
      "loss": 0.0152,
      "step": 3680
    },
    {
      "epoch": 5.061728395061729,
      "grad_norm": 1.0571846961975098,
      "learning_rate": 0.0019753086419753087,
      "loss": 0.0094,
      "step": 3690
    },
    {
      "epoch": 5.075445816186557,
      "grad_norm": 0.47824931144714355,
      "learning_rate": 0.0019698216735253775,
      "loss": 0.0056,
      "step": 3700
    },
    {
      "epoch": 5.089163237311386,
      "grad_norm": 0.06214536726474762,
      "learning_rate": 0.001964334705075446,
      "loss": 0.0048,
      "step": 3710
    },
    {
      "epoch": 5.102880658436214,
      "grad_norm": 0.33135801553726196,
      "learning_rate": 0.0019588477366255146,
      "loss": 0.0044,
      "step": 3720
    },
    {
      "epoch": 5.116598079561043,
      "grad_norm": 1.0213576555252075,
      "learning_rate": 0.001953360768175583,
      "loss": 0.0093,
      "step": 3730
    },
    {
      "epoch": 5.130315500685871,
      "grad_norm": 0.11252765357494354,
      "learning_rate": 0.0019478737997256517,
      "loss": 0.0094,
      "step": 3740
    },
    {
      "epoch": 5.1440329218107,
      "grad_norm": 0.5879969596862793,
      "learning_rate": 0.0019423868312757203,
      "loss": 0.0042,
      "step": 3750
    },
    {
      "epoch": 5.157750342935528,
      "grad_norm": 5.3987531661987305,
      "learning_rate": 0.0019368998628257889,
      "loss": 0.0204,
      "step": 3760
    },
    {
      "epoch": 5.171467764060357,
      "grad_norm": 0.2576494812965393,
      "learning_rate": 0.0019314128943758574,
      "loss": 0.0086,
      "step": 3770
    },
    {
      "epoch": 5.185185185185185,
      "grad_norm": 2.721618413925171,
      "learning_rate": 0.0019259259259259258,
      "loss": 0.0062,
      "step": 3780
    },
    {
      "epoch": 5.198902606310014,
      "grad_norm": 0.08590612560510635,
      "learning_rate": 0.0019204389574759945,
      "loss": 0.0034,
      "step": 3790
    },
    {
      "epoch": 5.212620027434842,
      "grad_norm": 0.5315452814102173,
      "learning_rate": 0.0019149519890260631,
      "loss": 0.0032,
      "step": 3800
    },
    {
      "epoch": 5.226337448559671,
      "grad_norm": 0.038113635033369064,
      "learning_rate": 0.0019094650205761317,
      "loss": 0.0023,
      "step": 3810
    },
    {
      "epoch": 5.240054869684499,
      "grad_norm": 0.018430158495903015,
      "learning_rate": 0.0019039780521262002,
      "loss": 0.0018,
      "step": 3820
    },
    {
      "epoch": 5.253772290809328,
      "grad_norm": 0.04861607402563095,
      "learning_rate": 0.001898491083676269,
      "loss": 0.0019,
      "step": 3830
    },
    {
      "epoch": 5.267489711934156,
      "grad_norm": 0.022092241793870926,
      "learning_rate": 0.0018930041152263376,
      "loss": 0.0019,
      "step": 3840
    },
    {
      "epoch": 5.2812071330589845,
      "grad_norm": 0.14168572425842285,
      "learning_rate": 0.0018875171467764061,
      "loss": 0.0043,
      "step": 3850
    },
    {
      "epoch": 5.2949245541838135,
      "grad_norm": 0.07903286069631577,
      "learning_rate": 0.0018820301783264747,
      "loss": 0.003,
      "step": 3860
    },
    {
      "epoch": 5.308641975308642,
      "grad_norm": 1.0831542015075684,
      "learning_rate": 0.001876543209876543,
      "loss": 0.0038,
      "step": 3870
    },
    {
      "epoch": 5.322359396433471,
      "grad_norm": 0.057868074625730515,
      "learning_rate": 0.0018710562414266118,
      "loss": 0.0039,
      "step": 3880
    },
    {
      "epoch": 5.336076817558299,
      "grad_norm": 0.0900258794426918,
      "learning_rate": 0.0018655692729766804,
      "loss": 0.0057,
      "step": 3890
    },
    {
      "epoch": 5.349794238683128,
      "grad_norm": 0.06087141111493111,
      "learning_rate": 0.001860082304526749,
      "loss": 0.0039,
      "step": 3900
    },
    {
      "epoch": 5.363511659807956,
      "grad_norm": 0.6593502163887024,
      "learning_rate": 0.0018545953360768175,
      "loss": 0.004,
      "step": 3910
    },
    {
      "epoch": 5.377229080932785,
      "grad_norm": 0.03837457299232483,
      "learning_rate": 0.0018491083676268863,
      "loss": 0.0019,
      "step": 3920
    },
    {
      "epoch": 5.390946502057613,
      "grad_norm": 0.021857429295778275,
      "learning_rate": 0.0018436213991769549,
      "loss": 0.0013,
      "step": 3930
    },
    {
      "epoch": 5.404663923182442,
      "grad_norm": 0.021607758477330208,
      "learning_rate": 0.0018381344307270234,
      "loss": 0.001,
      "step": 3940
    },
    {
      "epoch": 5.41838134430727,
      "grad_norm": 0.033390529453754425,
      "learning_rate": 0.0018326474622770918,
      "loss": 0.0006,
      "step": 3950
    },
    {
      "epoch": 5.432098765432099,
      "grad_norm": 0.005275570787489414,
      "learning_rate": 0.0018271604938271604,
      "loss": 0.0005,
      "step": 3960
    },
    {
      "epoch": 5.445816186556927,
      "grad_norm": 0.005998058710247278,
      "learning_rate": 0.0018216735253772291,
      "loss": 0.0004,
      "step": 3970
    },
    {
      "epoch": 5.459533607681756,
      "grad_norm": 0.00481463223695755,
      "learning_rate": 0.0018161865569272977,
      "loss": 0.0004,
      "step": 3980
    },
    {
      "epoch": 5.473251028806584,
      "grad_norm": 0.006019135471433401,
      "learning_rate": 0.0018106995884773663,
      "loss": 0.0004,
      "step": 3990
    },
    {
      "epoch": 5.486968449931413,
      "grad_norm": 0.004948472138494253,
      "learning_rate": 0.0018052126200274348,
      "loss": 0.0004,
      "step": 4000
    },
    {
      "epoch": 5.500685871056241,
      "grad_norm": 0.003985697403550148,
      "learning_rate": 0.0017997256515775036,
      "loss": 0.0003,
      "step": 4010
    },
    {
      "epoch": 5.51440329218107,
      "grad_norm": 0.00525720464065671,
      "learning_rate": 0.0017942386831275722,
      "loss": 0.0003,
      "step": 4020
    },
    {
      "epoch": 5.528120713305898,
      "grad_norm": 0.0028504496440291405,
      "learning_rate": 0.0017887517146776407,
      "loss": 0.0003,
      "step": 4030
    },
    {
      "epoch": 5.541838134430727,
      "grad_norm": 0.004092346411198378,
      "learning_rate": 0.001783264746227709,
      "loss": 0.0008,
      "step": 4040
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 0.005246244370937347,
      "learning_rate": 0.0017777777777777776,
      "loss": 0.0004,
      "step": 4050
    },
    {
      "epoch": 5.569272976680384,
      "grad_norm": 0.009412189945578575,
      "learning_rate": 0.0017722908093278464,
      "loss": 0.0003,
      "step": 4060
    },
    {
      "epoch": 5.5829903978052124,
      "grad_norm": 0.0025669927708804607,
      "learning_rate": 0.001766803840877915,
      "loss": 0.0003,
      "step": 4070
    },
    {
      "epoch": 5.596707818930041,
      "grad_norm": 0.0031320243142545223,
      "learning_rate": 0.0017613168724279836,
      "loss": 0.0003,
      "step": 4080
    },
    {
      "epoch": 5.6104252400548695,
      "grad_norm": 0.007580454461276531,
      "learning_rate": 0.0017558299039780521,
      "loss": 0.0003,
      "step": 4090
    },
    {
      "epoch": 5.6241426611796985,
      "grad_norm": 0.013372867368161678,
      "learning_rate": 0.001750342935528121,
      "loss": 0.0003,
      "step": 4100
    },
    {
      "epoch": 5.637860082304527,
      "grad_norm": 0.013668564148247242,
      "learning_rate": 0.0017448559670781895,
      "loss": 0.0003,
      "step": 4110
    },
    {
      "epoch": 5.651577503429356,
      "grad_norm": 0.0029545100405812263,
      "learning_rate": 0.001739368998628258,
      "loss": 0.0003,
      "step": 4120
    },
    {
      "epoch": 5.665294924554184,
      "grad_norm": 0.0021155420690774918,
      "learning_rate": 0.0017338820301783264,
      "loss": 0.0005,
      "step": 4130
    },
    {
      "epoch": 5.679012345679013,
      "grad_norm": 0.009846951812505722,
      "learning_rate": 0.001728395061728395,
      "loss": 0.0003,
      "step": 4140
    },
    {
      "epoch": 5.692729766803841,
      "grad_norm": 0.01772589422762394,
      "learning_rate": 0.0017229080932784637,
      "loss": 0.0016,
      "step": 4150
    },
    {
      "epoch": 5.70644718792867,
      "grad_norm": 0.013434678316116333,
      "learning_rate": 0.0017174211248285323,
      "loss": 0.0032,
      "step": 4160
    },
    {
      "epoch": 5.720164609053498,
      "grad_norm": 0.15452760457992554,
      "learning_rate": 0.0017119341563786008,
      "loss": 0.012,
      "step": 4170
    },
    {
      "epoch": 5.733882030178327,
      "grad_norm": 0.3430476486682892,
      "learning_rate": 0.0017064471879286694,
      "loss": 0.015,
      "step": 4180
    },
    {
      "epoch": 5.747599451303155,
      "grad_norm": 0.41648128628730774,
      "learning_rate": 0.0017009602194787382,
      "loss": 0.0126,
      "step": 4190
    },
    {
      "epoch": 5.761316872427983,
      "grad_norm": 0.10218796879053116,
      "learning_rate": 0.0016954732510288068,
      "loss": 0.0051,
      "step": 4200
    },
    {
      "epoch": 5.775034293552812,
      "grad_norm": 0.2926512062549591,
      "learning_rate": 0.001689986282578875,
      "loss": 0.0033,
      "step": 4210
    },
    {
      "epoch": 5.788751714677641,
      "grad_norm": 0.021196478977799416,
      "learning_rate": 0.0016844993141289437,
      "loss": 0.0016,
      "step": 4220
    },
    {
      "epoch": 5.802469135802469,
      "grad_norm": 0.010758630931377411,
      "learning_rate": 0.0016790123456790122,
      "loss": 0.0011,
      "step": 4230
    },
    {
      "epoch": 5.816186556927297,
      "grad_norm": 0.015994027256965637,
      "learning_rate": 0.001673525377229081,
      "loss": 0.0009,
      "step": 4240
    },
    {
      "epoch": 5.829903978052126,
      "grad_norm": 0.007134346757084131,
      "learning_rate": 0.0016680384087791496,
      "loss": 0.0008,
      "step": 4250
    },
    {
      "epoch": 5.843621399176955,
      "grad_norm": 0.008689332753419876,
      "learning_rate": 0.0016625514403292181,
      "loss": 0.0006,
      "step": 4260
    },
    {
      "epoch": 5.857338820301783,
      "grad_norm": 0.009823981672525406,
      "learning_rate": 0.0016570644718792867,
      "loss": 0.0006,
      "step": 4270
    },
    {
      "epoch": 5.871056241426611,
      "grad_norm": 0.00633180933073163,
      "learning_rate": 0.0016515775034293555,
      "loss": 0.0005,
      "step": 4280
    },
    {
      "epoch": 5.88477366255144,
      "grad_norm": 0.042111579328775406,
      "learning_rate": 0.001646090534979424,
      "loss": 0.0005,
      "step": 4290
    },
    {
      "epoch": 5.8984910836762685,
      "grad_norm": 0.004138863645493984,
      "learning_rate": 0.0016406035665294924,
      "loss": 0.0004,
      "step": 4300
    },
    {
      "epoch": 5.9122085048010975,
      "grad_norm": 0.004206242971122265,
      "learning_rate": 0.001635116598079561,
      "loss": 0.0006,
      "step": 4310
    },
    {
      "epoch": 5.925925925925926,
      "grad_norm": 0.005404283758252859,
      "learning_rate": 0.0016296296296296295,
      "loss": 0.0004,
      "step": 4320
    },
    {
      "epoch": 5.939643347050755,
      "grad_norm": 0.003564969403669238,
      "learning_rate": 0.0016241426611796983,
      "loss": 0.0004,
      "step": 4330
    },
    {
      "epoch": 5.953360768175583,
      "grad_norm": 0.004208914469927549,
      "learning_rate": 0.0016186556927297669,
      "loss": 0.0006,
      "step": 4340
    },
    {
      "epoch": 5.967078189300412,
      "grad_norm": 0.01327685359865427,
      "learning_rate": 0.0016131687242798354,
      "loss": 0.0004,
      "step": 4350
    },
    {
      "epoch": 5.98079561042524,
      "grad_norm": 0.004332383628934622,
      "learning_rate": 0.001607681755829904,
      "loss": 0.0006,
      "step": 4360
    },
    {
      "epoch": 5.994513031550069,
      "grad_norm": 0.003572257701307535,
      "learning_rate": 0.0016021947873799728,
      "loss": 0.0004,
      "step": 4370
    },
    {
      "epoch": 6.008230452674897,
      "grad_norm": 0.0037395984400063753,
      "learning_rate": 0.0015967078189300413,
      "loss": 0.0005,
      "step": 4380
    },
    {
      "epoch": 6.021947873799726,
      "grad_norm": 0.00413960637524724,
      "learning_rate": 0.0015912208504801097,
      "loss": 0.0003,
      "step": 4390
    },
    {
      "epoch": 6.035665294924554,
      "grad_norm": 0.006720247212797403,
      "learning_rate": 0.0015857338820301783,
      "loss": 0.0003,
      "step": 4400
    },
    {
      "epoch": 6.049382716049383,
      "grad_norm": 0.0028612203896045685,
      "learning_rate": 0.0015802469135802468,
      "loss": 0.0003,
      "step": 4410
    },
    {
      "epoch": 6.063100137174211,
      "grad_norm": 0.0020260175224393606,
      "learning_rate": 0.0015747599451303156,
      "loss": 0.0003,
      "step": 4420
    },
    {
      "epoch": 6.07681755829904,
      "grad_norm": 0.0025771083310246468,
      "learning_rate": 0.0015692729766803842,
      "loss": 0.0004,
      "step": 4430
    },
    {
      "epoch": 6.090534979423868,
      "grad_norm": 0.002787673147395253,
      "learning_rate": 0.0015637860082304527,
      "loss": 0.0003,
      "step": 4440
    },
    {
      "epoch": 6.104252400548697,
      "grad_norm": 0.002637429628521204,
      "learning_rate": 0.0015582990397805213,
      "loss": 0.0003,
      "step": 4450
    },
    {
      "epoch": 6.117969821673525,
      "grad_norm": 0.0022797402925789356,
      "learning_rate": 0.0015528120713305899,
      "loss": 0.0003,
      "step": 4460
    },
    {
      "epoch": 6.131687242798354,
      "grad_norm": 0.001945706782862544,
      "learning_rate": 0.0015473251028806586,
      "loss": 0.0002,
      "step": 4470
    },
    {
      "epoch": 6.145404663923182,
      "grad_norm": 0.003942638635635376,
      "learning_rate": 0.001541838134430727,
      "loss": 0.0002,
      "step": 4480
    },
    {
      "epoch": 6.159122085048011,
      "grad_norm": 0.0028641941025853157,
      "learning_rate": 0.0015363511659807956,
      "loss": 0.0002,
      "step": 4490
    },
    {
      "epoch": 6.172839506172839,
      "grad_norm": 0.0021266185212880373,
      "learning_rate": 0.0015308641975308641,
      "loss": 0.0003,
      "step": 4500
    },
    {
      "epoch": 6.186556927297668,
      "grad_norm": 0.005051765125244856,
      "learning_rate": 0.001525377229080933,
      "loss": 0.0006,
      "step": 4510
    },
    {
      "epoch": 6.2002743484224965,
      "grad_norm": 0.008463067933917046,
      "learning_rate": 0.0015198902606310015,
      "loss": 0.0005,
      "step": 4520
    },
    {
      "epoch": 6.2139917695473255,
      "grad_norm": 0.0034732508938759565,
      "learning_rate": 0.00151440329218107,
      "loss": 0.0003,
      "step": 4530
    },
    {
      "epoch": 6.227709190672154,
      "grad_norm": 0.015410933643579483,
      "learning_rate": 0.0015089163237311386,
      "loss": 0.0003,
      "step": 4540
    },
    {
      "epoch": 6.2414266117969825,
      "grad_norm": 0.002110618632286787,
      "learning_rate": 0.0015034293552812072,
      "loss": 0.0003,
      "step": 4550
    },
    {
      "epoch": 6.255144032921811,
      "grad_norm": 0.003799494355916977,
      "learning_rate": 0.0014979423868312757,
      "loss": 0.0002,
      "step": 4560
    },
    {
      "epoch": 6.26886145404664,
      "grad_norm": 0.004161981865763664,
      "learning_rate": 0.0014924554183813443,
      "loss": 0.0003,
      "step": 4570
    },
    {
      "epoch": 6.282578875171468,
      "grad_norm": 0.0024556093849241734,
      "learning_rate": 0.0014869684499314128,
      "loss": 0.0003,
      "step": 4580
    },
    {
      "epoch": 6.296296296296296,
      "grad_norm": 0.0020305311772972345,
      "learning_rate": 0.0014814814814814814,
      "loss": 0.0002,
      "step": 4590
    },
    {
      "epoch": 6.310013717421125,
      "grad_norm": 0.0023583632428199053,
      "learning_rate": 0.0014759945130315502,
      "loss": 0.0002,
      "step": 4600
    },
    {
      "epoch": 6.323731138545954,
      "grad_norm": 0.0014105980517342687,
      "learning_rate": 0.0014705075445816188,
      "loss": 0.0002,
      "step": 4610
    },
    {
      "epoch": 6.337448559670782,
      "grad_norm": 0.0012472491944208741,
      "learning_rate": 0.0014650205761316873,
      "loss": 0.0002,
      "step": 4620
    },
    {
      "epoch": 6.35116598079561,
      "grad_norm": 0.001362226321361959,
      "learning_rate": 0.0014595336076817559,
      "loss": 0.0002,
      "step": 4630
    },
    {
      "epoch": 6.364883401920439,
      "grad_norm": 0.001576939015649259,
      "learning_rate": 0.0014540466392318244,
      "loss": 0.0002,
      "step": 4640
    },
    {
      "epoch": 6.378600823045267,
      "grad_norm": 0.0014276904985308647,
      "learning_rate": 0.001448559670781893,
      "loss": 0.0002,
      "step": 4650
    },
    {
      "epoch": 6.392318244170096,
      "grad_norm": 0.0013750228099524975,
      "learning_rate": 0.0014430727023319616,
      "loss": 0.0002,
      "step": 4660
    },
    {
      "epoch": 6.406035665294924,
      "grad_norm": 0.0021780559327453375,
      "learning_rate": 0.0014375857338820301,
      "loss": 0.0002,
      "step": 4670
    },
    {
      "epoch": 6.419753086419753,
      "grad_norm": 0.0015442377189174294,
      "learning_rate": 0.0014320987654320987,
      "loss": 0.0002,
      "step": 4680
    },
    {
      "epoch": 6.433470507544581,
      "grad_norm": 0.0010530431754887104,
      "learning_rate": 0.0014266117969821675,
      "loss": 0.0002,
      "step": 4690
    },
    {
      "epoch": 6.44718792866941,
      "grad_norm": 0.001051423721946776,
      "learning_rate": 0.001421124828532236,
      "loss": 0.0002,
      "step": 4700
    },
    {
      "epoch": 6.460905349794238,
      "grad_norm": 0.0012689674040302634,
      "learning_rate": 0.0014156378600823046,
      "loss": 0.0002,
      "step": 4710
    },
    {
      "epoch": 6.474622770919067,
      "grad_norm": 0.001987198367714882,
      "learning_rate": 0.0014101508916323732,
      "loss": 0.0002,
      "step": 4720
    },
    {
      "epoch": 6.4883401920438954,
      "grad_norm": 0.005101791117340326,
      "learning_rate": 0.0014046639231824415,
      "loss": 0.0002,
      "step": 4730
    },
    {
      "epoch": 6.502057613168724,
      "grad_norm": 0.0026619776617735624,
      "learning_rate": 0.0013991769547325103,
      "loss": 0.0002,
      "step": 4740
    },
    {
      "epoch": 6.5157750342935525,
      "grad_norm": 0.0017184047028422356,
      "learning_rate": 0.0013936899862825789,
      "loss": 0.0004,
      "step": 4750
    },
    {
      "epoch": 6.5294924554183815,
      "grad_norm": 0.0027902096044272184,
      "learning_rate": 0.0013882030178326474,
      "loss": 0.0002,
      "step": 4760
    },
    {
      "epoch": 6.54320987654321,
      "grad_norm": 0.004209085367619991,
      "learning_rate": 0.001382716049382716,
      "loss": 0.0003,
      "step": 4770
    },
    {
      "epoch": 6.556927297668039,
      "grad_norm": 0.002260369947180152,
      "learning_rate": 0.0013772290809327848,
      "loss": 0.0002,
      "step": 4780
    },
    {
      "epoch": 6.570644718792867,
      "grad_norm": 0.002732283901423216,
      "learning_rate": 0.0013717421124828533,
      "loss": 0.0002,
      "step": 4790
    },
    {
      "epoch": 6.584362139917696,
      "grad_norm": 0.0013923950027674437,
      "learning_rate": 0.001366255144032922,
      "loss": 0.0002,
      "step": 4800
    },
    {
      "epoch": 6.598079561042524,
      "grad_norm": 0.0017239362932741642,
      "learning_rate": 0.0013607681755829905,
      "loss": 0.0002,
      "step": 4810
    },
    {
      "epoch": 6.611796982167353,
      "grad_norm": 0.0012330025201663375,
      "learning_rate": 0.0013552812071330588,
      "loss": 0.0002,
      "step": 4820
    },
    {
      "epoch": 6.625514403292181,
      "grad_norm": 0.0027491371147334576,
      "learning_rate": 0.0013497942386831276,
      "loss": 0.0002,
      "step": 4830
    },
    {
      "epoch": 6.63923182441701,
      "grad_norm": 0.0011224063346162438,
      "learning_rate": 0.0013443072702331962,
      "loss": 0.0001,
      "step": 4840
    },
    {
      "epoch": 6.652949245541838,
      "grad_norm": 0.0012178600300103426,
      "learning_rate": 0.0013388203017832647,
      "loss": 0.0001,
      "step": 4850
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.001398222055286169,
      "learning_rate": 0.0013333333333333333,
      "loss": 0.0002,
      "step": 4860
    },
    {
      "epoch": 6.680384087791495,
      "grad_norm": 0.0011267971713095903,
      "learning_rate": 0.001327846364883402,
      "loss": 0.0001,
      "step": 4870
    },
    {
      "epoch": 6.694101508916324,
      "grad_norm": 0.0011576279066503048,
      "learning_rate": 0.0013223593964334706,
      "loss": 0.0001,
      "step": 4880
    },
    {
      "epoch": 6.707818930041152,
      "grad_norm": 0.0009048433275893331,
      "learning_rate": 0.0013168724279835392,
      "loss": 0.0002,
      "step": 4890
    },
    {
      "epoch": 6.721536351165981,
      "grad_norm": 0.0012000671122223139,
      "learning_rate": 0.0013113854595336078,
      "loss": 0.0001,
      "step": 4900
    },
    {
      "epoch": 6.735253772290809,
      "grad_norm": 0.0011342039797455072,
      "learning_rate": 0.0013058984910836761,
      "loss": 0.0001,
      "step": 4910
    },
    {
      "epoch": 6.748971193415638,
      "grad_norm": 0.009522567503154278,
      "learning_rate": 0.0013004115226337449,
      "loss": 0.0002,
      "step": 4920
    },
    {
      "epoch": 6.762688614540466,
      "grad_norm": 0.001522210892289877,
      "learning_rate": 0.0012949245541838135,
      "loss": 0.0002,
      "step": 4930
    },
    {
      "epoch": 6.776406035665294,
      "grad_norm": 0.004994030110538006,
      "learning_rate": 0.001289437585733882,
      "loss": 0.0002,
      "step": 4940
    },
    {
      "epoch": 6.790123456790123,
      "grad_norm": 0.007150769699364901,
      "learning_rate": 0.0012839506172839506,
      "loss": 0.0002,
      "step": 4950
    },
    {
      "epoch": 6.803840877914952,
      "grad_norm": 0.002132778288796544,
      "learning_rate": 0.0012784636488340194,
      "loss": 0.0001,
      "step": 4960
    },
    {
      "epoch": 6.8175582990397805,
      "grad_norm": 0.0008648292277939618,
      "learning_rate": 0.001272976680384088,
      "loss": 0.0001,
      "step": 4970
    },
    {
      "epoch": 6.831275720164609,
      "grad_norm": 0.00103666668292135,
      "learning_rate": 0.0012674897119341565,
      "loss": 0.0002,
      "step": 4980
    },
    {
      "epoch": 6.844993141289438,
      "grad_norm": 0.0010714695090427995,
      "learning_rate": 0.0012620027434842248,
      "loss": 0.0001,
      "step": 4990
    },
    {
      "epoch": 6.858710562414267,
      "grad_norm": 0.0011435820488259196,
      "learning_rate": 0.0012565157750342934,
      "loss": 0.0001,
      "step": 5000
    },
    {
      "epoch": 6.872427983539095,
      "grad_norm": 0.0022607219871133566,
      "learning_rate": 0.0012510288065843622,
      "loss": 0.0001,
      "step": 5010
    },
    {
      "epoch": 6.886145404663923,
      "grad_norm": 0.000981822027824819,
      "learning_rate": 0.0012455418381344307,
      "loss": 0.0001,
      "step": 5020
    },
    {
      "epoch": 6.899862825788752,
      "grad_norm": 0.0016277807299047709,
      "learning_rate": 0.0012400548696844993,
      "loss": 0.0001,
      "step": 5030
    },
    {
      "epoch": 6.91358024691358,
      "grad_norm": 0.0019576367922127247,
      "learning_rate": 0.0012345679012345679,
      "loss": 0.0001,
      "step": 5040
    },
    {
      "epoch": 6.927297668038409,
      "grad_norm": 0.0014142628060653806,
      "learning_rate": 0.0012290809327846367,
      "loss": 0.0001,
      "step": 5050
    },
    {
      "epoch": 6.941015089163237,
      "grad_norm": 0.0009858778212219477,
      "learning_rate": 0.0012235939643347052,
      "loss": 0.0001,
      "step": 5060
    },
    {
      "epoch": 6.954732510288066,
      "grad_norm": 0.0017116281669586897,
      "learning_rate": 0.0012181069958847738,
      "loss": 0.0001,
      "step": 5070
    },
    {
      "epoch": 6.968449931412894,
      "grad_norm": 0.0016458173049613833,
      "learning_rate": 0.0012126200274348421,
      "loss": 0.0001,
      "step": 5080
    },
    {
      "epoch": 6.982167352537723,
      "grad_norm": 0.0007726759067736566,
      "learning_rate": 0.0012071330589849107,
      "loss": 0.0001,
      "step": 5090
    },
    {
      "epoch": 6.995884773662551,
      "grad_norm": 0.20348182320594788,
      "learning_rate": 0.0012016460905349795,
      "loss": 0.0002,
      "step": 5100
    },
    {
      "epoch": 7.00960219478738,
      "grad_norm": 0.004443278070539236,
      "learning_rate": 0.001196159122085048,
      "loss": 0.0002,
      "step": 5110
    },
    {
      "epoch": 7.023319615912208,
      "grad_norm": 0.0014432622119784355,
      "learning_rate": 0.0011906721536351166,
      "loss": 0.0002,
      "step": 5120
    },
    {
      "epoch": 7.037037037037037,
      "grad_norm": 0.0013319151476025581,
      "learning_rate": 0.0011851851851851852,
      "loss": 0.0002,
      "step": 5130
    },
    {
      "epoch": 7.050754458161865,
      "grad_norm": 0.003180575091391802,
      "learning_rate": 0.001179698216735254,
      "loss": 0.0001,
      "step": 5140
    },
    {
      "epoch": 7.064471879286694,
      "grad_norm": 0.0008323510410264134,
      "learning_rate": 0.0011742112482853225,
      "loss": 0.0001,
      "step": 5150
    },
    {
      "epoch": 7.078189300411522,
      "grad_norm": 0.0007861346239224076,
      "learning_rate": 0.001168724279835391,
      "loss": 0.0001,
      "step": 5160
    },
    {
      "epoch": 7.091906721536351,
      "grad_norm": 0.0011883517727255821,
      "learning_rate": 0.0011632373113854594,
      "loss": 0.0001,
      "step": 5170
    },
    {
      "epoch": 7.1056241426611795,
      "grad_norm": 0.001166895148344338,
      "learning_rate": 0.001157750342935528,
      "loss": 0.0001,
      "step": 5180
    },
    {
      "epoch": 7.1193415637860085,
      "grad_norm": 0.001431167358532548,
      "learning_rate": 0.0011522633744855968,
      "loss": 0.0001,
      "step": 5190
    },
    {
      "epoch": 7.133058984910837,
      "grad_norm": 0.000770050915889442,
      "learning_rate": 0.0011467764060356653,
      "loss": 0.0001,
      "step": 5200
    },
    {
      "epoch": 7.1467764060356656,
      "grad_norm": 0.0017978718969970942,
      "learning_rate": 0.001141289437585734,
      "loss": 0.0001,
      "step": 5210
    },
    {
      "epoch": 7.160493827160494,
      "grad_norm": 0.0008998416597023606,
      "learning_rate": 0.0011358024691358025,
      "loss": 0.0001,
      "step": 5220
    },
    {
      "epoch": 7.174211248285323,
      "grad_norm": 0.0015903357416391373,
      "learning_rate": 0.0011303155006858712,
      "loss": 0.0001,
      "step": 5230
    },
    {
      "epoch": 7.187928669410151,
      "grad_norm": 0.0012165179941803217,
      "learning_rate": 0.0011248285322359398,
      "loss": 0.0001,
      "step": 5240
    },
    {
      "epoch": 7.20164609053498,
      "grad_norm": 0.000988835352472961,
      "learning_rate": 0.0011193415637860084,
      "loss": 0.0001,
      "step": 5250
    },
    {
      "epoch": 7.215363511659808,
      "grad_norm": 0.00501368148252368,
      "learning_rate": 0.0011138545953360767,
      "loss": 0.0001,
      "step": 5260
    },
    {
      "epoch": 7.229080932784637,
      "grad_norm": 0.0020972297061234713,
      "learning_rate": 0.0011083676268861453,
      "loss": 0.0001,
      "step": 5270
    },
    {
      "epoch": 7.242798353909465,
      "grad_norm": 0.019962109625339508,
      "learning_rate": 0.001102880658436214,
      "loss": 0.0001,
      "step": 5280
    },
    {
      "epoch": 7.256515775034294,
      "grad_norm": 0.0029824902303516865,
      "learning_rate": 0.0010973936899862826,
      "loss": 0.0001,
      "step": 5290
    },
    {
      "epoch": 7.270233196159122,
      "grad_norm": 0.0007787616341374815,
      "learning_rate": 0.0010919067215363512,
      "loss": 0.0001,
      "step": 5300
    },
    {
      "epoch": 7.283950617283951,
      "grad_norm": 0.0007212703349068761,
      "learning_rate": 0.0010864197530864198,
      "loss": 0.0001,
      "step": 5310
    },
    {
      "epoch": 7.297668038408779,
      "grad_norm": 0.0015998943708837032,
      "learning_rate": 0.0010809327846364883,
      "loss": 0.0001,
      "step": 5320
    },
    {
      "epoch": 7.311385459533608,
      "grad_norm": 0.0030017767567187548,
      "learning_rate": 0.001075445816186557,
      "loss": 0.0002,
      "step": 5330
    },
    {
      "epoch": 7.325102880658436,
      "grad_norm": 0.0008598833228461444,
      "learning_rate": 0.0010699588477366254,
      "loss": 0.0001,
      "step": 5340
    },
    {
      "epoch": 7.338820301783265,
      "grad_norm": 0.001474131946451962,
      "learning_rate": 0.001064471879286694,
      "loss": 0.0001,
      "step": 5350
    },
    {
      "epoch": 7.352537722908093,
      "grad_norm": 0.0010563493706285954,
      "learning_rate": 0.0010589849108367626,
      "loss": 0.0001,
      "step": 5360
    },
    {
      "epoch": 7.366255144032921,
      "grad_norm": 0.0006795641384087503,
      "learning_rate": 0.0010534979423868314,
      "loss": 0.0001,
      "step": 5370
    },
    {
      "epoch": 7.37997256515775,
      "grad_norm": 0.0009836398530751467,
      "learning_rate": 0.0010480109739369,
      "loss": 0.0001,
      "step": 5380
    },
    {
      "epoch": 7.393689986282579,
      "grad_norm": 0.0009192709112539887,
      "learning_rate": 0.0010425240054869685,
      "loss": 0.0002,
      "step": 5390
    },
    {
      "epoch": 7.407407407407407,
      "grad_norm": 0.0011155173415318131,
      "learning_rate": 0.001037037037037037,
      "loss": 0.0001,
      "step": 5400
    },
    {
      "epoch": 7.4211248285322355,
      "grad_norm": 0.001138184918090701,
      "learning_rate": 0.0010315500685871056,
      "loss": 0.0001,
      "step": 5410
    },
    {
      "epoch": 7.4348422496570645,
      "grad_norm": 0.0010229999898001552,
      "learning_rate": 0.0010260631001371744,
      "loss": 0.0001,
      "step": 5420
    },
    {
      "epoch": 7.448559670781893,
      "grad_norm": 0.0008752515423111618,
      "learning_rate": 0.0010205761316872427,
      "loss": 0.0001,
      "step": 5430
    },
    {
      "epoch": 7.462277091906722,
      "grad_norm": 0.0011435073101893067,
      "learning_rate": 0.0010150891632373113,
      "loss": 0.0002,
      "step": 5440
    },
    {
      "epoch": 7.47599451303155,
      "grad_norm": 0.0007767900242470205,
      "learning_rate": 0.0010096021947873799,
      "loss": 0.0001,
      "step": 5450
    },
    {
      "epoch": 7.489711934156379,
      "grad_norm": 0.0017814581515267491,
      "learning_rate": 0.0010041152263374487,
      "loss": 0.0001,
      "step": 5460
    },
    {
      "epoch": 7.503429355281207,
      "grad_norm": 0.006739568430930376,
      "learning_rate": 0.0009986282578875172,
      "loss": 0.0001,
      "step": 5470
    },
    {
      "epoch": 7.517146776406036,
      "grad_norm": 0.002656546887010336,
      "learning_rate": 0.0009931412894375858,
      "loss": 0.0001,
      "step": 5480
    },
    {
      "epoch": 7.530864197530864,
      "grad_norm": 0.0010134080657735467,
      "learning_rate": 0.0009876543209876543,
      "loss": 0.0001,
      "step": 5490
    },
    {
      "epoch": 7.544581618655693,
      "grad_norm": 0.0009329757886007428,
      "learning_rate": 0.000982167352537723,
      "loss": 0.0001,
      "step": 5500
    },
    {
      "epoch": 7.558299039780521,
      "grad_norm": 0.0006250706501305103,
      "learning_rate": 0.0009766803840877915,
      "loss": 0.0001,
      "step": 5510
    },
    {
      "epoch": 7.57201646090535,
      "grad_norm": 0.0009211169090121984,
      "learning_rate": 0.0009711934156378601,
      "loss": 0.0001,
      "step": 5520
    },
    {
      "epoch": 7.585733882030178,
      "grad_norm": 0.008452112786471844,
      "learning_rate": 0.0009657064471879287,
      "loss": 0.0001,
      "step": 5530
    },
    {
      "epoch": 7.599451303155007,
      "grad_norm": 0.000817075720988214,
      "learning_rate": 0.0009602194787379973,
      "loss": 0.0001,
      "step": 5540
    },
    {
      "epoch": 7.613168724279835,
      "grad_norm": 0.0013599696103483438,
      "learning_rate": 0.0009547325102880658,
      "loss": 0.0001,
      "step": 5550
    },
    {
      "epoch": 7.626886145404664,
      "grad_norm": 0.0009048083447851241,
      "learning_rate": 0.0009492455418381345,
      "loss": 0.0001,
      "step": 5560
    },
    {
      "epoch": 7.640603566529492,
      "grad_norm": 0.0005626107449643314,
      "learning_rate": 0.0009437585733882031,
      "loss": 0.0001,
      "step": 5570
    },
    {
      "epoch": 7.654320987654321,
      "grad_norm": 0.0007461919449269772,
      "learning_rate": 0.0009382716049382715,
      "loss": 0.0001,
      "step": 5580
    },
    {
      "epoch": 7.668038408779149,
      "grad_norm": 0.0011404717806726694,
      "learning_rate": 0.0009327846364883402,
      "loss": 0.0001,
      "step": 5590
    },
    {
      "epoch": 7.681755829903978,
      "grad_norm": 0.000655999465379864,
      "learning_rate": 0.0009272976680384088,
      "loss": 0.0001,
      "step": 5600
    },
    {
      "epoch": 7.695473251028806,
      "grad_norm": 0.0013234858633950353,
      "learning_rate": 0.0009218106995884774,
      "loss": 0.0001,
      "step": 5610
    },
    {
      "epoch": 7.709190672153635,
      "grad_norm": 0.0008210537489503622,
      "learning_rate": 0.0009163237311385459,
      "loss": 0.0001,
      "step": 5620
    },
    {
      "epoch": 7.7229080932784635,
      "grad_norm": 0.0007623761775903404,
      "learning_rate": 0.0009108367626886146,
      "loss": 0.0001,
      "step": 5630
    },
    {
      "epoch": 7.7366255144032925,
      "grad_norm": 0.0018983859336003661,
      "learning_rate": 0.0009053497942386831,
      "loss": 0.0001,
      "step": 5640
    },
    {
      "epoch": 7.750342935528121,
      "grad_norm": 0.0006055665435269475,
      "learning_rate": 0.0008998628257887518,
      "loss": 0.0001,
      "step": 5650
    },
    {
      "epoch": 7.76406035665295,
      "grad_norm": 0.0008429254521615803,
      "learning_rate": 0.0008943758573388204,
      "loss": 0.0001,
      "step": 5660
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 0.000925852102227509,
      "learning_rate": 0.0008888888888888888,
      "loss": 0.0001,
      "step": 5670
    },
    {
      "epoch": 7.791495198902607,
      "grad_norm": 0.0012944808695465326,
      "learning_rate": 0.0008834019204389575,
      "loss": 0.0001,
      "step": 5680
    },
    {
      "epoch": 7.805212620027435,
      "grad_norm": 0.000503985327668488,
      "learning_rate": 0.0008779149519890261,
      "loss": 0.0001,
      "step": 5690
    },
    {
      "epoch": 7.818930041152264,
      "grad_norm": 0.0007546455599367619,
      "learning_rate": 0.0008724279835390947,
      "loss": 0.0001,
      "step": 5700
    },
    {
      "epoch": 7.832647462277092,
      "grad_norm": 0.000843508169054985,
      "learning_rate": 0.0008669410150891632,
      "loss": 0.0001,
      "step": 5710
    },
    {
      "epoch": 7.84636488340192,
      "grad_norm": 0.0005162303568795323,
      "learning_rate": 0.0008614540466392319,
      "loss": 0.0001,
      "step": 5720
    },
    {
      "epoch": 7.860082304526749,
      "grad_norm": 0.000701890850905329,
      "learning_rate": 0.0008559670781893004,
      "loss": 0.0001,
      "step": 5730
    },
    {
      "epoch": 7.873799725651578,
      "grad_norm": 0.0006456763367168605,
      "learning_rate": 0.0008504801097393691,
      "loss": 0.0001,
      "step": 5740
    },
    {
      "epoch": 7.887517146776406,
      "grad_norm": 0.0007007036474533379,
      "learning_rate": 0.0008449931412894376,
      "loss": 0.0001,
      "step": 5750
    },
    {
      "epoch": 7.901234567901234,
      "grad_norm": 0.0010874387808144093,
      "learning_rate": 0.0008395061728395061,
      "loss": 0.0001,
      "step": 5760
    },
    {
      "epoch": 7.914951989026063,
      "grad_norm": 0.0009092024411074817,
      "learning_rate": 0.0008340192043895748,
      "loss": 0.0001,
      "step": 5770
    },
    {
      "epoch": 7.928669410150892,
      "grad_norm": 0.0008406631532125175,
      "learning_rate": 0.0008285322359396434,
      "loss": 0.0001,
      "step": 5780
    },
    {
      "epoch": 7.94238683127572,
      "grad_norm": 0.0006693900213576853,
      "learning_rate": 0.000823045267489712,
      "loss": 0.0001,
      "step": 5790
    },
    {
      "epoch": 7.956104252400548,
      "grad_norm": 0.0005025365971960127,
      "learning_rate": 0.0008175582990397805,
      "loss": 0.0001,
      "step": 5800
    },
    {
      "epoch": 7.969821673525377,
      "grad_norm": 0.0005484382854774594,
      "learning_rate": 0.0008120713305898492,
      "loss": 0.0001,
      "step": 5810
    },
    {
      "epoch": 7.983539094650205,
      "grad_norm": 0.0015073124086484313,
      "learning_rate": 0.0008065843621399177,
      "loss": 0.0002,
      "step": 5820
    },
    {
      "epoch": 7.997256515775034,
      "grad_norm": 0.002962616039440036,
      "learning_rate": 0.0008010973936899864,
      "loss": 0.0002,
      "step": 5830
    },
    {
      "epoch": 8.010973936899862,
      "grad_norm": 0.0023284531198441982,
      "learning_rate": 0.0007956104252400548,
      "loss": 0.0001,
      "step": 5840
    },
    {
      "epoch": 8.024691358024691,
      "grad_norm": 0.0011098247487097979,
      "learning_rate": 0.0007901234567901234,
      "loss": 0.0001,
      "step": 5850
    },
    {
      "epoch": 8.03840877914952,
      "grad_norm": 0.00544121814891696,
      "learning_rate": 0.0007846364883401921,
      "loss": 0.0001,
      "step": 5860
    },
    {
      "epoch": 8.052126200274348,
      "grad_norm": 0.0009722936665639281,
      "learning_rate": 0.0007791495198902606,
      "loss": 0.0001,
      "step": 5870
    },
    {
      "epoch": 8.065843621399177,
      "grad_norm": 0.0009830874623730779,
      "learning_rate": 0.0007736625514403293,
      "loss": 0.0001,
      "step": 5880
    },
    {
      "epoch": 8.079561042524006,
      "grad_norm": 0.001845730934292078,
      "learning_rate": 0.0007681755829903978,
      "loss": 0.0001,
      "step": 5890
    },
    {
      "epoch": 8.093278463648835,
      "grad_norm": 0.0015871338546276093,
      "learning_rate": 0.0007626886145404664,
      "loss": 0.0001,
      "step": 5900
    },
    {
      "epoch": 8.106995884773662,
      "grad_norm": 0.0010950713185593486,
      "learning_rate": 0.000757201646090535,
      "loss": 0.0001,
      "step": 5910
    },
    {
      "epoch": 8.12071330589849,
      "grad_norm": 0.0010853284038603306,
      "learning_rate": 0.0007517146776406036,
      "loss": 0.0001,
      "step": 5920
    },
    {
      "epoch": 8.13443072702332,
      "grad_norm": 0.0009371800697408617,
      "learning_rate": 0.0007462277091906721,
      "loss": 0.0001,
      "step": 5930
    },
    {
      "epoch": 8.148148148148149,
      "grad_norm": 0.0008867141441442072,
      "learning_rate": 0.0007407407407407407,
      "loss": 0.0001,
      "step": 5940
    },
    {
      "epoch": 8.161865569272976,
      "grad_norm": 0.0010936399921774864,
      "learning_rate": 0.0007352537722908094,
      "loss": 0.0001,
      "step": 5950
    },
    {
      "epoch": 8.175582990397805,
      "grad_norm": 0.004007953684777021,
      "learning_rate": 0.0007297668038408779,
      "loss": 0.0001,
      "step": 5960
    },
    {
      "epoch": 8.189300411522634,
      "grad_norm": 0.0010238708928227425,
      "learning_rate": 0.0007242798353909465,
      "loss": 0.0001,
      "step": 5970
    },
    {
      "epoch": 8.203017832647463,
      "grad_norm": 0.0015678965719416738,
      "learning_rate": 0.0007187928669410151,
      "loss": 0.0001,
      "step": 5980
    },
    {
      "epoch": 8.21673525377229,
      "grad_norm": 0.0013050534762442112,
      "learning_rate": 0.0007133058984910837,
      "loss": 0.0001,
      "step": 5990
    },
    {
      "epoch": 8.23045267489712,
      "grad_norm": 0.0007108630961738527,
      "learning_rate": 0.0007078189300411523,
      "loss": 0.0001,
      "step": 6000
    },
    {
      "epoch": 8.244170096021948,
      "grad_norm": 0.0007438689353875816,
      "learning_rate": 0.0007023319615912208,
      "loss": 0.0001,
      "step": 6010
    },
    {
      "epoch": 8.257887517146777,
      "grad_norm": 0.0010245811427012086,
      "learning_rate": 0.0006968449931412894,
      "loss": 0.0001,
      "step": 6020
    },
    {
      "epoch": 8.271604938271604,
      "grad_norm": 0.033072181046009064,
      "learning_rate": 0.000691358024691358,
      "loss": 0.0001,
      "step": 6030
    },
    {
      "epoch": 8.285322359396433,
      "grad_norm": 0.014734958298504353,
      "learning_rate": 0.0006858710562414267,
      "loss": 0.0001,
      "step": 6040
    },
    {
      "epoch": 8.299039780521262,
      "grad_norm": 0.0004661167913582176,
      "learning_rate": 0.0006803840877914952,
      "loss": 0.0001,
      "step": 6050
    },
    {
      "epoch": 8.312757201646091,
      "grad_norm": 0.0006087158690206707,
      "learning_rate": 0.0006748971193415638,
      "loss": 0.0001,
      "step": 6060
    },
    {
      "epoch": 8.326474622770919,
      "grad_norm": 0.0006699092919006944,
      "learning_rate": 0.0006694101508916324,
      "loss": 0.0001,
      "step": 6070
    },
    {
      "epoch": 8.340192043895748,
      "grad_norm": 0.0009509912924841046,
      "learning_rate": 0.000663923182441701,
      "loss": 0.0001,
      "step": 6080
    },
    {
      "epoch": 8.353909465020577,
      "grad_norm": 0.002273176098242402,
      "learning_rate": 0.0006584362139917696,
      "loss": 0.0001,
      "step": 6090
    },
    {
      "epoch": 8.367626886145406,
      "grad_norm": 0.0007687041652388871,
      "learning_rate": 0.0006529492455418381,
      "loss": 0.0001,
      "step": 6100
    },
    {
      "epoch": 8.381344307270233,
      "grad_norm": 0.001074399333447218,
      "learning_rate": 0.0006474622770919067,
      "loss": 0.0001,
      "step": 6110
    },
    {
      "epoch": 8.395061728395062,
      "grad_norm": 0.0018028360791504383,
      "learning_rate": 0.0006419753086419753,
      "loss": 0.0001,
      "step": 6120
    },
    {
      "epoch": 8.40877914951989,
      "grad_norm": 0.0007617619703523815,
      "learning_rate": 0.000636488340192044,
      "loss": 0.0001,
      "step": 6130
    },
    {
      "epoch": 8.422496570644718,
      "grad_norm": 0.0007147123105823994,
      "learning_rate": 0.0006310013717421124,
      "loss": 0.0001,
      "step": 6140
    },
    {
      "epoch": 8.436213991769547,
      "grad_norm": 0.002091852715238929,
      "learning_rate": 0.0006255144032921811,
      "loss": 0.0001,
      "step": 6150
    },
    {
      "epoch": 8.449931412894376,
      "grad_norm": 0.0012909079669043422,
      "learning_rate": 0.0006200274348422497,
      "loss": 0.0001,
      "step": 6160
    },
    {
      "epoch": 8.463648834019205,
      "grad_norm": 0.0006613322184421122,
      "learning_rate": 0.0006145404663923183,
      "loss": 0.0001,
      "step": 6170
    },
    {
      "epoch": 8.477366255144032,
      "grad_norm": 0.001970085082575679,
      "learning_rate": 0.0006090534979423869,
      "loss": 0.0001,
      "step": 6180
    },
    {
      "epoch": 8.491083676268861,
      "grad_norm": 0.003964259289205074,
      "learning_rate": 0.0006035665294924553,
      "loss": 0.0001,
      "step": 6190
    },
    {
      "epoch": 8.50480109739369,
      "grad_norm": 0.0007844837382435799,
      "learning_rate": 0.000598079561042524,
      "loss": 0.0002,
      "step": 6200
    },
    {
      "epoch": 8.518518518518519,
      "grad_norm": 0.0007090562721714377,
      "learning_rate": 0.0005925925925925926,
      "loss": 0.0001,
      "step": 6210
    },
    {
      "epoch": 8.532235939643346,
      "grad_norm": 0.0008174914983101189,
      "learning_rate": 0.0005871056241426613,
      "loss": 0.0001,
      "step": 6220
    },
    {
      "epoch": 8.545953360768175,
      "grad_norm": 0.0017642517341300845,
      "learning_rate": 0.0005816186556927297,
      "loss": 0.0001,
      "step": 6230
    },
    {
      "epoch": 8.559670781893004,
      "grad_norm": 0.0007029061089269817,
      "learning_rate": 0.0005761316872427984,
      "loss": 0.0001,
      "step": 6240
    },
    {
      "epoch": 8.573388203017833,
      "grad_norm": 0.0008418067591264844,
      "learning_rate": 0.000570644718792867,
      "loss": 0.0001,
      "step": 6250
    },
    {
      "epoch": 8.58710562414266,
      "grad_norm": 0.0008700393955223262,
      "learning_rate": 0.0005651577503429356,
      "loss": 0.0001,
      "step": 6260
    },
    {
      "epoch": 8.60082304526749,
      "grad_norm": 0.0007726331241428852,
      "learning_rate": 0.0005596707818930042,
      "loss": 0.0001,
      "step": 6270
    },
    {
      "epoch": 8.614540466392318,
      "grad_norm": 0.001204830827191472,
      "learning_rate": 0.0005541838134430726,
      "loss": 0.0001,
      "step": 6280
    },
    {
      "epoch": 8.628257887517147,
      "grad_norm": 0.0015369545435532928,
      "learning_rate": 0.0005486968449931413,
      "loss": 0.0001,
      "step": 6290
    },
    {
      "epoch": 8.641975308641975,
      "grad_norm": 0.0006673407624475658,
      "learning_rate": 0.0005432098765432099,
      "loss": 0.0001,
      "step": 6300
    },
    {
      "epoch": 8.655692729766804,
      "grad_norm": 0.0013933393638581038,
      "learning_rate": 0.0005377229080932786,
      "loss": 0.0001,
      "step": 6310
    },
    {
      "epoch": 8.669410150891633,
      "grad_norm": 0.000864601053763181,
      "learning_rate": 0.000532235939643347,
      "loss": 0.0001,
      "step": 6320
    },
    {
      "epoch": 8.683127572016462,
      "grad_norm": 0.0008148555643856525,
      "learning_rate": 0.0005267489711934157,
      "loss": 0.0001,
      "step": 6330
    },
    {
      "epoch": 8.696844993141289,
      "grad_norm": 0.0008124402957037091,
      "learning_rate": 0.0005212620027434842,
      "loss": 0.0001,
      "step": 6340
    },
    {
      "epoch": 8.710562414266118,
      "grad_norm": 0.0017215642146766186,
      "learning_rate": 0.0005157750342935528,
      "loss": 0.0001,
      "step": 6350
    },
    {
      "epoch": 8.724279835390947,
      "grad_norm": 0.0009968426311388612,
      "learning_rate": 0.0005102880658436214,
      "loss": 0.0001,
      "step": 6360
    },
    {
      "epoch": 8.737997256515776,
      "grad_norm": 0.0007238849648274481,
      "learning_rate": 0.0005048010973936899,
      "loss": 0.0001,
      "step": 6370
    },
    {
      "epoch": 8.751714677640603,
      "grad_norm": 0.000958321092184633,
      "learning_rate": 0.0004993141289437586,
      "loss": 0.0001,
      "step": 6380
    },
    {
      "epoch": 8.765432098765432,
      "grad_norm": 0.000511332240421325,
      "learning_rate": 0.0004938271604938272,
      "loss": 0.0001,
      "step": 6390
    },
    {
      "epoch": 8.779149519890261,
      "grad_norm": 0.0010682856664061546,
      "learning_rate": 0.0004883401920438957,
      "loss": 0.0002,
      "step": 6400
    },
    {
      "epoch": 8.79286694101509,
      "grad_norm": 0.0016582588432356715,
      "learning_rate": 0.00048285322359396435,
      "loss": 0.0001,
      "step": 6410
    },
    {
      "epoch": 8.806584362139917,
      "grad_norm": 0.0007862420752644539,
      "learning_rate": 0.0004773662551440329,
      "loss": 0.0001,
      "step": 6420
    },
    {
      "epoch": 8.820301783264746,
      "grad_norm": 0.0008603698806837201,
      "learning_rate": 0.00047187928669410154,
      "loss": 0.0001,
      "step": 6430
    },
    {
      "epoch": 8.834019204389575,
      "grad_norm": 0.0012669602874666452,
      "learning_rate": 0.0004663923182441701,
      "loss": 0.0001,
      "step": 6440
    },
    {
      "epoch": 8.847736625514404,
      "grad_norm": 0.0006039637955836952,
      "learning_rate": 0.0004609053497942387,
      "loss": 0.0001,
      "step": 6450
    },
    {
      "epoch": 8.861454046639231,
      "grad_norm": 0.0006199124036356807,
      "learning_rate": 0.0004554183813443073,
      "loss": 0.0001,
      "step": 6460
    },
    {
      "epoch": 8.87517146776406,
      "grad_norm": 0.0009608466643840075,
      "learning_rate": 0.0004499314128943759,
      "loss": 0.0001,
      "step": 6470
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 0.0008682804182171822,
      "learning_rate": 0.0004444444444444444,
      "loss": 0.0001,
      "step": 6480
    },
    {
      "epoch": 8.902606310013716,
      "grad_norm": 0.0006572150159627199,
      "learning_rate": 0.00043895747599451303,
      "loss": 0.0001,
      "step": 6490
    },
    {
      "epoch": 8.916323731138545,
      "grad_norm": 0.0005949171027168632,
      "learning_rate": 0.0004334705075445816,
      "loss": 0.0001,
      "step": 6500
    },
    {
      "epoch": 8.930041152263374,
      "grad_norm": 0.0005383729585446417,
      "learning_rate": 0.0004279835390946502,
      "loss": 0.0001,
      "step": 6510
    },
    {
      "epoch": 8.943758573388203,
      "grad_norm": 0.000829147407785058,
      "learning_rate": 0.0004224965706447188,
      "loss": 0.0001,
      "step": 6520
    },
    {
      "epoch": 8.957475994513032,
      "grad_norm": 0.0005588457570411265,
      "learning_rate": 0.0004170096021947874,
      "loss": 0.0001,
      "step": 6530
    },
    {
      "epoch": 8.97119341563786,
      "grad_norm": 0.0013594834599643946,
      "learning_rate": 0.000411522633744856,
      "loss": 0.0001,
      "step": 6540
    },
    {
      "epoch": 8.984910836762689,
      "grad_norm": 0.0007485917303711176,
      "learning_rate": 0.0004060356652949246,
      "loss": 0.0001,
      "step": 6550
    },
    {
      "epoch": 8.998628257887518,
      "grad_norm": 0.00044602566049434245,
      "learning_rate": 0.0004005486968449932,
      "loss": 0.0001,
      "step": 6560
    }
  ],
  "logging_steps": 10,
  "max_steps": 7290,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
