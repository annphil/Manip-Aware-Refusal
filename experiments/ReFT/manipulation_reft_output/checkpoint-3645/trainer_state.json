{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 3645,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013717421124828532,
      "grad_norm": 7.102995872497559,
      "learning_rate": 0.003994513031550069,
      "loss": 7.909,
      "step": 10
    },
    {
      "epoch": 0.027434842249657063,
      "grad_norm": 5.449845790863037,
      "learning_rate": 0.003989026063100137,
      "loss": 4.2655,
      "step": 20
    },
    {
      "epoch": 0.0411522633744856,
      "grad_norm": 9.055180549621582,
      "learning_rate": 0.0039835390946502056,
      "loss": 3.5615,
      "step": 30
    },
    {
      "epoch": 0.05486968449931413,
      "grad_norm": 9.314657211303711,
      "learning_rate": 0.003978052126200274,
      "loss": 3.0856,
      "step": 40
    },
    {
      "epoch": 0.06858710562414266,
      "grad_norm": 19.166606903076172,
      "learning_rate": 0.003972565157750343,
      "loss": 2.6442,
      "step": 50
    },
    {
      "epoch": 0.0823045267489712,
      "grad_norm": 13.384329795837402,
      "learning_rate": 0.003967078189300412,
      "loss": 2.2823,
      "step": 60
    },
    {
      "epoch": 0.09602194787379972,
      "grad_norm": 16.77313995361328,
      "learning_rate": 0.00396159122085048,
      "loss": 1.9177,
      "step": 70
    },
    {
      "epoch": 0.10973936899862825,
      "grad_norm": 12.387334823608398,
      "learning_rate": 0.003956104252400549,
      "loss": 1.6896,
      "step": 80
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 14.599374771118164,
      "learning_rate": 0.003950617283950617,
      "loss": 1.6219,
      "step": 90
    },
    {
      "epoch": 0.13717421124828533,
      "grad_norm": 12.941634178161621,
      "learning_rate": 0.003945130315500686,
      "loss": 1.4573,
      "step": 100
    },
    {
      "epoch": 0.15089163237311384,
      "grad_norm": 14.008685111999512,
      "learning_rate": 0.003939643347050755,
      "loss": 1.1522,
      "step": 110
    },
    {
      "epoch": 0.1646090534979424,
      "grad_norm": 16.199888229370117,
      "learning_rate": 0.003934156378600823,
      "loss": 1.0397,
      "step": 120
    },
    {
      "epoch": 0.17832647462277093,
      "grad_norm": 18.411029815673828,
      "learning_rate": 0.003928669410150892,
      "loss": 1.1183,
      "step": 130
    },
    {
      "epoch": 0.19204389574759945,
      "grad_norm": 21.276185989379883,
      "learning_rate": 0.00392318244170096,
      "loss": 0.8733,
      "step": 140
    },
    {
      "epoch": 0.205761316872428,
      "grad_norm": 16.384662628173828,
      "learning_rate": 0.003917695473251029,
      "loss": 0.8043,
      "step": 150
    },
    {
      "epoch": 0.2194787379972565,
      "grad_norm": 17.66779327392578,
      "learning_rate": 0.003912208504801098,
      "loss": 0.6612,
      "step": 160
    },
    {
      "epoch": 0.23319615912208505,
      "grad_norm": 12.729955673217773,
      "learning_rate": 0.003906721536351166,
      "loss": 0.6627,
      "step": 170
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 10.72763442993164,
      "learning_rate": 0.0039012345679012347,
      "loss": 0.6115,
      "step": 180
    },
    {
      "epoch": 0.2606310013717421,
      "grad_norm": 19.470672607421875,
      "learning_rate": 0.0038957475994513035,
      "loss": 0.6592,
      "step": 190
    },
    {
      "epoch": 0.27434842249657065,
      "grad_norm": 9.79128360748291,
      "learning_rate": 0.003890260631001372,
      "loss": 0.4412,
      "step": 200
    },
    {
      "epoch": 0.2880658436213992,
      "grad_norm": 17.319538116455078,
      "learning_rate": 0.0038847736625514406,
      "loss": 0.4442,
      "step": 210
    },
    {
      "epoch": 0.3017832647462277,
      "grad_norm": 20.70102310180664,
      "learning_rate": 0.003879286694101509,
      "loss": 0.4374,
      "step": 220
    },
    {
      "epoch": 0.31550068587105623,
      "grad_norm": 11.593032836914062,
      "learning_rate": 0.0038737997256515777,
      "loss": 0.3548,
      "step": 230
    },
    {
      "epoch": 0.3292181069958848,
      "grad_norm": 10.380108833312988,
      "learning_rate": 0.0038683127572016465,
      "loss": 0.3424,
      "step": 240
    },
    {
      "epoch": 0.3429355281207133,
      "grad_norm": 9.524140357971191,
      "learning_rate": 0.003862825788751715,
      "loss": 0.3228,
      "step": 250
    },
    {
      "epoch": 0.35665294924554186,
      "grad_norm": 9.343277931213379,
      "learning_rate": 0.0038573388203017836,
      "loss": 0.2435,
      "step": 260
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 13.622091293334961,
      "learning_rate": 0.0038518518518518515,
      "loss": 0.2275,
      "step": 270
    },
    {
      "epoch": 0.3840877914951989,
      "grad_norm": 14.499710083007812,
      "learning_rate": 0.0038463648834019203,
      "loss": 0.3775,
      "step": 280
    },
    {
      "epoch": 0.39780521262002744,
      "grad_norm": 16.33543586730957,
      "learning_rate": 0.003840877914951989,
      "loss": 0.263,
      "step": 290
    },
    {
      "epoch": 0.411522633744856,
      "grad_norm": 10.830759048461914,
      "learning_rate": 0.0038353909465020574,
      "loss": 0.1856,
      "step": 300
    },
    {
      "epoch": 0.4252400548696845,
      "grad_norm": 6.924062728881836,
      "learning_rate": 0.0038299039780521262,
      "loss": 0.152,
      "step": 310
    },
    {
      "epoch": 0.438957475994513,
      "grad_norm": 6.442953109741211,
      "learning_rate": 0.003824417009602195,
      "loss": 0.1087,
      "step": 320
    },
    {
      "epoch": 0.45267489711934156,
      "grad_norm": 8.133803367614746,
      "learning_rate": 0.0038189300411522633,
      "loss": 0.1757,
      "step": 330
    },
    {
      "epoch": 0.4663923182441701,
      "grad_norm": 13.79932689666748,
      "learning_rate": 0.003813443072702332,
      "loss": 0.1208,
      "step": 340
    },
    {
      "epoch": 0.48010973936899864,
      "grad_norm": 10.184813499450684,
      "learning_rate": 0.0038079561042524005,
      "loss": 0.1287,
      "step": 350
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 3.2577786445617676,
      "learning_rate": 0.0038024691358024693,
      "loss": 0.1784,
      "step": 360
    },
    {
      "epoch": 0.5075445816186557,
      "grad_norm": 4.324190616607666,
      "learning_rate": 0.003796982167352538,
      "loss": 0.112,
      "step": 370
    },
    {
      "epoch": 0.5212620027434842,
      "grad_norm": 3.94382905960083,
      "learning_rate": 0.0037914951989026064,
      "loss": 0.1031,
      "step": 380
    },
    {
      "epoch": 0.5349794238683128,
      "grad_norm": 4.351624011993408,
      "learning_rate": 0.003786008230452675,
      "loss": 0.0946,
      "step": 390
    },
    {
      "epoch": 0.5486968449931413,
      "grad_norm": 0.9790947437286377,
      "learning_rate": 0.0037805212620027435,
      "loss": 0.1055,
      "step": 400
    },
    {
      "epoch": 0.5624142661179699,
      "grad_norm": 11.866278648376465,
      "learning_rate": 0.0037750342935528123,
      "loss": 0.0813,
      "step": 410
    },
    {
      "epoch": 0.5761316872427984,
      "grad_norm": 4.954287052154541,
      "learning_rate": 0.003769547325102881,
      "loss": 0.1122,
      "step": 420
    },
    {
      "epoch": 0.5898491083676269,
      "grad_norm": 3.6281094551086426,
      "learning_rate": 0.0037640603566529494,
      "loss": 0.0781,
      "step": 430
    },
    {
      "epoch": 0.6035665294924554,
      "grad_norm": 4.920687675476074,
      "learning_rate": 0.0037585733882030178,
      "loss": 0.1171,
      "step": 440
    },
    {
      "epoch": 0.6172839506172839,
      "grad_norm": 7.36521577835083,
      "learning_rate": 0.003753086419753086,
      "loss": 0.1346,
      "step": 450
    },
    {
      "epoch": 0.6310013717421125,
      "grad_norm": 3.5788347721099854,
      "learning_rate": 0.003747599451303155,
      "loss": 0.088,
      "step": 460
    },
    {
      "epoch": 0.644718792866941,
      "grad_norm": 3.4002041816711426,
      "learning_rate": 0.0037421124828532237,
      "loss": 0.0826,
      "step": 470
    },
    {
      "epoch": 0.6584362139917695,
      "grad_norm": 2.4556453227996826,
      "learning_rate": 0.003736625514403292,
      "loss": 0.0534,
      "step": 480
    },
    {
      "epoch": 0.6721536351165981,
      "grad_norm": 2.1229560375213623,
      "learning_rate": 0.003731138545953361,
      "loss": 0.0935,
      "step": 490
    },
    {
      "epoch": 0.6858710562414266,
      "grad_norm": 1.7936378717422485,
      "learning_rate": 0.0037256515775034296,
      "loss": 0.074,
      "step": 500
    },
    {
      "epoch": 0.6995884773662552,
      "grad_norm": 4.88102388381958,
      "learning_rate": 0.003720164609053498,
      "loss": 0.0567,
      "step": 510
    },
    {
      "epoch": 0.7133058984910837,
      "grad_norm": 2.9650697708129883,
      "learning_rate": 0.0037146776406035667,
      "loss": 0.042,
      "step": 520
    },
    {
      "epoch": 0.7270233196159122,
      "grad_norm": 3.334686040878296,
      "learning_rate": 0.003709190672153635,
      "loss": 0.085,
      "step": 530
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 10.097945213317871,
      "learning_rate": 0.003703703703703704,
      "loss": 0.0608,
      "step": 540
    },
    {
      "epoch": 0.7544581618655692,
      "grad_norm": 5.526248931884766,
      "learning_rate": 0.0036982167352537726,
      "loss": 0.0475,
      "step": 550
    },
    {
      "epoch": 0.7681755829903978,
      "grad_norm": 2.3787221908569336,
      "learning_rate": 0.003692729766803841,
      "loss": 0.0489,
      "step": 560
    },
    {
      "epoch": 0.7818930041152263,
      "grad_norm": 9.216018676757812,
      "learning_rate": 0.0036872427983539098,
      "loss": 0.2081,
      "step": 570
    },
    {
      "epoch": 0.7956104252400549,
      "grad_norm": 3.5417046546936035,
      "learning_rate": 0.003681755829903978,
      "loss": 0.0483,
      "step": 580
    },
    {
      "epoch": 0.8093278463648834,
      "grad_norm": 2.9877257347106934,
      "learning_rate": 0.003676268861454047,
      "loss": 0.0465,
      "step": 590
    },
    {
      "epoch": 0.823045267489712,
      "grad_norm": 2.1434154510498047,
      "learning_rate": 0.0036707818930041157,
      "loss": 0.0446,
      "step": 600
    },
    {
      "epoch": 0.8367626886145405,
      "grad_norm": 0.45098498463630676,
      "learning_rate": 0.0036652949245541836,
      "loss": 0.0276,
      "step": 610
    },
    {
      "epoch": 0.850480109739369,
      "grad_norm": 1.5228772163391113,
      "learning_rate": 0.0036598079561042524,
      "loss": 0.0393,
      "step": 620
    },
    {
      "epoch": 0.8641975308641975,
      "grad_norm": 3.817784070968628,
      "learning_rate": 0.0036543209876543207,
      "loss": 0.0506,
      "step": 630
    },
    {
      "epoch": 0.877914951989026,
      "grad_norm": 1.6846778392791748,
      "learning_rate": 0.0036488340192043895,
      "loss": 0.0483,
      "step": 640
    },
    {
      "epoch": 0.8916323731138546,
      "grad_norm": 0.6752075552940369,
      "learning_rate": 0.0036433470507544583,
      "loss": 0.0302,
      "step": 650
    },
    {
      "epoch": 0.9053497942386831,
      "grad_norm": 0.5913817882537842,
      "learning_rate": 0.0036378600823045266,
      "loss": 0.0677,
      "step": 660
    },
    {
      "epoch": 0.9190672153635117,
      "grad_norm": 0.4712817370891571,
      "learning_rate": 0.0036323731138545954,
      "loss": 0.0311,
      "step": 670
    },
    {
      "epoch": 0.9327846364883402,
      "grad_norm": 0.5882495641708374,
      "learning_rate": 0.003626886145404664,
      "loss": 0.0364,
      "step": 680
    },
    {
      "epoch": 0.9465020576131687,
      "grad_norm": 7.433635234832764,
      "learning_rate": 0.0036213991769547325,
      "loss": 0.0494,
      "step": 690
    },
    {
      "epoch": 0.9602194787379973,
      "grad_norm": 3.19130802154541,
      "learning_rate": 0.0036159122085048013,
      "loss": 0.0379,
      "step": 700
    },
    {
      "epoch": 0.9739368998628258,
      "grad_norm": 1.097072720527649,
      "learning_rate": 0.0036104252400548697,
      "loss": 0.0248,
      "step": 710
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 3.953758955001831,
      "learning_rate": 0.0036049382716049384,
      "loss": 0.0362,
      "step": 720
    },
    {
      "epoch": 1.0013717421124828,
      "grad_norm": 1.9627350568771362,
      "learning_rate": 0.003599451303155007,
      "loss": 0.0169,
      "step": 730
    },
    {
      "epoch": 1.0150891632373114,
      "grad_norm": 7.423328876495361,
      "learning_rate": 0.0035939643347050756,
      "loss": 0.0419,
      "step": 740
    },
    {
      "epoch": 1.02880658436214,
      "grad_norm": 1.5784693956375122,
      "learning_rate": 0.0035884773662551443,
      "loss": 0.0295,
      "step": 750
    },
    {
      "epoch": 1.0425240054869684,
      "grad_norm": 1.6370735168457031,
      "learning_rate": 0.0035829903978052127,
      "loss": 0.0295,
      "step": 760
    },
    {
      "epoch": 1.056241426611797,
      "grad_norm": 1.974791407585144,
      "learning_rate": 0.0035775034293552815,
      "loss": 0.0357,
      "step": 770
    },
    {
      "epoch": 1.0699588477366255,
      "grad_norm": 3.13201904296875,
      "learning_rate": 0.0035720164609053503,
      "loss": 0.0297,
      "step": 780
    },
    {
      "epoch": 1.083676268861454,
      "grad_norm": 2.4929585456848145,
      "learning_rate": 0.003566529492455418,
      "loss": 0.0502,
      "step": 790
    },
    {
      "epoch": 1.0973936899862826,
      "grad_norm": 2.195676803588867,
      "learning_rate": 0.003561042524005487,
      "loss": 0.0333,
      "step": 800
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 3.32153582572937,
      "learning_rate": 0.0035555555555555553,
      "loss": 0.0288,
      "step": 810
    },
    {
      "epoch": 1.1248285322359397,
      "grad_norm": 0.33802416920661926,
      "learning_rate": 0.003550068587105624,
      "loss": 0.0299,
      "step": 820
    },
    {
      "epoch": 1.1385459533607682,
      "grad_norm": 3.0308303833007812,
      "learning_rate": 0.003544581618655693,
      "loss": 0.0136,
      "step": 830
    },
    {
      "epoch": 1.1522633744855968,
      "grad_norm": 0.5001335144042969,
      "learning_rate": 0.003539094650205761,
      "loss": 0.0166,
      "step": 840
    },
    {
      "epoch": 1.1659807956104253,
      "grad_norm": 1.3040770292282104,
      "learning_rate": 0.00353360768175583,
      "loss": 0.0221,
      "step": 850
    },
    {
      "epoch": 1.1796982167352539,
      "grad_norm": 1.849737286567688,
      "learning_rate": 0.0035281207133058983,
      "loss": 0.0176,
      "step": 860
    },
    {
      "epoch": 1.1934156378600824,
      "grad_norm": 3.2244155406951904,
      "learning_rate": 0.003522633744855967,
      "loss": 0.0261,
      "step": 870
    },
    {
      "epoch": 1.2071330589849107,
      "grad_norm": 5.836394786834717,
      "learning_rate": 0.003517146776406036,
      "loss": 0.0257,
      "step": 880
    },
    {
      "epoch": 1.2208504801097393,
      "grad_norm": 1.5565156936645508,
      "learning_rate": 0.0035116598079561042,
      "loss": 0.0308,
      "step": 890
    },
    {
      "epoch": 1.2345679012345678,
      "grad_norm": 0.5373246669769287,
      "learning_rate": 0.003506172839506173,
      "loss": 0.0073,
      "step": 900
    },
    {
      "epoch": 1.2482853223593964,
      "grad_norm": 0.11659617722034454,
      "learning_rate": 0.003500685871056242,
      "loss": 0.0035,
      "step": 910
    },
    {
      "epoch": 1.262002743484225,
      "grad_norm": 0.24629347026348114,
      "learning_rate": 0.00349519890260631,
      "loss": 0.0041,
      "step": 920
    },
    {
      "epoch": 1.2757201646090535,
      "grad_norm": 0.1418679803609848,
      "learning_rate": 0.003489711934156379,
      "loss": 0.003,
      "step": 930
    },
    {
      "epoch": 1.289437585733882,
      "grad_norm": 2.758559465408325,
      "learning_rate": 0.0034842249657064473,
      "loss": 0.0081,
      "step": 940
    },
    {
      "epoch": 1.3031550068587106,
      "grad_norm": 4.704490661621094,
      "learning_rate": 0.003478737997256516,
      "loss": 0.0327,
      "step": 950
    },
    {
      "epoch": 1.316872427983539,
      "grad_norm": 1.6501725912094116,
      "learning_rate": 0.0034732510288065844,
      "loss": 0.0142,
      "step": 960
    },
    {
      "epoch": 1.3305898491083676,
      "grad_norm": 3.161900043487549,
      "learning_rate": 0.0034677640603566528,
      "loss": 0.0149,
      "step": 970
    },
    {
      "epoch": 1.3443072702331962,
      "grad_norm": 2.3209738731384277,
      "learning_rate": 0.0034622770919067215,
      "loss": 0.0193,
      "step": 980
    },
    {
      "epoch": 1.3580246913580247,
      "grad_norm": 0.1960245668888092,
      "learning_rate": 0.00345679012345679,
      "loss": 0.0086,
      "step": 990
    },
    {
      "epoch": 1.3717421124828533,
      "grad_norm": 1.492215633392334,
      "learning_rate": 0.0034513031550068587,
      "loss": 0.0107,
      "step": 1000
    },
    {
      "epoch": 1.3854595336076818,
      "grad_norm": 0.2733677327632904,
      "learning_rate": 0.0034458161865569274,
      "loss": 0.0117,
      "step": 1010
    },
    {
      "epoch": 1.3991769547325104,
      "grad_norm": 1.0395753383636475,
      "learning_rate": 0.003440329218106996,
      "loss": 0.0105,
      "step": 1020
    },
    {
      "epoch": 1.412894375857339,
      "grad_norm": 2.691480875015259,
      "learning_rate": 0.0034348422496570646,
      "loss": 0.0158,
      "step": 1030
    },
    {
      "epoch": 1.4266117969821672,
      "grad_norm": 2.866596221923828,
      "learning_rate": 0.003429355281207133,
      "loss": 0.0213,
      "step": 1040
    },
    {
      "epoch": 1.4403292181069958,
      "grad_norm": 0.7269473671913147,
      "learning_rate": 0.0034238683127572017,
      "loss": 0.0124,
      "step": 1050
    },
    {
      "epoch": 1.4540466392318243,
      "grad_norm": 3.53381609916687,
      "learning_rate": 0.0034183813443072705,
      "loss": 0.0154,
      "step": 1060
    },
    {
      "epoch": 1.4677640603566529,
      "grad_norm": 5.748310565948486,
      "learning_rate": 0.003412894375857339,
      "loss": 0.0259,
      "step": 1070
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 2.57140851020813,
      "learning_rate": 0.0034074074074074076,
      "loss": 0.0155,
      "step": 1080
    },
    {
      "epoch": 1.49519890260631,
      "grad_norm": 1.7304370403289795,
      "learning_rate": 0.0034019204389574764,
      "loss": 0.0244,
      "step": 1090
    },
    {
      "epoch": 1.5089163237311385,
      "grad_norm": 0.2701631486415863,
      "learning_rate": 0.0033964334705075447,
      "loss": 0.0079,
      "step": 1100
    },
    {
      "epoch": 1.522633744855967,
      "grad_norm": 0.058133356273174286,
      "learning_rate": 0.0033909465020576135,
      "loss": 0.006,
      "step": 1110
    },
    {
      "epoch": 1.5363511659807956,
      "grad_norm": 0.13815462589263916,
      "learning_rate": 0.003385459533607682,
      "loss": 0.0072,
      "step": 1120
    },
    {
      "epoch": 1.5500685871056241,
      "grad_norm": 0.10726580023765564,
      "learning_rate": 0.00337997256515775,
      "loss": 0.003,
      "step": 1130
    },
    {
      "epoch": 1.5637860082304527,
      "grad_norm": 0.07423245161771774,
      "learning_rate": 0.003374485596707819,
      "loss": 0.0025,
      "step": 1140
    },
    {
      "epoch": 1.5775034293552812,
      "grad_norm": 0.042405132204294205,
      "learning_rate": 0.0033689986282578873,
      "loss": 0.0029,
      "step": 1150
    },
    {
      "epoch": 1.5912208504801097,
      "grad_norm": 0.15457452833652496,
      "learning_rate": 0.003363511659807956,
      "loss": 0.0038,
      "step": 1160
    },
    {
      "epoch": 1.6049382716049383,
      "grad_norm": 0.19390183687210083,
      "learning_rate": 0.0033580246913580245,
      "loss": 0.0037,
      "step": 1170
    },
    {
      "epoch": 1.6186556927297668,
      "grad_norm": 0.5642995834350586,
      "learning_rate": 0.0033525377229080932,
      "loss": 0.0021,
      "step": 1180
    },
    {
      "epoch": 1.6323731138545954,
      "grad_norm": 0.041702333837747574,
      "learning_rate": 0.003347050754458162,
      "loss": 0.0014,
      "step": 1190
    },
    {
      "epoch": 1.646090534979424,
      "grad_norm": 1.3787133693695068,
      "learning_rate": 0.0033415637860082304,
      "loss": 0.0023,
      "step": 1200
    },
    {
      "epoch": 1.6598079561042525,
      "grad_norm": 0.3663009703159332,
      "learning_rate": 0.003336076817558299,
      "loss": 0.0022,
      "step": 1210
    },
    {
      "epoch": 1.673525377229081,
      "grad_norm": 0.029018864035606384,
      "learning_rate": 0.0033305898491083675,
      "loss": 0.0012,
      "step": 1220
    },
    {
      "epoch": 1.6872427983539096,
      "grad_norm": 0.030573470517992973,
      "learning_rate": 0.0033251028806584363,
      "loss": 0.0011,
      "step": 1230
    },
    {
      "epoch": 1.700960219478738,
      "grad_norm": 0.019351551309227943,
      "learning_rate": 0.003319615912208505,
      "loss": 0.0008,
      "step": 1240
    },
    {
      "epoch": 1.7146776406035666,
      "grad_norm": 0.06518267095088959,
      "learning_rate": 0.0033141289437585734,
      "loss": 0.0007,
      "step": 1250
    },
    {
      "epoch": 1.7283950617283952,
      "grad_norm": 0.007179753389209509,
      "learning_rate": 0.003308641975308642,
      "loss": 0.0005,
      "step": 1260
    },
    {
      "epoch": 1.7421124828532237,
      "grad_norm": 0.004783597309142351,
      "learning_rate": 0.003303155006858711,
      "loss": 0.0005,
      "step": 1270
    },
    {
      "epoch": 1.7558299039780523,
      "grad_norm": 0.006074214819818735,
      "learning_rate": 0.0032976680384087793,
      "loss": 0.0004,
      "step": 1280
    },
    {
      "epoch": 1.7695473251028808,
      "grad_norm": 0.21831804513931274,
      "learning_rate": 0.003292181069958848,
      "loss": 0.0006,
      "step": 1290
    },
    {
      "epoch": 1.7832647462277091,
      "grad_norm": 0.008676720783114433,
      "learning_rate": 0.0032866941015089165,
      "loss": 0.0005,
      "step": 1300
    },
    {
      "epoch": 1.7969821673525377,
      "grad_norm": 0.018993107602000237,
      "learning_rate": 0.003281207133058985,
      "loss": 0.0004,
      "step": 1310
    },
    {
      "epoch": 1.8106995884773662,
      "grad_norm": 0.01370448712259531,
      "learning_rate": 0.0032757201646090536,
      "loss": 0.0004,
      "step": 1320
    },
    {
      "epoch": 1.8244170096021948,
      "grad_norm": 0.0071778870187699795,
      "learning_rate": 0.003270233196159122,
      "loss": 0.0004,
      "step": 1330
    },
    {
      "epoch": 1.8381344307270233,
      "grad_norm": 0.004453766159713268,
      "learning_rate": 0.0032647462277091907,
      "loss": 0.0005,
      "step": 1340
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 0.010501434095203876,
      "learning_rate": 0.003259259259259259,
      "loss": 0.0003,
      "step": 1350
    },
    {
      "epoch": 1.8655692729766804,
      "grad_norm": 0.003696098690852523,
      "learning_rate": 0.003253772290809328,
      "loss": 0.0003,
      "step": 1360
    },
    {
      "epoch": 1.879286694101509,
      "grad_norm": 0.006534223910421133,
      "learning_rate": 0.0032482853223593966,
      "loss": 0.0059,
      "step": 1370
    },
    {
      "epoch": 1.8930041152263375,
      "grad_norm": 0.1527172178030014,
      "learning_rate": 0.003242798353909465,
      "loss": 0.0052,
      "step": 1380
    },
    {
      "epoch": 1.906721536351166,
      "grad_norm": 0.31320658326148987,
      "learning_rate": 0.0032373113854595337,
      "loss": 0.0043,
      "step": 1390
    },
    {
      "epoch": 1.9204389574759944,
      "grad_norm": 2.9821789264678955,
      "learning_rate": 0.003231824417009602,
      "loss": 0.0075,
      "step": 1400
    },
    {
      "epoch": 1.934156378600823,
      "grad_norm": 0.3538370430469513,
      "learning_rate": 0.003226337448559671,
      "loss": 0.0082,
      "step": 1410
    },
    {
      "epoch": 1.9478737997256514,
      "grad_norm": 0.32395270466804504,
      "learning_rate": 0.0032208504801097397,
      "loss": 0.0035,
      "step": 1420
    },
    {
      "epoch": 1.96159122085048,
      "grad_norm": 2.4334521293640137,
      "learning_rate": 0.003215363511659808,
      "loss": 0.003,
      "step": 1430
    },
    {
      "epoch": 1.9753086419753085,
      "grad_norm": 17.39132308959961,
      "learning_rate": 0.0032098765432098768,
      "loss": 0.0242,
      "step": 1440
    },
    {
      "epoch": 1.989026063100137,
      "grad_norm": 3.919358491897583,
      "learning_rate": 0.0032043895747599456,
      "loss": 0.0182,
      "step": 1450
    },
    {
      "epoch": 2.0027434842249656,
      "grad_norm": 0.43184858560562134,
      "learning_rate": 0.003198902606310014,
      "loss": 0.0226,
      "step": 1460
    },
    {
      "epoch": 2.016460905349794,
      "grad_norm": 3.5691072940826416,
      "learning_rate": 0.0031934156378600827,
      "loss": 0.0195,
      "step": 1470
    },
    {
      "epoch": 2.0301783264746227,
      "grad_norm": 0.7574926018714905,
      "learning_rate": 0.0031879286694101506,
      "loss": 0.0098,
      "step": 1480
    },
    {
      "epoch": 2.0438957475994513,
      "grad_norm": 0.1012134924530983,
      "learning_rate": 0.0031824417009602194,
      "loss": 0.0075,
      "step": 1490
    },
    {
      "epoch": 2.05761316872428,
      "grad_norm": 0.6531967520713806,
      "learning_rate": 0.003176954732510288,
      "loss": 0.0033,
      "step": 1500
    },
    {
      "epoch": 2.0713305898491083,
      "grad_norm": 0.038570135831832886,
      "learning_rate": 0.0031714677640603565,
      "loss": 0.0029,
      "step": 1510
    },
    {
      "epoch": 2.085048010973937,
      "grad_norm": 0.11704251170158386,
      "learning_rate": 0.0031659807956104253,
      "loss": 0.0019,
      "step": 1520
    },
    {
      "epoch": 2.0987654320987654,
      "grad_norm": 0.02710708975791931,
      "learning_rate": 0.0031604938271604936,
      "loss": 0.0028,
      "step": 1530
    },
    {
      "epoch": 2.112482853223594,
      "grad_norm": 0.0403568297624588,
      "learning_rate": 0.0031550068587105624,
      "loss": 0.0024,
      "step": 1540
    },
    {
      "epoch": 2.1262002743484225,
      "grad_norm": 0.1495818793773651,
      "learning_rate": 0.003149519890260631,
      "loss": 0.0046,
      "step": 1550
    },
    {
      "epoch": 2.139917695473251,
      "grad_norm": 0.26200664043426514,
      "learning_rate": 0.0031440329218106996,
      "loss": 0.0067,
      "step": 1560
    },
    {
      "epoch": 2.1536351165980796,
      "grad_norm": 0.08924920856952667,
      "learning_rate": 0.0031385459533607683,
      "loss": 0.003,
      "step": 1570
    },
    {
      "epoch": 2.167352537722908,
      "grad_norm": 0.6207463145256042,
      "learning_rate": 0.0031330589849108367,
      "loss": 0.0021,
      "step": 1580
    },
    {
      "epoch": 2.1810699588477367,
      "grad_norm": 0.021208368241786957,
      "learning_rate": 0.0031275720164609055,
      "loss": 0.0014,
      "step": 1590
    },
    {
      "epoch": 2.1947873799725652,
      "grad_norm": 0.015034540556371212,
      "learning_rate": 0.0031220850480109742,
      "loss": 0.0015,
      "step": 1600
    },
    {
      "epoch": 2.2085048010973938,
      "grad_norm": 0.022754216566681862,
      "learning_rate": 0.0031165980795610426,
      "loss": 0.0012,
      "step": 1610
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.1630395650863647,
      "learning_rate": 0.0031111111111111114,
      "loss": 0.0116,
      "step": 1620
    },
    {
      "epoch": 2.235939643347051,
      "grad_norm": 0.09275396913290024,
      "learning_rate": 0.0031056241426611797,
      "loss": 0.0065,
      "step": 1630
    },
    {
      "epoch": 2.2496570644718794,
      "grad_norm": 3.0868425369262695,
      "learning_rate": 0.0031001371742112485,
      "loss": 0.0055,
      "step": 1640
    },
    {
      "epoch": 2.263374485596708,
      "grad_norm": 0.17457349598407745,
      "learning_rate": 0.0030946502057613173,
      "loss": 0.0046,
      "step": 1650
    },
    {
      "epoch": 2.2770919067215365,
      "grad_norm": 0.2302320897579193,
      "learning_rate": 0.003089163237311385,
      "loss": 0.0071,
      "step": 1660
    },
    {
      "epoch": 2.290809327846365,
      "grad_norm": 2.841557025909424,
      "learning_rate": 0.003083676268861454,
      "loss": 0.0084,
      "step": 1670
    },
    {
      "epoch": 2.3045267489711936,
      "grad_norm": 0.07868708670139313,
      "learning_rate": 0.0030781893004115228,
      "loss": 0.0057,
      "step": 1680
    },
    {
      "epoch": 2.318244170096022,
      "grad_norm": 2.7797863483428955,
      "learning_rate": 0.003072702331961591,
      "loss": 0.0048,
      "step": 1690
    },
    {
      "epoch": 2.3319615912208507,
      "grad_norm": 0.32815805077552795,
      "learning_rate": 0.00306721536351166,
      "loss": 0.0038,
      "step": 1700
    },
    {
      "epoch": 2.3456790123456788,
      "grad_norm": 2.8385255336761475,
      "learning_rate": 0.0030617283950617282,
      "loss": 0.0069,
      "step": 1710
    },
    {
      "epoch": 2.3593964334705078,
      "grad_norm": 0.1052158772945404,
      "learning_rate": 0.003056241426611797,
      "loss": 0.0093,
      "step": 1720
    },
    {
      "epoch": 2.373113854595336,
      "grad_norm": 0.16486582159996033,
      "learning_rate": 0.003050754458161866,
      "loss": 0.0044,
      "step": 1730
    },
    {
      "epoch": 2.386831275720165,
      "grad_norm": 1.8320555686950684,
      "learning_rate": 0.003045267489711934,
      "loss": 0.0034,
      "step": 1740
    },
    {
      "epoch": 2.400548696844993,
      "grad_norm": 0.06956902891397476,
      "learning_rate": 0.003039780521262003,
      "loss": 0.0036,
      "step": 1750
    },
    {
      "epoch": 2.4142661179698215,
      "grad_norm": 0.04147619009017944,
      "learning_rate": 0.0030342935528120713,
      "loss": 0.0044,
      "step": 1760
    },
    {
      "epoch": 2.42798353909465,
      "grad_norm": 0.1378372609615326,
      "learning_rate": 0.00302880658436214,
      "loss": 0.0064,
      "step": 1770
    },
    {
      "epoch": 2.4417009602194786,
      "grad_norm": 0.2938307225704193,
      "learning_rate": 0.003023319615912209,
      "loss": 0.0102,
      "step": 1780
    },
    {
      "epoch": 2.455418381344307,
      "grad_norm": 0.07331833243370056,
      "learning_rate": 0.003017832647462277,
      "loss": 0.0063,
      "step": 1790
    },
    {
      "epoch": 2.4691358024691357,
      "grad_norm": 3.1299517154693604,
      "learning_rate": 0.003012345679012346,
      "loss": 0.0295,
      "step": 1800
    },
    {
      "epoch": 2.482853223593964,
      "grad_norm": 0.24580276012420654,
      "learning_rate": 0.0030068587105624143,
      "loss": 0.0417,
      "step": 1810
    },
    {
      "epoch": 2.4965706447187928,
      "grad_norm": 0.6949389576911926,
      "learning_rate": 0.003001371742112483,
      "loss": 0.0104,
      "step": 1820
    },
    {
      "epoch": 2.5102880658436213,
      "grad_norm": 0.44596511125564575,
      "learning_rate": 0.0029958847736625514,
      "loss": 0.0055,
      "step": 1830
    },
    {
      "epoch": 2.52400548696845,
      "grad_norm": 0.3634553849697113,
      "learning_rate": 0.0029903978052126198,
      "loss": 0.0064,
      "step": 1840
    },
    {
      "epoch": 2.5377229080932784,
      "grad_norm": 0.30697545409202576,
      "learning_rate": 0.0029849108367626886,
      "loss": 0.0071,
      "step": 1850
    },
    {
      "epoch": 2.551440329218107,
      "grad_norm": 0.09807620942592621,
      "learning_rate": 0.0029794238683127573,
      "loss": 0.0027,
      "step": 1860
    },
    {
      "epoch": 2.5651577503429355,
      "grad_norm": 0.023437203839421272,
      "learning_rate": 0.0029739368998628257,
      "loss": 0.0015,
      "step": 1870
    },
    {
      "epoch": 2.578875171467764,
      "grad_norm": 0.02568647265434265,
      "learning_rate": 0.0029684499314128945,
      "loss": 0.0012,
      "step": 1880
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 0.011096304282546043,
      "learning_rate": 0.002962962962962963,
      "loss": 0.0008,
      "step": 1890
    },
    {
      "epoch": 2.606310013717421,
      "grad_norm": 0.012583097442984581,
      "learning_rate": 0.0029574759945130316,
      "loss": 0.0007,
      "step": 1900
    },
    {
      "epoch": 2.6200274348422496,
      "grad_norm": 4.343296527862549,
      "learning_rate": 0.0029519890260631004,
      "loss": 0.0062,
      "step": 1910
    },
    {
      "epoch": 2.633744855967078,
      "grad_norm": 0.1520501673221588,
      "learning_rate": 0.0029465020576131687,
      "loss": 0.0012,
      "step": 1920
    },
    {
      "epoch": 2.6474622770919067,
      "grad_norm": 0.2666127383708954,
      "learning_rate": 0.0029410150891632375,
      "loss": 0.0014,
      "step": 1930
    },
    {
      "epoch": 2.6611796982167353,
      "grad_norm": 0.029684538021683693,
      "learning_rate": 0.002935528120713306,
      "loss": 0.0009,
      "step": 1940
    },
    {
      "epoch": 2.674897119341564,
      "grad_norm": 0.010009915567934513,
      "learning_rate": 0.0029300411522633746,
      "loss": 0.0009,
      "step": 1950
    },
    {
      "epoch": 2.6886145404663924,
      "grad_norm": 0.011954572051763535,
      "learning_rate": 0.0029245541838134434,
      "loss": 0.0005,
      "step": 1960
    },
    {
      "epoch": 2.702331961591221,
      "grad_norm": 0.005731647834181786,
      "learning_rate": 0.0029190672153635118,
      "loss": 0.0005,
      "step": 1970
    },
    {
      "epoch": 2.7160493827160495,
      "grad_norm": 0.010606400668621063,
      "learning_rate": 0.0029135802469135805,
      "loss": 0.0012,
      "step": 1980
    },
    {
      "epoch": 2.729766803840878,
      "grad_norm": 0.1387273520231247,
      "learning_rate": 0.002908093278463649,
      "loss": 0.0018,
      "step": 1990
    },
    {
      "epoch": 2.7434842249657065,
      "grad_norm": 0.06201981380581856,
      "learning_rate": 0.0029026063100137172,
      "loss": 0.0015,
      "step": 2000
    },
    {
      "epoch": 2.757201646090535,
      "grad_norm": 0.16873539984226227,
      "learning_rate": 0.002897119341563786,
      "loss": 0.0068,
      "step": 2010
    },
    {
      "epoch": 2.7709190672153636,
      "grad_norm": 0.07312056422233582,
      "learning_rate": 0.0028916323731138544,
      "loss": 0.0065,
      "step": 2020
    },
    {
      "epoch": 2.784636488340192,
      "grad_norm": 1.4947420358657837,
      "learning_rate": 0.002886145404663923,
      "loss": 0.0151,
      "step": 2030
    },
    {
      "epoch": 2.7983539094650207,
      "grad_norm": 0.07982174307107925,
      "learning_rate": 0.002880658436213992,
      "loss": 0.003,
      "step": 2040
    },
    {
      "epoch": 2.8120713305898493,
      "grad_norm": 0.6236079931259155,
      "learning_rate": 0.0028751714677640603,
      "loss": 0.0025,
      "step": 2050
    },
    {
      "epoch": 2.825788751714678,
      "grad_norm": 0.03273460641503334,
      "learning_rate": 0.002869684499314129,
      "loss": 0.0022,
      "step": 2060
    },
    {
      "epoch": 2.8395061728395063,
      "grad_norm": 0.015444724820554256,
      "learning_rate": 0.0028641975308641974,
      "loss": 0.0011,
      "step": 2070
    },
    {
      "epoch": 2.8532235939643344,
      "grad_norm": 0.019327662885189056,
      "learning_rate": 0.002858710562414266,
      "loss": 0.0012,
      "step": 2080
    },
    {
      "epoch": 2.8669410150891634,
      "grad_norm": 0.008190714754164219,
      "learning_rate": 0.002853223593964335,
      "loss": 0.0009,
      "step": 2090
    },
    {
      "epoch": 2.8806584362139915,
      "grad_norm": 0.2650699317455292,
      "learning_rate": 0.0028477366255144033,
      "loss": 0.0017,
      "step": 2100
    },
    {
      "epoch": 2.8943758573388205,
      "grad_norm": 0.5351266860961914,
      "learning_rate": 0.002842249657064472,
      "loss": 0.0072,
      "step": 2110
    },
    {
      "epoch": 2.9080932784636486,
      "grad_norm": 0.1880808174610138,
      "learning_rate": 0.0028367626886145404,
      "loss": 0.0027,
      "step": 2120
    },
    {
      "epoch": 2.9218106995884776,
      "grad_norm": 6.402782440185547,
      "learning_rate": 0.0028312757201646092,
      "loss": 0.0059,
      "step": 2130
    },
    {
      "epoch": 2.9355281207133057,
      "grad_norm": 0.1735450029373169,
      "learning_rate": 0.002825788751714678,
      "loss": 0.0084,
      "step": 2140
    },
    {
      "epoch": 2.9492455418381347,
      "grad_norm": 0.06327307969331741,
      "learning_rate": 0.0028203017832647463,
      "loss": 0.0026,
      "step": 2150
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 2.2099366188049316,
      "learning_rate": 0.002814814814814815,
      "loss": 0.0036,
      "step": 2160
    },
    {
      "epoch": 2.9766803840877913,
      "grad_norm": 0.06577832996845245,
      "learning_rate": 0.002809327846364883,
      "loss": 0.0017,
      "step": 2170
    },
    {
      "epoch": 2.99039780521262,
      "grad_norm": 0.034665368497371674,
      "learning_rate": 0.002803840877914952,
      "loss": 0.0013,
      "step": 2180
    },
    {
      "epoch": 3.0041152263374484,
      "grad_norm": 0.07215996831655502,
      "learning_rate": 0.0027983539094650206,
      "loss": 0.0029,
      "step": 2190
    },
    {
      "epoch": 3.017832647462277,
      "grad_norm": 0.13844533264636993,
      "learning_rate": 0.002792866941015089,
      "loss": 0.0032,
      "step": 2200
    },
    {
      "epoch": 3.0315500685871055,
      "grad_norm": 1.9053531885147095,
      "learning_rate": 0.0027873799725651577,
      "loss": 0.0171,
      "step": 2210
    },
    {
      "epoch": 3.045267489711934,
      "grad_norm": 1.5286877155303955,
      "learning_rate": 0.0027818930041152265,
      "loss": 0.0075,
      "step": 2220
    },
    {
      "epoch": 3.0589849108367626,
      "grad_norm": 0.037277668714523315,
      "learning_rate": 0.002776406035665295,
      "loss": 0.0107,
      "step": 2230
    },
    {
      "epoch": 3.072702331961591,
      "grad_norm": 0.6392796039581299,
      "learning_rate": 0.0027709190672153636,
      "loss": 0.0046,
      "step": 2240
    },
    {
      "epoch": 3.0864197530864197,
      "grad_norm": 0.06115949898958206,
      "learning_rate": 0.002765432098765432,
      "loss": 0.0029,
      "step": 2250
    },
    {
      "epoch": 3.1001371742112482,
      "grad_norm": 0.02142229676246643,
      "learning_rate": 0.0027599451303155008,
      "loss": 0.0025,
      "step": 2260
    },
    {
      "epoch": 3.113854595336077,
      "grad_norm": 0.7401732206344604,
      "learning_rate": 0.0027544581618655696,
      "loss": 0.003,
      "step": 2270
    },
    {
      "epoch": 3.1275720164609053,
      "grad_norm": 0.13516952097415924,
      "learning_rate": 0.002748971193415638,
      "loss": 0.0027,
      "step": 2280
    },
    {
      "epoch": 3.141289437585734,
      "grad_norm": 0.025928517803549767,
      "learning_rate": 0.0027434842249657067,
      "loss": 0.0029,
      "step": 2290
    },
    {
      "epoch": 3.1550068587105624,
      "grad_norm": 0.06315171718597412,
      "learning_rate": 0.002737997256515775,
      "loss": 0.0112,
      "step": 2300
    },
    {
      "epoch": 3.168724279835391,
      "grad_norm": 0.11292346566915512,
      "learning_rate": 0.002732510288065844,
      "loss": 0.0075,
      "step": 2310
    },
    {
      "epoch": 3.1824417009602195,
      "grad_norm": 0.4172564744949341,
      "learning_rate": 0.0027270233196159126,
      "loss": 0.0024,
      "step": 2320
    },
    {
      "epoch": 3.196159122085048,
      "grad_norm": 1.852839708328247,
      "learning_rate": 0.002721536351165981,
      "loss": 0.006,
      "step": 2330
    },
    {
      "epoch": 3.2098765432098766,
      "grad_norm": 0.030120542272925377,
      "learning_rate": 0.0027160493827160497,
      "loss": 0.004,
      "step": 2340
    },
    {
      "epoch": 3.223593964334705,
      "grad_norm": 0.11482139676809311,
      "learning_rate": 0.0027105624142661176,
      "loss": 0.004,
      "step": 2350
    },
    {
      "epoch": 3.2373113854595337,
      "grad_norm": 0.06720826029777527,
      "learning_rate": 0.0027050754458161864,
      "loss": 0.0021,
      "step": 2360
    },
    {
      "epoch": 3.251028806584362,
      "grad_norm": 0.02401961386203766,
      "learning_rate": 0.002699588477366255,
      "loss": 0.0015,
      "step": 2370
    },
    {
      "epoch": 3.2647462277091908,
      "grad_norm": 0.02388683333992958,
      "learning_rate": 0.0026941015089163235,
      "loss": 0.0012,
      "step": 2380
    },
    {
      "epoch": 3.2784636488340193,
      "grad_norm": 0.00940149836242199,
      "learning_rate": 0.0026886145404663923,
      "loss": 0.0008,
      "step": 2390
    },
    {
      "epoch": 3.292181069958848,
      "grad_norm": 0.08504175394773483,
      "learning_rate": 0.002683127572016461,
      "loss": 0.0006,
      "step": 2400
    },
    {
      "epoch": 3.3058984910836764,
      "grad_norm": 0.11855009198188782,
      "learning_rate": 0.0026776406035665294,
      "loss": 0.0006,
      "step": 2410
    },
    {
      "epoch": 3.319615912208505,
      "grad_norm": 0.00688385684043169,
      "learning_rate": 0.0026721536351165982,
      "loss": 0.0004,
      "step": 2420
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.005145902745425701,
      "learning_rate": 0.0026666666666666666,
      "loss": 0.0004,
      "step": 2430
    },
    {
      "epoch": 3.347050754458162,
      "grad_norm": 0.00445746211335063,
      "learning_rate": 0.0026611796982167354,
      "loss": 0.0004,
      "step": 2440
    },
    {
      "epoch": 3.3607681755829906,
      "grad_norm": 0.010125778615474701,
      "learning_rate": 0.002655692729766804,
      "loss": 0.0004,
      "step": 2450
    },
    {
      "epoch": 3.374485596707819,
      "grad_norm": 0.002951627131551504,
      "learning_rate": 0.0026502057613168725,
      "loss": 0.0004,
      "step": 2460
    },
    {
      "epoch": 3.388203017832647,
      "grad_norm": 0.002508001634851098,
      "learning_rate": 0.0026447187928669413,
      "loss": 0.0003,
      "step": 2470
    },
    {
      "epoch": 3.401920438957476,
      "grad_norm": 0.0026546609587967396,
      "learning_rate": 0.0026392318244170096,
      "loss": 0.0003,
      "step": 2480
    },
    {
      "epoch": 3.4156378600823043,
      "grad_norm": 0.004047069698572159,
      "learning_rate": 0.0026337448559670784,
      "loss": 0.0003,
      "step": 2490
    },
    {
      "epoch": 3.4293552812071333,
      "grad_norm": 0.00400233268737793,
      "learning_rate": 0.002628257887517147,
      "loss": 0.0003,
      "step": 2500
    },
    {
      "epoch": 3.4430727023319614,
      "grad_norm": 0.0021905070170760155,
      "learning_rate": 0.0026227709190672155,
      "loss": 0.0003,
      "step": 2510
    },
    {
      "epoch": 3.45679012345679,
      "grad_norm": 0.002513401908800006,
      "learning_rate": 0.002617283950617284,
      "loss": 0.0003,
      "step": 2520
    },
    {
      "epoch": 3.4705075445816185,
      "grad_norm": 0.002495032735168934,
      "learning_rate": 0.0026117969821673522,
      "loss": 0.0003,
      "step": 2530
    },
    {
      "epoch": 3.484224965706447,
      "grad_norm": 0.002583242254331708,
      "learning_rate": 0.002606310013717421,
      "loss": 0.0002,
      "step": 2540
    },
    {
      "epoch": 3.4979423868312756,
      "grad_norm": 0.002090156776830554,
      "learning_rate": 0.0026008230452674898,
      "loss": 0.0003,
      "step": 2550
    },
    {
      "epoch": 3.511659807956104,
      "grad_norm": 0.007597356103360653,
      "learning_rate": 0.002595336076817558,
      "loss": 0.0002,
      "step": 2560
    },
    {
      "epoch": 3.5253772290809327,
      "grad_norm": 0.0020055435597896576,
      "learning_rate": 0.002589849108367627,
      "loss": 0.0002,
      "step": 2570
    },
    {
      "epoch": 3.539094650205761,
      "grad_norm": 0.00487479055300355,
      "learning_rate": 0.0025843621399176953,
      "loss": 0.0002,
      "step": 2580
    },
    {
      "epoch": 3.5528120713305897,
      "grad_norm": 0.0018157637678086758,
      "learning_rate": 0.002578875171467764,
      "loss": 0.0003,
      "step": 2590
    },
    {
      "epoch": 3.5665294924554183,
      "grad_norm": 0.017930366098880768,
      "learning_rate": 0.002573388203017833,
      "loss": 0.0003,
      "step": 2600
    },
    {
      "epoch": 3.580246913580247,
      "grad_norm": 0.005410470068454742,
      "learning_rate": 0.002567901234567901,
      "loss": 0.0002,
      "step": 2610
    },
    {
      "epoch": 3.5939643347050754,
      "grad_norm": 0.002658593701198697,
      "learning_rate": 0.00256241426611797,
      "loss": 0.0002,
      "step": 2620
    },
    {
      "epoch": 3.607681755829904,
      "grad_norm": 0.008872374892234802,
      "learning_rate": 0.0025569272976680387,
      "loss": 0.0003,
      "step": 2630
    },
    {
      "epoch": 3.6213991769547325,
      "grad_norm": 0.031665947288274765,
      "learning_rate": 0.002551440329218107,
      "loss": 0.0003,
      "step": 2640
    },
    {
      "epoch": 3.635116598079561,
      "grad_norm": 0.0031069600954651833,
      "learning_rate": 0.002545953360768176,
      "loss": 0.0002,
      "step": 2650
    },
    {
      "epoch": 3.6488340192043895,
      "grad_norm": 0.00202539237216115,
      "learning_rate": 0.002540466392318244,
      "loss": 0.0002,
      "step": 2660
    },
    {
      "epoch": 3.662551440329218,
      "grad_norm": 0.0023754937574267387,
      "learning_rate": 0.002534979423868313,
      "loss": 0.0005,
      "step": 2670
    },
    {
      "epoch": 3.6762688614540466,
      "grad_norm": 0.0036066994071006775,
      "learning_rate": 0.0025294924554183818,
      "loss": 0.0003,
      "step": 2680
    },
    {
      "epoch": 3.689986282578875,
      "grad_norm": 0.0048004393465816975,
      "learning_rate": 0.0025240054869684497,
      "loss": 0.0002,
      "step": 2690
    },
    {
      "epoch": 3.7037037037037037,
      "grad_norm": 0.0022084855008870363,
      "learning_rate": 0.0025185185185185185,
      "loss": 0.0002,
      "step": 2700
    },
    {
      "epoch": 3.7174211248285323,
      "grad_norm": 0.002644112566486001,
      "learning_rate": 0.002513031550068587,
      "loss": 0.0002,
      "step": 2710
    },
    {
      "epoch": 3.731138545953361,
      "grad_norm": 0.004102760925889015,
      "learning_rate": 0.0025075445816186556,
      "loss": 0.0002,
      "step": 2720
    },
    {
      "epoch": 3.7448559670781894,
      "grad_norm": 0.004511853214353323,
      "learning_rate": 0.0025020576131687244,
      "loss": 0.0002,
      "step": 2730
    },
    {
      "epoch": 3.758573388203018,
      "grad_norm": 0.004910784773528576,
      "learning_rate": 0.0024965706447187927,
      "loss": 0.0002,
      "step": 2740
    },
    {
      "epoch": 3.7722908093278464,
      "grad_norm": 0.0016586155397817492,
      "learning_rate": 0.0024910836762688615,
      "loss": 0.0002,
      "step": 2750
    },
    {
      "epoch": 3.786008230452675,
      "grad_norm": 0.004278423264622688,
      "learning_rate": 0.00248559670781893,
      "loss": 0.0002,
      "step": 2760
    },
    {
      "epoch": 3.7997256515775035,
      "grad_norm": 0.001237638178281486,
      "learning_rate": 0.0024801097393689986,
      "loss": 0.0002,
      "step": 2770
    },
    {
      "epoch": 3.813443072702332,
      "grad_norm": 0.0033684903755784035,
      "learning_rate": 0.0024746227709190674,
      "loss": 0.0002,
      "step": 2780
    },
    {
      "epoch": 3.8271604938271606,
      "grad_norm": 0.0025746922474354506,
      "learning_rate": 0.0024691358024691358,
      "loss": 0.0002,
      "step": 2790
    },
    {
      "epoch": 3.840877914951989,
      "grad_norm": 0.0012643272057175636,
      "learning_rate": 0.0024636488340192045,
      "loss": 0.0002,
      "step": 2800
    },
    {
      "epoch": 3.8545953360768177,
      "grad_norm": 0.0020842733792960644,
      "learning_rate": 0.0024581618655692733,
      "loss": 0.0002,
      "step": 2810
    },
    {
      "epoch": 3.8683127572016462,
      "grad_norm": 0.001501398510299623,
      "learning_rate": 0.0024526748971193417,
      "loss": 0.0002,
      "step": 2820
    },
    {
      "epoch": 3.882030178326475,
      "grad_norm": 0.0027897374238818884,
      "learning_rate": 0.0024471879286694104,
      "loss": 0.0002,
      "step": 2830
    },
    {
      "epoch": 3.895747599451303,
      "grad_norm": 0.003549241228029132,
      "learning_rate": 0.002441700960219479,
      "loss": 0.0002,
      "step": 2840
    },
    {
      "epoch": 3.909465020576132,
      "grad_norm": 0.0017101753037422895,
      "learning_rate": 0.0024362139917695476,
      "loss": 0.0002,
      "step": 2850
    },
    {
      "epoch": 3.92318244170096,
      "grad_norm": 0.0019579664804041386,
      "learning_rate": 0.0024307270233196164,
      "loss": 0.0002,
      "step": 2860
    },
    {
      "epoch": 3.936899862825789,
      "grad_norm": 0.0023383598309010267,
      "learning_rate": 0.0024252400548696843,
      "loss": 0.0001,
      "step": 2870
    },
    {
      "epoch": 3.950617283950617,
      "grad_norm": 0.0015113148838281631,
      "learning_rate": 0.002419753086419753,
      "loss": 0.0002,
      "step": 2880
    },
    {
      "epoch": 3.964334705075446,
      "grad_norm": 0.005561290308833122,
      "learning_rate": 0.0024142661179698214,
      "loss": 0.0001,
      "step": 2890
    },
    {
      "epoch": 3.978052126200274,
      "grad_norm": 0.001230288646183908,
      "learning_rate": 0.00240877914951989,
      "loss": 0.0002,
      "step": 2900
    },
    {
      "epoch": 3.991769547325103,
      "grad_norm": 0.0015020390274003148,
      "learning_rate": 0.002403292181069959,
      "loss": 0.0001,
      "step": 2910
    },
    {
      "epoch": 4.005486968449931,
      "grad_norm": 0.0017874010372906923,
      "learning_rate": 0.0023978052126200273,
      "loss": 0.0001,
      "step": 2920
    },
    {
      "epoch": 4.01920438957476,
      "grad_norm": 0.0011951122432947159,
      "learning_rate": 0.002392318244170096,
      "loss": 0.0002,
      "step": 2930
    },
    {
      "epoch": 4.032921810699588,
      "grad_norm": 0.0010632078628987074,
      "learning_rate": 0.0023868312757201644,
      "loss": 0.0002,
      "step": 2940
    },
    {
      "epoch": 4.046639231824417,
      "grad_norm": 0.0015212746802717447,
      "learning_rate": 0.002381344307270233,
      "loss": 0.0001,
      "step": 2950
    },
    {
      "epoch": 4.060356652949245,
      "grad_norm": 0.002663977211341262,
      "learning_rate": 0.002375857338820302,
      "loss": 0.0002,
      "step": 2960
    },
    {
      "epoch": 4.074074074074074,
      "grad_norm": 0.0016571758314967155,
      "learning_rate": 0.0023703703703703703,
      "loss": 0.0001,
      "step": 2970
    },
    {
      "epoch": 4.0877914951989025,
      "grad_norm": 0.0014706564834341407,
      "learning_rate": 0.002364883401920439,
      "loss": 0.0002,
      "step": 2980
    },
    {
      "epoch": 4.1015089163237315,
      "grad_norm": 0.0011087526800110936,
      "learning_rate": 0.002359396433470508,
      "loss": 0.0002,
      "step": 2990
    },
    {
      "epoch": 4.11522633744856,
      "grad_norm": 0.0018552889814600348,
      "learning_rate": 0.0023539094650205762,
      "loss": 0.0002,
      "step": 3000
    },
    {
      "epoch": 4.128943758573389,
      "grad_norm": 0.0018833172507584095,
      "learning_rate": 0.002348422496570645,
      "loss": 0.0001,
      "step": 3010
    },
    {
      "epoch": 4.142661179698217,
      "grad_norm": 0.0014493359485641122,
      "learning_rate": 0.0023429355281207134,
      "loss": 0.0002,
      "step": 3020
    },
    {
      "epoch": 4.156378600823046,
      "grad_norm": 0.0011537574464455247,
      "learning_rate": 0.002337448559670782,
      "loss": 0.0001,
      "step": 3030
    },
    {
      "epoch": 4.170096021947874,
      "grad_norm": 0.0013885247753933072,
      "learning_rate": 0.0023319615912208505,
      "loss": 0.0002,
      "step": 3040
    },
    {
      "epoch": 4.183813443072703,
      "grad_norm": 0.005427801050245762,
      "learning_rate": 0.002326474622770919,
      "loss": 0.0001,
      "step": 3050
    },
    {
      "epoch": 4.197530864197531,
      "grad_norm": 0.0011422134703025222,
      "learning_rate": 0.0023209876543209876,
      "loss": 0.0001,
      "step": 3060
    },
    {
      "epoch": 4.211248285322359,
      "grad_norm": 0.003809132380411029,
      "learning_rate": 0.002315500685871056,
      "loss": 0.0001,
      "step": 3070
    },
    {
      "epoch": 4.224965706447188,
      "grad_norm": 0.0016624514246359468,
      "learning_rate": 0.0023100137174211248,
      "loss": 0.0001,
      "step": 3080
    },
    {
      "epoch": 4.238683127572016,
      "grad_norm": 0.0012955807615071535,
      "learning_rate": 0.0023045267489711935,
      "loss": 0.0001,
      "step": 3090
    },
    {
      "epoch": 4.252400548696845,
      "grad_norm": 0.0011392554733902216,
      "learning_rate": 0.002299039780521262,
      "loss": 0.0001,
      "step": 3100
    },
    {
      "epoch": 4.266117969821673,
      "grad_norm": 0.0010633831843733788,
      "learning_rate": 0.0022935528120713307,
      "loss": 0.0001,
      "step": 3110
    },
    {
      "epoch": 4.279835390946502,
      "grad_norm": 0.0007296721450984478,
      "learning_rate": 0.002288065843621399,
      "loss": 0.0001,
      "step": 3120
    },
    {
      "epoch": 4.29355281207133,
      "grad_norm": 0.0009504772606305778,
      "learning_rate": 0.002282578875171468,
      "loss": 0.0001,
      "step": 3130
    },
    {
      "epoch": 4.307270233196159,
      "grad_norm": 0.0019498406909406185,
      "learning_rate": 0.0022770919067215366,
      "loss": 0.0001,
      "step": 3140
    },
    {
      "epoch": 4.320987654320987,
      "grad_norm": 0.002592818345874548,
      "learning_rate": 0.002271604938271605,
      "loss": 0.0001,
      "step": 3150
    },
    {
      "epoch": 4.334705075445816,
      "grad_norm": 0.001106193638406694,
      "learning_rate": 0.0022661179698216737,
      "loss": 0.0001,
      "step": 3160
    },
    {
      "epoch": 4.348422496570644,
      "grad_norm": 0.0013510981807485223,
      "learning_rate": 0.0022606310013717425,
      "loss": 0.0001,
      "step": 3170
    },
    {
      "epoch": 4.362139917695473,
      "grad_norm": 0.001131572644226253,
      "learning_rate": 0.002255144032921811,
      "loss": 0.0001,
      "step": 3180
    },
    {
      "epoch": 4.3758573388203015,
      "grad_norm": 0.001454192795790732,
      "learning_rate": 0.0022496570644718796,
      "loss": 0.0001,
      "step": 3190
    },
    {
      "epoch": 4.3895747599451305,
      "grad_norm": 0.0006763855344615877,
      "learning_rate": 0.002244170096021948,
      "loss": 0.0001,
      "step": 3200
    },
    {
      "epoch": 4.403292181069959,
      "grad_norm": 0.0014144409215077758,
      "learning_rate": 0.0022386831275720167,
      "loss": 0.0001,
      "step": 3210
    },
    {
      "epoch": 4.4170096021947876,
      "grad_norm": 0.0011557945981621742,
      "learning_rate": 0.002233196159122085,
      "loss": 0.0001,
      "step": 3220
    },
    {
      "epoch": 4.430727023319616,
      "grad_norm": 0.0010556734632700682,
      "learning_rate": 0.0022277091906721534,
      "loss": 0.0001,
      "step": 3230
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.0010192819172516465,
      "learning_rate": 0.0022222222222222222,
      "loss": 0.0001,
      "step": 3240
    },
    {
      "epoch": 4.458161865569273,
      "grad_norm": 0.0016562347300350666,
      "learning_rate": 0.0022167352537722906,
      "loss": 0.0001,
      "step": 3250
    },
    {
      "epoch": 4.471879286694102,
      "grad_norm": 0.0013456550659611821,
      "learning_rate": 0.0022112482853223593,
      "loss": 0.0001,
      "step": 3260
    },
    {
      "epoch": 4.48559670781893,
      "grad_norm": 0.0017605352913960814,
      "learning_rate": 0.002205761316872428,
      "loss": 0.0001,
      "step": 3270
    },
    {
      "epoch": 4.499314128943759,
      "grad_norm": 0.000689368462190032,
      "learning_rate": 0.0022002743484224965,
      "loss": 0.0001,
      "step": 3280
    },
    {
      "epoch": 4.513031550068587,
      "grad_norm": 0.0013552384916692972,
      "learning_rate": 0.0021947873799725653,
      "loss": 0.0001,
      "step": 3290
    },
    {
      "epoch": 4.526748971193416,
      "grad_norm": 0.0007789325900375843,
      "learning_rate": 0.0021893004115226336,
      "loss": 0.0001,
      "step": 3300
    },
    {
      "epoch": 4.540466392318244,
      "grad_norm": 0.002561878878623247,
      "learning_rate": 0.0021838134430727024,
      "loss": 0.0001,
      "step": 3310
    },
    {
      "epoch": 4.554183813443073,
      "grad_norm": 0.0015518440632149577,
      "learning_rate": 0.002178326474622771,
      "loss": 0.0001,
      "step": 3320
    },
    {
      "epoch": 4.567901234567901,
      "grad_norm": 0.0011058344971388578,
      "learning_rate": 0.0021728395061728395,
      "loss": 0.0001,
      "step": 3330
    },
    {
      "epoch": 4.58161865569273,
      "grad_norm": 0.0008706008084118366,
      "learning_rate": 0.0021673525377229083,
      "loss": 0.0001,
      "step": 3340
    },
    {
      "epoch": 4.595336076817558,
      "grad_norm": 0.0010210041655227542,
      "learning_rate": 0.0021618655692729766,
      "loss": 0.0001,
      "step": 3350
    },
    {
      "epoch": 4.609053497942387,
      "grad_norm": 0.0025373089592903852,
      "learning_rate": 0.0021563786008230454,
      "loss": 0.0001,
      "step": 3360
    },
    {
      "epoch": 4.622770919067215,
      "grad_norm": 0.001905267359688878,
      "learning_rate": 0.002150891632373114,
      "loss": 0.0001,
      "step": 3370
    },
    {
      "epoch": 4.636488340192044,
      "grad_norm": 0.0038350936956703663,
      "learning_rate": 0.0021454046639231826,
      "loss": 0.0011,
      "step": 3380
    },
    {
      "epoch": 4.650205761316872,
      "grad_norm": 0.03215411677956581,
      "learning_rate": 0.002139917695473251,
      "loss": 0.0015,
      "step": 3390
    },
    {
      "epoch": 4.663923182441701,
      "grad_norm": 0.050950098782777786,
      "learning_rate": 0.0021344307270233197,
      "loss": 0.0114,
      "step": 3400
    },
    {
      "epoch": 4.677640603566529,
      "grad_norm": 0.22245721518993378,
      "learning_rate": 0.002128943758573388,
      "loss": 0.0079,
      "step": 3410
    },
    {
      "epoch": 4.6913580246913575,
      "grad_norm": 0.05828854441642761,
      "learning_rate": 0.002123456790123457,
      "loss": 0.0045,
      "step": 3420
    },
    {
      "epoch": 4.7050754458161865,
      "grad_norm": 0.17242659628391266,
      "learning_rate": 0.002117969821673525,
      "loss": 0.0037,
      "step": 3430
    },
    {
      "epoch": 4.7187928669410155,
      "grad_norm": 0.3634680509567261,
      "learning_rate": 0.002112482853223594,
      "loss": 0.0029,
      "step": 3440
    },
    {
      "epoch": 4.732510288065844,
      "grad_norm": 0.028742166236042976,
      "learning_rate": 0.0021069958847736627,
      "loss": 0.0015,
      "step": 3450
    },
    {
      "epoch": 4.746227709190672,
      "grad_norm": 0.20705001056194305,
      "learning_rate": 0.002101508916323731,
      "loss": 0.0011,
      "step": 3460
    },
    {
      "epoch": 4.759945130315501,
      "grad_norm": 0.02369321510195732,
      "learning_rate": 0.0020960219478738,
      "loss": 0.0006,
      "step": 3470
    },
    {
      "epoch": 4.77366255144033,
      "grad_norm": 0.004729756619781256,
      "learning_rate": 0.002090534979423868,
      "loss": 0.0004,
      "step": 3480
    },
    {
      "epoch": 4.787379972565158,
      "grad_norm": 0.004917320795357227,
      "learning_rate": 0.002085048010973937,
      "loss": 0.0005,
      "step": 3490
    },
    {
      "epoch": 4.801097393689986,
      "grad_norm": 0.018097400665283203,
      "learning_rate": 0.0020795610425240058,
      "loss": 0.0003,
      "step": 3500
    },
    {
      "epoch": 4.814814814814815,
      "grad_norm": 0.0038112145848572254,
      "learning_rate": 0.002074074074074074,
      "loss": 0.0003,
      "step": 3510
    },
    {
      "epoch": 4.828532235939643,
      "grad_norm": 0.002780374838039279,
      "learning_rate": 0.002068587105624143,
      "loss": 0.0004,
      "step": 3520
    },
    {
      "epoch": 4.842249657064472,
      "grad_norm": 0.05895451828837395,
      "learning_rate": 0.0020631001371742112,
      "loss": 0.0015,
      "step": 3530
    },
    {
      "epoch": 4.8559670781893,
      "grad_norm": 0.0640050619840622,
      "learning_rate": 0.00205761316872428,
      "loss": 0.0013,
      "step": 3540
    },
    {
      "epoch": 4.869684499314129,
      "grad_norm": 0.03705447539687157,
      "learning_rate": 0.002052126200274349,
      "loss": 0.0018,
      "step": 3550
    },
    {
      "epoch": 4.883401920438957,
      "grad_norm": 0.008830877020955086,
      "learning_rate": 0.0020466392318244167,
      "loss": 0.001,
      "step": 3560
    },
    {
      "epoch": 4.897119341563786,
      "grad_norm": 0.018128203228116035,
      "learning_rate": 0.0020411522633744855,
      "loss": 0.0005,
      "step": 3570
    },
    {
      "epoch": 4.910836762688614,
      "grad_norm": 0.030765797942876816,
      "learning_rate": 0.0020356652949245543,
      "loss": 0.0008,
      "step": 3580
    },
    {
      "epoch": 4.924554183813443,
      "grad_norm": 0.45671403408050537,
      "learning_rate": 0.0020301783264746226,
      "loss": 0.0009,
      "step": 3590
    },
    {
      "epoch": 4.938271604938271,
      "grad_norm": 0.028181204572319984,
      "learning_rate": 0.0020246913580246914,
      "loss": 0.0006,
      "step": 3600
    },
    {
      "epoch": 4.9519890260631,
      "grad_norm": 0.040401920676231384,
      "learning_rate": 0.0020192043895747597,
      "loss": 0.0005,
      "step": 3610
    },
    {
      "epoch": 4.965706447187928,
      "grad_norm": 0.03618493676185608,
      "learning_rate": 0.0020137174211248285,
      "loss": 0.0009,
      "step": 3620
    },
    {
      "epoch": 4.979423868312757,
      "grad_norm": 0.035240333527326584,
      "learning_rate": 0.0020082304526748973,
      "loss": 0.001,
      "step": 3630
    },
    {
      "epoch": 4.9931412894375855,
      "grad_norm": 0.741183876991272,
      "learning_rate": 0.0020027434842249657,
      "loss": 0.0018,
      "step": 3640
    }
  ],
  "logging_steps": 10,
  "max_steps": 7290,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
